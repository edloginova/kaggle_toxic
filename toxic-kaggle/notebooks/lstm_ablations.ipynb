{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:\n",
    "* clean text vs original\n",
    "* word2vec vs glove vs fasttext\n",
    "* oversampling vs undersampling\n",
    "* spellchecking vs no spellchecking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import logging\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.utils import class_weight\n",
    "from keras.callbacks import *\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/\"\n",
    "data = pd.read_csv(DATA_PATH + \"preprocessed/train_heavy_clean_no-stopwords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>heavy_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>' aww ! matches background colour seemingly st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man , really trying edit war . guy constan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" cannotmake real suggestions improvement - wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>, sir , hero . chance remember page ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0           0  0000997932d777bf   \n",
       "1           1  000103f0d9cfb60f   \n",
       "2           2  000113f07ec002fd   \n",
       "3           3  0001b41b1c6bb37e   \n",
       "4           4  0001d958c54c6e35   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \\\n",
       "0        0       0       0              0   \n",
       "1        0       0       0              0   \n",
       "2        0       0       0              0   \n",
       "3        0       0       0              0   \n",
       "4        0       0       0              0   \n",
       "\n",
       "                                         heavy_clean  \n",
       "0  explanation edits made username hardcore metal...  \n",
       "1  ' aww ! matches background colour seemingly st...  \n",
       "2  hey man , really trying edit war . guy constan...  \n",
       "3  \" cannotmake real suggestions improvement - wo...  \n",
       "4              , sir , hero . chance remember page ?  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[['comment_text', 'heavy_clean']], data[list_classes], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=None, oov_token=0)\n",
    "tokenizer.fit_on_texts(X_train['comment_text'])\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# X_train.loc[pd.isnull(X_train['heavy_clean'])]['heavy_clean'] = X_train[pd.isnull(X_train['heavy_clean'])]['comment_text'].apply(heavy_clean)\n",
    "\n",
    "# X_train.loc[8846]['heavy_clean'] = heavy_clean(X_train.loc[8846]['comment_text'])\n",
    "\n",
    "# X_train[pd.isnull(X_train['heavy_clean'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index_w2v = gensim.models.KeyedVectors.load_word2vec_format('../../../embeddings/GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index_w2v.vocab))\n",
    "embedding_matrix = np.zeros((len(word_index) + 2, 300))\n",
    "oov = []\n",
    "embedding_matrix[0] = np.random.rand(1, 300)\n",
    "embedding_matrix[-1] = np.zeros((1, 300))\n",
    "for word, i in word_index.items():\n",
    "    word = str(word)\n",
    "    if word in embeddings_index_w2v.vocab:\n",
    "        embedding_vector = embeddings_index_w2v[word]\n",
    "        embedding_matrix[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    elif word.lower() in embeddings_index_w2v.vocab:\n",
    "        embedding_vector = embeddings_index_w2v[word.lower()]\n",
    "        embedding_matrix[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    elif word.capitalize() in embeddings_index_w2v.vocab:\n",
    "        embedding_vector = embeddings_index_w2v[word.capitalize()]\n",
    "        embedding_matrix[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    elif word.lower().capitalize() in embeddings_index_w2v.vocab:\n",
    "        embedding_vector = embeddings_index_w2v[word.lower().capitalize()]\n",
    "        embedding_matrix[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    else:\n",
    "        oov.append(word)\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164436"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58515"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(word_index) & set(embeddings_index_w2v.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80765"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.81057241,  0.26098693,  0.14808145, ...,  0.86761916,\n",
       "         0.30566176,  0.29109141],\n",
       "       [ 0.07468531,  0.09791063,  0.04645062, ...,  0.00341549,\n",
       "         0.04440133, -0.06421115],\n",
       "       [ 0.01803273,  0.0796839 ,  0.03568782, ...,  0.03077838,\n",
       "        -0.01652214, -0.09403456],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.04803049, -0.03817808,  0.03217427, ...,  0.00434891,\n",
       "        -0.0180884 ,  0.08744012],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length, trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_clean = Tokenizer(num_words=None, oov_token=0)\n",
    "tokenizer_clean.fit_on_texts(X_train['heavy_clean'])\n",
    "word_index_clean = tokenizer_clean.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index_w2v.vocab))\n",
    "embedding_matrix_clean = np.zeros((len(word_index_clean) + 2, 300))\n",
    "oov_clean = []\n",
    "embedding_matrix_clean[0] = np.random.rand(1, 300)\n",
    "embedding_matrix_clean[-1] = np.zeros((1, 300))\n",
    "for word, i in word_index_clean.items():\n",
    "    word = str(word)\n",
    "    if word in embeddings_index_w2v.vocab:\n",
    "        embedding_vector = embeddings_index_w2v[word]\n",
    "        embedding_matrix_clean[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    elif word.lower() in embeddings_index_w2v.vocab:\n",
    "        embedding_vector = embeddings_index_w2v[word.lower()]\n",
    "        embedding_matrix_clean[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    elif word.capitalize() in embeddings_index_w2v.vocab:\n",
    "        embedding_vector = embeddings_index_w2v[word.capitalize()]\n",
    "        embedding_matrix_clean[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    elif word.lower().capitalize() in embeddings_index_w2v.vocab:\n",
    "        embedding_vector = embeddings_index_w2v[word.lower().capitalize()]\n",
    "        embedding_matrix_clean[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    else:\n",
    "        oov_clean.append(word)\n",
    "embedding_size = 300\n",
    "embedding_layer_clean = Embedding(embedding_matrix_clean.shape[0],\n",
    "                            embedding_size,\n",
    "                            weights=[embedding_matrix_clean],\n",
    "                            input_length=max_length, trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(clipnorm=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text vs Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_texts = tokenizer.texts_to_sequences(X_train['comment_text'])\n",
    "x_train_texts = pad_sequences(x_train_texts, maxlen=max_length, padding='post')\n",
    "\n",
    "x_test_texts = tokenizer.texts_to_sequences(X_test['comment_text'])\n",
    "x_test_texts = pad_sequences(x_test_texts, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 106912 samples, validate on 52659 samples\n",
      "Epoch 1/5\n",
      "106912/106912 [==============================] - 529s 5ms/step - loss: 0.0955 - acc: 0.9717 - val_loss: 0.0530 - val_acc: 0.9804\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.967719 \n",
      "\n",
      "Epoch 2/5\n",
      "106912/106912 [==============================] - 532s 5ms/step - loss: 0.0512 - acc: 0.9814 - val_loss: 0.0520 - val_acc: 0.9811\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.971533 \n",
      "\n",
      "Epoch 3/5\n",
      "106912/106912 [==============================] - 589s 6ms/step - loss: 0.0489 - acc: 0.9819 - val_loss: 0.0492 - val_acc: 0.9818\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.973195 \n",
      "\n",
      "Epoch 4/5\n",
      "106912/106912 [==============================] - 582s 5ms/step - loss: 0.0475 - acc: 0.9823 - val_loss: 0.0495 - val_acc: 0.9817\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.973869 \n",
      "\n",
      "Epoch 5/5\n",
      "106912/106912 [==============================] - 564s 5ms/step - loss: 0.0462 - acc: 0.9827 - val_loss: 0.0493 - val_acc: 0.9819\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.974547 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa2d6b6eb00>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_input = Input(shape=(max_length,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "lstm1 = CuDNNLSTM(20, return_sequences=True)(embedded_sequences)\n",
    "lstm2 = CuDNNLSTM(5)(lstm1)\n",
    "output = Dense(units=6, activation='sigmoid')(lstm2)\n",
    "model =  Model(sequence_input, output)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])    \n",
    "model.summary()\n",
    "RocAuc = RocAucEvaluation(validation_data=(x_test_texts, y_test), interval=1)\n",
    "model.fit(x_train_texts, y_train, validation_data=(x_test_texts, y_test),\n",
    "              epochs=5, batch_size=5, callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_texts_clean = tokenizer_clean.texts_to_sequences(X_train['heavy_clean'])\n",
    "x_train_texts_clean = pad_sequences(x_train_texts_clean, maxlen=max_length, padding='post')\n",
    "\n",
    "x_test_texts_clean = tokenizer_clean.texts_to_sequences(X_test['heavy_clean'])\n",
    "x_test_texts_clean = pad_sequences(x_test_texts_clean, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 200, 300)          45433500  \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_5 (CuDNNLSTM)     (None, 200, 20)           25760     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_6 (CuDNNLSTM)     (None, 5)                 540       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 36        \n",
      "=================================================================\n",
      "Total params: 45,459,836\n",
      "Trainable params: 26,336\n",
      "Non-trainable params: 45,433,500\n",
      "_________________________________________________________________\n",
      "Train on 106912 samples, validate on 52659 samples\n",
      "Epoch 1/5\n",
      "106912/106912 [==============================] - 567s 5ms/step - loss: 0.1095 - acc: 0.9677 - val_loss: 0.0575 - val_acc: 0.9799\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.612091 \n",
      "\n",
      "Epoch 2/5\n",
      "106912/106912 [==============================] - 556s 5ms/step - loss: 0.0537 - acc: 0.9808 - val_loss: 0.0518 - val_acc: 0.9809\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.584266 \n",
      "\n",
      "Epoch 3/5\n",
      "106912/106912 [==============================] - 565s 5ms/step - loss: 0.0506 - acc: 0.9815 - val_loss: 0.0511 - val_acc: 0.9813\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.563102 \n",
      "\n",
      "Epoch 4/5\n",
      "106912/106912 [==============================] - 565s 5ms/step - loss: 0.0486 - acc: 0.9819 - val_loss: 0.0495 - val_acc: 0.9814\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.583568 \n",
      "\n",
      "Epoch 5/5\n",
      "106912/106912 [==============================] - 565s 5ms/step - loss: 0.0465 - acc: 0.9824 - val_loss: 0.0503 - val_acc: 0.9809\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.579813 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa11196a358>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_input = Input(shape=(max_length,), dtype='int32')\n",
    "embedded_sequences = embedding_layer_clean(sequence_input)\n",
    "lstm1 = CuDNNLSTM(20, return_sequences=True)(embedded_sequences)\n",
    "lstm2 = CuDNNLSTM(5)(lstm1)\n",
    "output = Dense(units=6, activation='sigmoid')(lstm2)\n",
    "model_clean =  Model(sequence_input, output)\n",
    "model_clean.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])    \n",
    "model_clean.summary()\n",
    "RocAuc = RocAucEvaluation(validation_data=(x_test_texts, y_test), interval=1)\n",
    "model_clean.fit(x_train_texts_clean, y_train, validation_data=(x_test_texts_clean, y_test),\n",
    "              epochs=5, batch_size=5, callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char vs Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_char = Tokenizer(num_words=None, oov_token=0, char_level=True)\n",
    "tokenizer_char.fit_on_texts(X_train['comment_text'])\n",
    "char_index = tokenizer_char.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(\"../../../embeddings/pretrained_character_embeddings.txt\")\n",
    "for line in f:\n",
    "    if line[0] != ' ':\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "    else:\n",
    "        values = line.split()\n",
    "        word = ' '\n",
    "        coefs = np.asarray(values, dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "embeddings_index['\\n'] = embeddings_index.pop('\\\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 91 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "embedding_matrix = np.zeros((len(char_index) + 1, 300))\n",
    "oov = []\n",
    "for char, i in char_index.items():\n",
    "    try:\n",
    "        embedding_vector = embeddings_index[char]\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except Exception as e:\n",
    "        oov.append(char)\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length, trainable = False, mask_zero = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_texts = tokenizer_char.texts_to_sequences(X_train['comment_text'])\n",
    "x_train_texts = pad_sequences(x_train_texts, maxlen=max_length, padding='post')\n",
    "\n",
    "x_test_texts = tokenizer_char.texts_to_sequences(X_test['comment_text'])\n",
    "x_test_texts = pad_sequences(x_test_texts, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 200, 300)          612000    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_10 (CuDNNLSTM)    (None, 200, 20)           25760     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_11 (CuDNNLSTM)    (None, 5)                 540       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 36        \n",
      "=================================================================\n",
      "Total params: 638,336\n",
      "Trainable params: 26,336\n",
      "Non-trainable params: 612,000\n",
      "_________________________________________________________________\n",
      "Train on 106912 samples, validate on 52659 samples\n",
      "Epoch 1/5\n",
      "106912/106912 [==============================] - 574s 5ms/step - loss: 0.1147 - acc: 0.9669 - val_loss: 0.0894 - val_acc: 0.9736\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.896769 \n",
      "\n",
      "Epoch 2/5\n",
      "106912/106912 [==============================] - 563s 5ms/step - loss: 0.0833 - acc: 0.9748 - val_loss: 0.0831 - val_acc: 0.9750\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.916569 \n",
      "\n",
      "Epoch 3/5\n",
      "106912/106912 [==============================] - 590s 6ms/step - loss: 0.0780 - acc: 0.9760 - val_loss: 0.0781 - val_acc: 0.9757\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.926900 \n",
      "\n",
      "Epoch 4/5\n",
      "106912/106912 [==============================] - 594s 6ms/step - loss: 0.0743 - acc: 0.9769 - val_loss: 0.0759 - val_acc: 0.9766\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.932017 \n",
      "\n",
      "Epoch 5/5\n",
      "106912/106912 [==============================] - 597s 6ms/step - loss: 0.0711 - acc: 0.9777 - val_loss: 0.0727 - val_acc: 0.9776\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.936539 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa10dc25240>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_input = Input(shape=(max_length,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "lstm1 = CuDNNLSTM(20, return_sequences=True)(embedded_sequences)\n",
    "lstm2 = CuDNNLSTM(5)(lstm1)\n",
    "output = Dense(units=6, activation='sigmoid')(lstm2)\n",
    "model_char =  Model(sequence_input, output)\n",
    "model_char.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])    \n",
    "model_char.summary()\n",
    "RocAuc = RocAucEvaluation(validation_data=(x_test_texts, y_test), interval=1)\n",
    "model_char.fit(x_train_texts, y_train, validation_data=(x_test_texts, y_test),\n",
    "              epochs=5, batch_size=5, callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove vs FastText vs word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200\n",
    "\n",
    "x_train_texts = tokenizer.texts_to_sequences(X_train['comment_text'])\n",
    "x_train_texts = pad_sequences(x_train_texts, maxlen=max_length, padding='post')\n",
    "\n",
    "x_test_texts = tokenizer.texts_to_sequences(X_test['comment_text'])\n",
    "x_test_texts = pad_sequences(x_test_texts, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index_w2v = gensim.models.KeyedVectors.load_word2vec_format('../../../embeddings/GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000000 word vectors.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 300)          49331400  \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 200, 20)           25760     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 5)                 540       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 36        \n",
      "=================================================================\n",
      "Total params: 49,357,736\n",
      "Trainable params: 26,336\n",
      "Non-trainable params: 49,331,400\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RocAucEvaluation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-073593ff12e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m               metrics=['acc'])    \n\u001b[1;32m     36\u001b[0m \u001b[0mmodelw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mRocAuc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRocAucEvaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m modelw2v.fit(x_train_texts, y_train, validation_data=(x_test_texts, y_test),\n\u001b[1;32m     39\u001b[0m               epochs=5, batch_size=5, callbacks=[RocAuc])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RocAucEvaluation' is not defined"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index_w2v.vocab))\n",
    "embedding_matrix = np.zeros((len(word_index) + 2, 300))\n",
    "oov = []\n",
    "embedding_matrix[0] = np.random.rand(1, 300)\n",
    "embedding_matrix[-1] = np.zeros((1, 300))\n",
    "for word, i in word_index.items():\n",
    "    word = str(word)\n",
    "    if word in embeddings_index_w2v.vocab:\n",
    "        embedding_vector = embeddings_index_w2v[word]\n",
    "        embedding_matrix[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    elif word.lower() in embeddings_index_w2v.vocab:\n",
    "        embedding_vector = embeddings_index_w2v[word.lower()]\n",
    "        embedding_matrix[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    elif word.capitalize() in embeddings_index_w2v.vocab:\n",
    "        embedding_vector = embeddings_index_w2v[word.capitalize()]\n",
    "        embedding_matrix[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    elif word.lower().capitalize() in embeddings_index_w2v.vocab:\n",
    "        embedding_vector = embeddings_index_w2v[word.lower().capitalize()]\n",
    "        embedding_matrix[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    else:\n",
    "        oov.append(word)\n",
    "embedding_size = 300\n",
    "embedding_layer_w2v = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length, trainable = False)\n",
    "sequence_input = Input(shape=(max_length,), dtype='int32')\n",
    "embedded_sequences = embedding_layer_w2v(sequence_input)\n",
    "lstm1 = CuDNNLSTM(20, return_sequences=True)(embedded_sequences)\n",
    "lstm2 = CuDNNLSTM(5)(lstm1)\n",
    "output = Dense(units=6, activation='sigmoid')(lstm2)\n",
    "modelw2v =  Model(sequence_input, output)\n",
    "modelw2v.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])    \n",
    "modelw2v.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 106912 samples, validate on 52659 samples\n",
      "Epoch 1/5\n",
      "106912/106912 [==============================] - 601s 6ms/step - loss: 0.1448 - acc: 0.9631 - val_loss: 0.1412 - val_acc: 0.9630\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.515201 \n",
      "\n",
      "Epoch 2/5\n",
      "106912/106912 [==============================] - 593s 6ms/step - loss: 0.0775 - acc: 0.9735 - val_loss: 0.0553 - val_acc: 0.9809\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.970679 \n",
      "\n",
      "Epoch 3/5\n",
      "106912/106912 [==============================] - 583s 5ms/step - loss: 0.0522 - acc: 0.9813 - val_loss: 0.0521 - val_acc: 0.9808\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.972156 \n",
      "\n",
      "Epoch 4/5\n",
      "106912/106912 [==============================] - 739s 7ms/step - loss: 0.0491 - acc: 0.9821 - val_loss: 0.0507 - val_acc: 0.9811\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.973756 \n",
      "\n",
      "Epoch 5/5\n",
      "106912/106912 [==============================] - 964s 9ms/step - loss: 0.0475 - acc: 0.9825 - val_loss: 0.0525 - val_acc: 0.9815\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.973140 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f278d406390>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(x_test_texts, y_test), interval=1)\n",
    "modelw2v.fit(x_train_texts, y_train, validation_data=(x_test_texts, y_test),\n",
    "              epochs=5, batch_size=5, callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText\n",
    "embeddings_index_fasttext = gensim.models.KeyedVectors.load_word2vec_format('../../../embeddings/crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000000 word vectors.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 200, 300)          49331400  \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, 200, 20)           25760     \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_4 (CuDNNLSTM)     (None, 5)                 540       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 36        \n",
      "=================================================================\n",
      "Total params: 49,357,736\n",
      "Trainable params: 26,336\n",
      "Non-trainable params: 49,331,400\n",
      "_________________________________________________________________\n",
      "Train on 106912 samples, validate on 52659 samples\n",
      "Epoch 1/5\n",
      "106912/106912 [==============================] - 957s 9ms/step - loss: 0.1452 - acc: 0.9618 - val_loss: 0.1133 - val_acc: 0.9630\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.890204 \n",
      "\n",
      "Epoch 2/5\n",
      "106912/106912 [==============================] - 944s 9ms/step - loss: 0.0622 - acc: 0.9776 - val_loss: 0.0506 - val_acc: 0.9814\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.973730 \n",
      "\n",
      "Epoch 3/5\n",
      "106912/106912 [==============================] - 944s 9ms/step - loss: 0.0481 - acc: 0.9821 - val_loss: 0.0479 - val_acc: 0.9820\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.975343 \n",
      "\n",
      "Epoch 4/5\n",
      "106912/106912 [==============================] - 955s 9ms/step - loss: 0.0455 - acc: 0.9828 - val_loss: 0.0470 - val_acc: 0.9825\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.976307 \n",
      "\n",
      "Epoch 5/5\n",
      "106912/106912 [==============================] - 943s 9ms/step - loss: 0.0440 - acc: 0.9832 - val_loss: 0.0477 - val_acc: 0.9819\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.976534 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2678f3b978>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index_fasttext.vocab))\n",
    "embedding_matrix = np.zeros((len(word_index) + 2, 300))\n",
    "oov = []\n",
    "embedding_matrix[0] = np.random.rand(1, 300)\n",
    "embedding_matrix[-1] = np.zeros((1, 300))\n",
    "for word, i in word_index.items():\n",
    "    word = str(word)\n",
    "    if word in embeddings_index_fasttext.vocab:\n",
    "        embedding_vector = embeddings_index_fasttext[word]\n",
    "        embedding_matrix[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    elif word.lower() in embeddings_index_fasttext.vocab:\n",
    "        embedding_vector = embeddings_index_fasttext[word.lower()]\n",
    "        embedding_matrix[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    elif word.capitalize() in embeddings_index_fasttext.vocab:\n",
    "        embedding_vector = embeddings_index_fasttext[word.capitalize()]\n",
    "        embedding_matrix[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    elif word.lower().capitalize() in embeddings_index_fasttext.vocab:\n",
    "        embedding_vector = embeddings_index_fasttext[word.lower().capitalize()]\n",
    "        embedding_matrix[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    else:\n",
    "        oov.append(word)\n",
    "embedding_size = 300\n",
    "embedding_layer_fasttext = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length, trainable = False)\n",
    "sequence_input = Input(shape=(max_length,), dtype='int32')\n",
    "embedded_sequences = embedding_layer_fasttext(sequence_input)\n",
    "lstm1 = CuDNNLSTM(20, return_sequences=True)(embedded_sequences)\n",
    "lstm2 = CuDNNLSTM(5)(lstm1)\n",
    "output = Dense(units=6, activation='sigmoid')(lstm2)\n",
    "modelfasttext =  Model(sequence_input, output)\n",
    "modelfasttext.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])    \n",
    "modelfasttext.summary()\n",
    "RocAuc = RocAucEvaluation(validation_data=(x_test_texts, y_test), interval=1)\n",
    "modelfasttext.fit(x_train_texts, y_train, validation_data=(x_test_texts, y_test),\n",
    "              epochs=5, batch_size=5, callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_FILE = '../../../embeddings/glove.42B.300d.txt'\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(GLOVE_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index.vocab))\n",
    "embedding_matrix = np.zeros((len(word_index) + 2, 300))\n",
    "oov = []\n",
    "embedding_matrix[0] = np.random.rand(1, 300)\n",
    "embedding_matrix[-1] = np.zeros((1, 300))\n",
    "for word, i in word_index.items():\n",
    "    word = str(word)\n",
    "    if word in embeddings_index.vocab:\n",
    "        embedding_vector = embeddings_index_glove[word]\n",
    "        embedding_matrix[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    elif word.lower() in embeddings_index_glove.vocab:\n",
    "        embedding_vector = embeddings_index_glove[word.lower()]\n",
    "        embedding_matrix[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    elif word.capitalize() in embeddings_index_glove.vocab:\n",
    "        embedding_vector = embeddings_index_glove[word.capitalize()]\n",
    "        embedding_matrix[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    elif word.lower().capitalize() in embeddings_index_glove.vocab:\n",
    "        embedding_vector = embeddings_index_glove[word.lower().capitalize()]\n",
    "        embedding_matrix[i] = embedding_vector / np.linalg.norm(embedding_vector)\n",
    "    else:\n",
    "        oov.append(word)\n",
    "embedding_size = 300\n",
    "embedding_layer_glove = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length, trainable = False)\n",
    "sequence_input = Input(shape=(max_length,), dtype='int32')\n",
    "embedded_sequences = embedding_layer_glove(sequence_input)\n",
    "lstm1 = CuDNNLSTM(20, return_sequences=True)(embedded_sequences)\n",
    "lstm2 = CuDNNLSTM(5)(lstm1)\n",
    "output = Dense(units=6, activation='sigmoid')(lstm2)\n",
    "modelglove =  Model(sequence_input, output)\n",
    "modelglove.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])    \n",
    "modelglove.summary()\n",
    "RocAuc = RocAucEvaluation(validation_data=(x_test_texts, y_test), interval=1)\n",
    "modelglove.fit(x_train_textsglove, y_train, validation_data=(x_test_textsglove, y_test),\n",
    "              epochs=5, batch_size=5, callbacks=[RocAuc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
