{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.utils import class_weight\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/\"\n",
    "X_train = pickle.load(open(DATA_PATH + \"X_train.p\", \"rb\"))\n",
    "X_dev = pickle.load(open(DATA_PATH + \"X_dev.p\", \"rb\"))\n",
    "y_train = pickle.load(open(DATA_PATH + \"y_train.p\", \"rb\"))\n",
    "y_dev = pickle.load(open(DATA_PATH + \"y_dev.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111699, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = X_train['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=18400)\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "sequences = tokenizer.texts_to_sequences(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText\n",
    "embeddings_index_fasttext = gensim.models.KeyedVectors.load_word2vec_format('../../../embeddings/crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index_w2v = gensim.models.KeyedVectors.load_word2vec_format('../../../embeddings/GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200\n",
    "\n",
    "x_train_texts = tokenizer.texts_to_sequences(X_train['comment_text'])\n",
    "x_train_texts = pad_sequences(x_train_texts, maxlen=max_length, padding='post')\n",
    "\n",
    "x_dev_texts = tokenizer.texts_to_sequences(X_dev['comment_text'])\n",
    "x_dev_texts = pad_sequences(x_dev_texts, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000000 word vectors.\n",
      "Found 2000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index_w2v.vocab))\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "oov = []\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = embeddings_index_w2v[word]\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except Exception as e:\n",
    "        oov.append(word)\n",
    "embedding_size = 300\n",
    "embedding_layer_w2v = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=200, trainable = False)\n",
    "print('Found %s word vectors.' % len(embeddings_index_fasttext.vocab))\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "oov = []\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = embeddings_index_fasttext[word]\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except Exception as e:\n",
    "        oov.append(word)\n",
    "embedding_size = 300\n",
    "embedding_layer_fasttext = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=200, trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80572"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(oov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     50649300    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 200, 300)     50649300    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 200, 600)     0           embedding_1[1][0]                \n",
      "                                                                 embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 120000)       0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 20)           2400020     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 20, 1)        0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 20, 20)       1760        reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 400)          0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 6)            2406        flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 103,702,786\n",
      "Trainable params: 2,404,186\n",
      "Non-trainable params: 101,298,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(max_length,), dtype='int32')\n",
    "embedded_sequences_w2v = embedding_layer_w2v(sequence_input)\n",
    "embedded_sequences_fasttext = embedding_layer_fasttext(sequence_input)\n",
    "concat = keras.layers.Concatenate()([embedded_sequences_w2v, embedded_sequences_fasttext])\n",
    "flatten = Flatten()(concat)\n",
    "dense = Dense(20)(flatten)\n",
    "reshape = Reshape((-1, 1))(dense)\n",
    "lstm = LSTM(20, return_sequences=True)(reshape)\n",
    "flatten_lstm = Flatten()(lstm)\n",
    "output = Dense(units=6, activation='sigmoid')(flatten_lstm)\n",
    "model = Model(inputs=sequence_input, outputs=output)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['acc'])    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"keras_cnn_embeddings-ensemble_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 111699 samples, validate on 47872 samples\n",
      "Epoch 1/15\n",
      "111699/111699 [==============================] - 975s 9ms/step - loss: 0.0668 - acc: 0.9770 - val_loss: 0.0624 - val_acc: 0.9774\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.966530 \n",
      "\n",
      "Epoch 2/15\n",
      "111699/111699 [==============================] - 974s 9ms/step - loss: 0.0455 - acc: 0.9828 - val_loss: 0.0640 - val_acc: 0.9788\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.962169 \n",
      "\n",
      "Epoch 3/15\n",
      "111699/111699 [==============================] - 969s 9ms/step - loss: 0.0333 - acc: 0.9872 - val_loss: 0.0723 - val_acc: 0.9778\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.956504 \n",
      "\n",
      "Epoch 4/15\n",
      "111699/111699 [==============================] - 977s 9ms/step - loss: 0.0244 - acc: 0.9908 - val_loss: 0.0899 - val_acc: 0.9778\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.946031 \n",
      "\n",
      "Epoch 5/15\n",
      "111699/111699 [==============================] - 966s 9ms/step - loss: 0.0178 - acc: 0.9933 - val_loss: 0.0998 - val_acc: 0.9761\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.944926 \n",
      "\n",
      "Epoch 6/15\n",
      "111699/111699 [==============================] - 978s 9ms/step - loss: 0.0138 - acc: 0.9949 - val_loss: 0.1112 - val_acc: 0.9765\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.939071 \n",
      "\n",
      "Epoch 7/15\n",
      "111699/111699 [==============================] - 971s 9ms/step - loss: 0.0115 - acc: 0.9958 - val_loss: 0.1337 - val_acc: 0.9769\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.940198 \n",
      "\n",
      "Epoch 8/15\n",
      "111699/111699 [==============================] - 978s 9ms/step - loss: 0.0098 - acc: 0.9965 - val_loss: 0.1371 - val_acc: 0.9763\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.939484 \n",
      "\n",
      "Epoch 9/15\n",
      "111699/111699 [==============================] - 978s 9ms/step - loss: 0.0083 - acc: 0.9970 - val_loss: 0.1390 - val_acc: 0.9752\n",
      "\n",
      " ROC-AUC - epoch: 9 - score: 0.940458 \n",
      "\n",
      "Epoch 10/15\n",
      "111699/111699 [==============================] - 978s 9ms/step - loss: 0.0074 - acc: 0.9974 - val_loss: 0.1534 - val_acc: 0.9761\n",
      "\n",
      " ROC-AUC - epoch: 10 - score: 0.927821 \n",
      "\n",
      "Epoch 11/15\n",
      "111699/111699 [==============================] - 959s 9ms/step - loss: 0.0065 - acc: 0.9977 - val_loss: 0.1669 - val_acc: 0.9760\n",
      "\n",
      " ROC-AUC - epoch: 11 - score: 0.925480 \n",
      "\n",
      "Epoch 12/15\n",
      "111695/111699 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9979"
     ]
    }
   ],
   "source": [
    "RocAuc = RocAucEvaluation(validation_data=(x_dev_texts, y_dev), interval=1)\n",
    "model.fit(x_train_texts, y_train, validation_data=(x_dev_texts, y_dev),\n",
    "              epochs=15, batch_size=5, callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(MODEL_PATH + model_name + \".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(MODEL_PATH + model_name + \".h5\")\n",
    "print(\"Saved model to disk\")\n",
    "# ensemble_cnn.load_weights(check_point_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47872/47872 [==============================] - 15s 317us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17409613444422264, 0.9758279218393213]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_dev_texts, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dev = model.predict(x_dev_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_roc_auc(y_true, y_pred):\n",
    "    roc_auc_scores = []\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        roc_auc_scores.append(metrics.roc_auc_score(y_true[:, i], y_pred[:, i]))\n",
    "    print(roc_auc_scores)\n",
    "    return np.mean(roc_auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9194586462700822, 0.9729842416688508, 0.9362496427452032, 0.8994808456313645, 0.9448475024412867, 0.9398699282914871]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9354818011747125"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_roc_auc(y_dev, pred_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_official_test = pd.read_csv(\"../data/raw/test.csv\")\n",
    "x_dev_texts = tokenizer.texts_to_sequences(X_official_test['comment_text'])\n",
    "x_dev_texts = pad_sequences(x_dev_texts, maxlen=max_length, padding='post')\n",
    "pred = model.predict(x_dev_texts)\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "sample_submission = pd.read_csv('../submissions/sample_submission.csv')\n",
    "sample_submission[list_classes] = pred\n",
    "sample_submission.to_csv(\"../submissions/\" + model_name + \".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
