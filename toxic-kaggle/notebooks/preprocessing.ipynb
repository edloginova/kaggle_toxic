{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Data Preprocessing</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('../ipython_notebook_toc_nonumbers.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('../ipython_notebook_toc_nonumbers.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from scipy.special import logit, expit\n",
    "import string\n",
    "import re    #for regex\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer   \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import collections\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import spacy\n",
    "import enchant\n",
    "import gensim\n",
    "import operator\n",
    "import sys, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/\"\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "d = enchant.Dict(\"en_US\")\n",
    "nlp = spacy.load('en')\n",
    "eng_stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_PATH + 'raw/train.csv').fillna(' ')\n",
    "test = pd.read_csv(DATA_PATH + 'raw/test.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0007e25b2121310b</td>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>001810bf8c45bf5f</td>\n",
       "      <td>You are gay or antisemmitian? \\n\\nArchangel WH...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>00190820581d90ce</td>\n",
       "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text  \\\n",
       "6   0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "12  0005c987bdfc9d4b  Hey... what is it..\\n@ | talk .\\nWhat is it......   \n",
       "16  0007e25b2121310b  Bye! \\n\\nDon't look, come or think of comming ...   \n",
       "42  001810bf8c45bf5f  You are gay or antisemmitian? \\n\\nArchangel WH...   \n",
       "43  00190820581d90ce           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!   \n",
       "\n",
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "6       1             1        1       0       1              0  \n",
       "12      1             0        0       0       0              0  \n",
       "16      1             0        0       0       0              0  \n",
       "42      1             0        1       0       1              1  \n",
       "43      1             0        1       0       1              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate toxic comments from clean ones:\n",
    "train_toxic = train[(train.toxic == 1) | (train.obscene == 1) | (train.threat == 1) | (train.insult == 1) | (train.identity_hate == 1)]\n",
    "train_toxic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of swear words in vocabulary:  1166\n"
     ]
    }
   ],
   "source": [
    "with open(\"../ling_src/obscene_words.txt\", \"r\") as f:\n",
    "    content = f.readlines()\n",
    "swear_words = set([x.strip() for x in content])\n",
    "print('Number of swear words in vocabulary: ', len(swear_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove rare & common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_toxic['tokens'] = train_toxic['comment_text'].apply(nltk.tokenize.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist([x for y in train_toxic['tokens'] for x in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52026"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_words = list(freq_dist.most_common())[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = list(freq_dist.most_common())[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare_word(tokens):\n",
    "    return [word for word in tokens if word not in rare_words]\n",
    "def remove_common_words(tokens):\n",
    "    return [word for word in tokens if word not in common_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_toxic['no_rare_tokens'] = train_toxic['tokens'].apply(remove_rare_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_toxic['no_common_tokens'] = train_toxic['tokens'].apply(remove_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_textblob(text:str):\n",
    "    return TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_toxic['textblob'] = train_toxic['comment_text'].apply(convert_textblob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textblob_spellcheck(textblob):\n",
    "    return textblob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_toxic['textblob_spellcheck'] = train_toxic['textblob'].apply(textblob_spellcheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hatebase speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_ethnicity = pickle.load(open(\"../ling_src/words_ethnicity.p\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.detect import Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphabet_detector import AlphabetDetector\n",
    "ad = AlphabetDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_language(x):\n",
    "    try:\n",
    "        return Detector(x).language.code\n",
    "    except Exception as e:\n",
    "        return 'unknown'\n",
    "def determine_alphabet(x):\n",
    "    try:\n",
    "        return ' '.join(ad.detect_alphabet(x))\n",
    "    except Exception as e:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_toxic['alphabet'] = train_toxic['comment_text'].apply(determine_alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_toxic['lang'] = train_toxic['comment_text'].apply(determine_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ARABIC LATIN': 4,\n",
       "         'CJK LATIN': 5,\n",
       "         'CUNEIFORM LATIN': 1,\n",
       "         'FULLWIDTH LATIN': 1,\n",
       "         'GREEK LATIN': 16,\n",
       "         'GREEK LATIN CYRILLIC': 2,\n",
       "         'HEBREW ARABIC LATIN': 1,\n",
       "         'HEBREW LATIN': 2,\n",
       "         'KATAKANA LATIN': 2,\n",
       "         'LATIN': 16174,\n",
       "         'LATIN CYRILLIC': 14,\n",
       "         'RUNIC LATIN': 1,\n",
       "         'SCRIPT LATIN': 1,\n",
       "         'SINHALA LATIN': 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "collections.Counter(train_toxic['alphabet'].values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['alphabet'] = train['comment_text'].apply(determine_alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'': 7,\n",
       "         'ARABIC LATIN': 54,\n",
       "         'ARABIC LATIN CYRILLIC': 2,\n",
       "         'BENGALI LATIN': 13,\n",
       "         'BOPOMOFO LATIN': 1,\n",
       "         'CJK ARABIC LATIN': 1,\n",
       "         'CJK ARABIC LATIN CYRILLIC': 1,\n",
       "         'CJK HIRAGANA LATIN CYRILLIC': 2,\n",
       "         'CJK LATIN': 258,\n",
       "         'CJK LATIN HIRAGANA': 47,\n",
       "         'CUNEIFORM LATIN': 37,\n",
       "         'CUNEIFORM RUNIC LATIN': 1,\n",
       "         'DEVANAGARI BENGALI LATIN': 1,\n",
       "         'DEVANAGARI LATIN': 45,\n",
       "         'DOUBLE-STRUCK LATIN': 1,\n",
       "         'ETHIOPIC LATIN': 19,\n",
       "         'FULLWIDTH LATIN': 1,\n",
       "         'GOTHIC MODIFIER GREEK LATIN': 1,\n",
       "         'GREEK ARABIC LATIN CYRILLIC': 1,\n",
       "         'GREEK CYRILLIC LATIN HIRAGANA': 1,\n",
       "         'GREEK ETHIOPIC LATIN': 1,\n",
       "         'GREEK HEBREW LATIN': 2,\n",
       "         'GREEK LATIN': 416,\n",
       "         'GREEK LATIN CYRILLIC': 27,\n",
       "         'GREEK MASCULINE LATIN': 1,\n",
       "         'GREEK MODIFIER LATIN CYRILLIC': 2,\n",
       "         'GURMUKHI LATIN': 4,\n",
       "         'GURMUKHI MODIFIER DEVANAGARI ARABIC LATIN': 1,\n",
       "         'HALFWIDTH CJK LATIN HIRAGANA': 1,\n",
       "         'HANGUL CJK LATIN': 5,\n",
       "         'HANGUL HEBREW ARABIC LATIN CYRILLIC': 1,\n",
       "         'HANGUL LATIN': 9,\n",
       "         'HEBREW ARABIC LATIN': 1,\n",
       "         'HEBREW LATIN': 39,\n",
       "         'HEBREW MASCULINE LATIN': 1,\n",
       "         'KANNADA CJK LATIN GEORGIAN': 1,\n",
       "         'KATAKANA CJK LATIN': 4,\n",
       "         'KATAKANA CJK LATIN CYRILLIC': 1,\n",
       "         'KATAKANA CJK LATIN HIRAGANA': 2,\n",
       "         'KATAKANA HALFWIDTH CJK LATIN': 1,\n",
       "         'KATAKANA HANGUL CJK LATIN': 3,\n",
       "         'KATAKANA LATIN': 27,\n",
       "         'KATAKANA-HIRAGANA KATAKANA CJK LATIN': 8,\n",
       "         'KATAKANA-HIRAGANA KATAKANA CJK LATIN HIRAGANA': 5,\n",
       "         'KATAKANA-HIRAGANA KATAKANA LATIN': 3,\n",
       "         'KHMER HEBREW LAO BENGALI LATIN ARMENIAN CYRILLIC GEORGIAN HANGUL THAI TELUGU KANNADA CJK GUJARATI TAMIL GREEK ARABIC DEVANAGARI': 1,\n",
       "         'KHMER HEBREW LAO BENGALI LATIN CYRILLIC ARMENIAN GEORGIAN HANGUL THAI TELUGU KANNADA CJK GUJARATI TAMIL GREEK ARABIC DEVANAGARI': 2,\n",
       "         'LAO LATIN': 1,\n",
       "         'LATIN': 158129,\n",
       "         'LATIN ARMENIAN': 3,\n",
       "         'LATIN CYRILLIC': 223,\n",
       "         'LATIN FEMININE': 3,\n",
       "         'LATIN GEORGIAN': 12,\n",
       "         'LATIN HIRAGANA': 8,\n",
       "         'LATIN MICRO': 9,\n",
       "         'MALAYALAM LATIN': 1,\n",
       "         'MASCULINE LATIN': 11,\n",
       "         'MODIFIER ARABIC LATIN': 4,\n",
       "         'MODIFIER COPTIC LATIN': 1,\n",
       "         'MODIFIER LATIN': 57,\n",
       "         'MODIFIER LATIN CYRILLIC': 2,\n",
       "         'MODIFIER LATIN GEORGIAN': 1,\n",
       "         'MODIFIER LATIN MICRO': 1,\n",
       "         'MODIFIER SUPERSCRIPT LATIN': 4,\n",
       "         'MODIFIER SUPERSCRIPT LATIN CYRILLIC SCRIPT GREEK': 1,\n",
       "         'RUNIC LATIN': 21,\n",
       "         'SCRIPT DOUBLE-STRUCK LATIN': 1,\n",
       "         'SCRIPT LATIN': 2,\n",
       "         'SINHALA LATIN': 2,\n",
       "         'SUPERSCRIPT LATIN': 1,\n",
       "         'SYRIAC LATIN': 1,\n",
       "         'TAMIL CJK ARABIC LATIN CYRILLIC': 1,\n",
       "         'TAMIL LATIN': 4,\n",
       "         'THAI LATIN': 2,\n",
       "         'TURNED LATIN': 2})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(train['alphabet'].values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 5),\n",
    "    max_features=25000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_tfidf = hstack([train_char_features, train_word_features])\n",
    "test_features_tfidf = hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train_features_tfidf, open(DATA_PATH + \"train_features_tfidf.p\", \"wb\"))\n",
    "pickle.dump(test_features_tfidf, open(DATA_PATH + \"test_features_tfidf.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_simple_features(dataset_original):\n",
    "    dataset = dataset_original.copy(deep = True)\n",
    "    dataset['count_sent']=dataset[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "    #Word count in each comment:\n",
    "    dataset['count_word']=dataset[\"comment_text\"].apply(lambda x: len(str(x).split()))\n",
    "    #Unique word count\n",
    "    dataset['count_unique_word']=dataset[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "    #Letter count\n",
    "    dataset['count_letters']=dataset[\"comment_text\"].apply(lambda x: len(str(x)))\n",
    "    #punctuation count\n",
    "    dataset[\"count_punctuations\"] =dataset[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    #upper case words count\n",
    "    dataset[\"count_words_upper\"] = dataset[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    #title case words count\n",
    "    dataset[\"count_words_title\"] = dataset[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    #Number of stopwords\n",
    "    dataset[\"count_stopwords\"] = dataset[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "    #Average length of the words\n",
    "    dataset[\"mean_word_len\"] = dataset[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    #derived features\n",
    "    #Word count percent in each comment:\n",
    "    dataset['word_unique_percent']=dataset['count_unique_word']*100/dataset['count_word']\n",
    "    #derived features\n",
    "    #Punct percent in each comment:\n",
    "    dataset['punct_percent']=dataset['count_punctuations']*100/dataset['count_word']\n",
    "    dataset[\"count_swear_words\"] = dataset[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.lower() in swear_words]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train = add_simple_features(train)\n",
    "test = add_simple_features(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(DATA_PATH + 'preprocessed/train.csv')\n",
    "test.to_csv(DATA_PATH + 'preprocessed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### code for finding potential obscene words that were not in the original list\n",
    "\n",
    "# toxic_vocab = []\n",
    "# for idx, row in train_toxic.iterrows():\n",
    "#     text = str(row['comment_text']).lower()\n",
    "#     text = re.sub('([.,!?()])', r' \\1 ', text)\n",
    "#     text = re.sub('\\s{2,}', ' ', text)\n",
    "#     for w in text.split():\n",
    "#         if w not in eng_stopwords:\n",
    "#             toxic_vocab.append(w)\n",
    "\n",
    "# c = collections.Counter(toxic_vocab)\n",
    "\n",
    "# sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# for x in set(toxic_vocab):\n",
    "#     if x not in swear_words:\n",
    "#         score = sid.polarity_scores(x)\n",
    "#         if score['compound'] < 0:\n",
    "#             print(x)\n",
    "\n",
    "# for x in c.most_common(50):\n",
    "#     if x[0] not in swear_words:\n",
    "#         print(x[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### emoticons\n",
    "\n",
    "Replacing emoticons with their word meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons = {\":-)\": \"happy\", \":)\": \"happy\", \":-]\"\":]\": \"happy\",\":-3\": \"happy\",\":3\": \"happy\",\":->\": \"happy\",\":>\": \"happy\", \\\n",
    "                 \"8-)\": \"happy\",\"8)\": \"happy\",\":-}\": \"happy\",\":}\": \"happy\",\":o)\": \"happy\",\":c)\": \"happy\",\":^)\": \"happy\", \\\n",
    "                 \"=]\": \"happy\",\"=)\": \"happy\",\":-D\": \"happy\",\":D\": \"laugh\", \"8-D\": \"laugh\",\"8D\": \"laugh\", \"x-D\": \"laugh\", \\\n",
    "                 \"xD\": \"laugh\", \"X-D\": \"laugh\",\"XD\": \"laugh\", \"=D\": \"laugh\",\"=3\": \"happy\", \"B^D\": \"laugh\",\":-(\": \"sad\", \\\n",
    "                 \":(\": \"sad\",\":-c\": \"sad\",\":c\": \"sad\",\":-<\": \"sad\",\":<\": \"sad\",\":-[\": \"sad\",\":[\": \"sad\",\":-||\": \"sad\", \\\n",
    "                 \">:[\": \"angry\",\":{\": \"sad\",\":@\": \"sad\",\">:(\": \"angry\",\";-)\": \"wink\",\";)\": \"wink\",\"*-)\": \"wink\", \\\n",
    "                 \"*)\": \"wink\",\";-]\": \"wink\",\";]\": \"wink\",\";^)\": \"wink\",\":-,\": \"wink\",\";D\": \"laugh\", \\\n",
    "                 \":-/\": \"scepticism\",\":/\": \"scepticism\",\":-.\": \"scepticism\",\">:\\\\\": \"angry\",\">:/\": \"angry\", \\\n",
    "                 \":\\\\\": \"scepticism\",\"=/\": \"scepticism\",\"=\\\\\": \"scepticism\",\":L\": \"scepticism\",\"=L\": \"scepticism\", \\\n",
    "                 \":S\": \"scepticism\"}\n",
    "emoticons_re = {}\n",
    "for key, val in emoticons.items():\n",
    "    new_key = key\n",
    "    for c in new_key:\n",
    "        if c in ['[','\\\\','^','$','.','|','?','*','+','(',')']:\n",
    "            new_key = new_key.replace(c, \"\\\\\" + c)\n",
    "        new_key = new_key.replace(\"\\\\\\|\", \"\\\\|\")\n",
    "    regex = re.compile(new_key + \"+\")\n",
    "    emoticons_re[regex] = val\n",
    "\n",
    "def replace_emoticons(text, tag = 0):\n",
    "    transformed_text = text\n",
    "    for emoticon in emoticons_re.keys():\n",
    "        if emoticon.search(text):\n",
    "            for m in emoticon.finditer(text):\n",
    "                if tag:\n",
    "                    placeholder = \" [EMOTICON:\" + emoticons_re[emoticon] + \"] \"\n",
    "                else:\n",
    "                    placeholder = \" \" + emoticons_re[emoticon] + \" \"\n",
    "                transformed_text = transformed_text.replace(m.group(), placeholder)\n",
    "    return transformed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lowercase + tokens + lemmata + spellcheck\n",
    "\n",
    "Converting to lowercase, removing multiple spaces, usernames, ip, users. Tokenization, spellchecking, lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(comment):\n",
    "    comment = comment.lower() #Convert to lower case\n",
    "    comment = re.sub(\"\\\\n\",\" \",comment)\n",
    "    comment = comment.strip() #remove \\n\n",
    "    comment = re.sub(' +',' ',comment)\n",
    "    comment = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)  # remove leaky elements like ip,user\n",
    "    comment = re.sub(\"\\[\\[.*\\]\",\"\",comment)     # removing usernamed\n",
    "#     comment = replace_emoticons(comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heavy_clean(comment):\n",
    "    comment = clean(comment)\n",
    "    for key, val in contractions_lowercase.items():\n",
    "        comment = comment.replace(key, val)\n",
    "    comment = re.sub('([!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~])', r' \\1 ', comment) # padding punctuation\n",
    "    comment = re.sub('\\s{2,}', ' ', comment) \n",
    "#     comment = ' '.join([x for x in comment.split() if x not in eng_stopwords])\n",
    "#     comment = replace_emoticons(comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_lowercase = {k.lower():v.lower() for k, v in contractions.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['heavy_clean'] = train['comment_text'].apply(heavy_clean)\n",
    "train.to_csv(DATA_PATH + 'preprocessed/train_heavy_clean_no-stopwords.csv')\n",
    "test['heavy_clean'] = test['comment_text'].apply(heavy_clean)\n",
    "test.to_csv(DATA_PATH + 'preprocessed/test_heavy_clean_no-stopwords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['heavy_clean'] = train['comment_text'].apply(heavy_clean)\n",
    "train.to_csv(DATA_PATH + 'preprocessed/train_heavy_clean_stopwords.csv')\n",
    "test['heavy_clean'] = test['comment_text'].apply(heavy_clean)\n",
    "test.to_csv(DATA_PATH + 'preprocessed/test_heavy_clean_stopwords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nlp(text):\n",
    "    return nlp(text)\n",
    "def tokenize(text):\n",
    "    return [word.text for word in text]\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if not token in eng_stopwords]\n",
    "def spellcheck(tokens):\n",
    "    words_spellchecked = [d.suggest(token)[0] if not d.check(token) and len(d.suggest(token)) > 0 and not token in string.punctuation else token for token in tokens]\n",
    "    return \" \".join(words_spellchecked)\n",
    "def lemmatize(text):\n",
    "    return [word.lemma_ for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_linguistic_features(dataset_original):\n",
    "    dataset = dataset_original.copy(deep = True)\n",
    "    print('Cleaning texts.')\n",
    "    begin = datetime.datetime.now()\n",
    "    dataset['clean_text'] = dataset['comment_text'].apply(clean)\n",
    "    print(\"Finished cleaning. Time elapsed:\",  datetime.datetime.now() - begin)\n",
    "    dataset['nlp'] = dataset['comment_text'].apply(generate_nlp)\n",
    "    print(\"Finished NLP. Time elapsed:\",  datetime.datetime.now() - begin)\n",
    "    dataset['tokens'] = dataset['nlp'].apply(tokenize)\n",
    "    print(\"Finished tokenizing. Time elapsed:\",  datetime.datetime.now() - begin)\n",
    "    dataset['lemmata'] = dataset['nlp'].apply(lemmatize)\n",
    "    print(\"Finished lemmatizing. Time elapsed:\",  datetime.datetime.now() - begin)\n",
    "    dataset['no_stopwords'] = dataset['tokens'].apply(remove_stopwords)\n",
    "    print(\"Finished removing stopwords. Time elapsed:\",  datetime.datetime.now() - begin)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spellcheck(dataset_original):\n",
    "    dataset = dataset_original.copy(deep = True)\n",
    "    dataset['spellcheck'] = dataset['tokens'].apply(spellcheck)\n",
    "    print(\"Finished spellchecking. Time elapsed:\",  datetime.datetime.now() - begin)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment(dataset_original, level = 'sentence'):\n",
    "    print('Calculating sentiment.')\n",
    "    begin = datetime.datetime.now()\n",
    "    dataset = dataset_original.copy(deep = True)\n",
    "    if level == 'sentence':\n",
    "        dataset[\"sentiment\"] = dataset[\"comment_text\"].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "    elif level == 'word':\n",
    "        dataset[\"mean_sentiment\"] = dataset[\"comment_text\"].apply(lambda x: np.mean([sid.polarity_scores(w)['compound'] for w in x.split()])) \n",
    "    print('Sentiment calculated. Time elapsed: %d',  (datetime.datetime.now() - begin).seconds)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning texts.\n",
      "Finished cleaning. Time elapsed: 0:00:09.800715\n",
      "Finished NLP. Time elapsed: 1:12:31.142858\n",
      "Finished tokenizing. Time elapsed: 1:12:50.982745\n",
      "Finished lemmatizing. Time elapsed: 1:13:09.474122\n",
      "Finished removing stopwords. Time elapsed: 1:13:12.586340\n"
     ]
    }
   ],
   "source": [
    "train_preprocessed = add_linguistic_features(train)\n",
    "train_preprocessed.to_csv(DATA_PATH + 'preprocessed/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning texts.\n",
      "Finished cleaning. Time elapsed: 0:00:08.707683\n",
      "Finished NLP. Time elapsed: 1:33:20.827369\n",
      "Finished tokenizing. Time elapsed: 1:33:37.481318\n",
      "Finished lemmatizing. Time elapsed: 1:33:52.894425\n",
      "Finished removing stopwords. Time elapsed: 1:33:56.486631\n"
     ]
    }
   ],
   "source": [
    "test_preprocessed = add_linguistic_features(test)\n",
    "test_preprocessed.to_csv(DATA_PATH + 'preprocessed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating sentiment.\n",
      "Sentiment calculated. Time elapsed: %d 216\n",
      "Calculating sentiment.\n",
      "Sentiment calculated. Time elapsed: %d 0\n"
     ]
    }
   ],
   "source": [
    "train_preprocessed = add_sentiment(train_preprocessed)\n",
    "train_preprocessed.to_csv(DATA_PATH + 'preprocessed/train_sentiment.csv')\n",
    "test_preprocessed = add_sentiment(test_preprocessed)\n",
    "test_preprocessed.to_csv(DATA_PATH + 'preprocessed/test_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed = add_spellcheck(train_preprocessed)\n",
    "train_preprocessed.to_csv(DATA_PATH + 'preprocessed/train_spellcheck.csv')\n",
    "test_preprocessed = add_spellcheck(test_preprocessed, 'test')\n",
    "test_preprocessed.to_csv(DATA_PATH + 'preprocessed/test_spellcheck.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings & Out-of-vocabulary\n",
    "\n",
    "These files are too big for git, can be acquired here:\n",
    "* Glove https://nlp.stanford.edu/projects/glove/\n",
    "* word2vec https://code.google.com/archive/p/word2vec/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FOLDER = \"../../../embeddings/\"\n",
    "GLOVE_PATH = EMBEDDING_FOLDER + \"glove.6B.300d.txt\"\n",
    "WORD2VEC_PATH = EMBEDDING_FOLDER + \"GoogleNews-vectors-negative300.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "def load_pretrained(word_embedding_type):\n",
    "    if word_embedding_type == 'Glove':\n",
    "        embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(GLOVE_PATH))\n",
    "        embedding_model = np.stack(embeddings_index.values())\n",
    "        vocab_embedding = set(embeddings_index.keys())\n",
    "    elif word_embedding_type == 'word2vec':\n",
    "        embedding_model = gensim.models.KeyedVectors.load_word2vec_format(WORD2VEC_PATH, binary = True, unicode_errors = 'ignore')\n",
    "        vocab_embedding = embedding_model.vocab\n",
    "    return embedding_model, vocab_embedding\n",
    "\n",
    "def check_word_vocab(word, vocab):\n",
    "    if word in vocab:\n",
    "        return True, [word], 'none'\n",
    "    elif word.lower() in vocab:\n",
    "        return True, [word.lower()], 'lower'\n",
    "    elif word.capitalize() in vocab:\n",
    "        return True, [word.capitalize()], 'capitalize'\n",
    "    elif word.lower().capitalize() in vocab:\n",
    "        return True, [word.lower().capitalize()], 'lower+capitalize'\n",
    "    else:\n",
    "        return False, None, None\n",
    "\n",
    "def generate_word_vocab(datasets):\n",
    "    vocab = set()\n",
    "    cntr = 0\n",
    "    for dataset in datasets:\n",
    "        for idx, row in dataset.iterrows():\n",
    "            print(\"Progress: %0.0f %%\" % (100*cntr/len(dataset)), end = \"\\r\")\n",
    "            cntr += 1\n",
    "            sys.stdout.flush()\n",
    "            vocab.update([token.text for token in nlp(row['comment_text'])])\n",
    "    sys.stdout.flush()\n",
    "    vocab = list(vocab)\n",
    "    vocab.sort()\n",
    "    print(\"vocab size: \", len(vocab))\n",
    "    vocab_encoded = list2encoded_vocab(vocab)\n",
    "    return vocab_encoded\n",
    "\n",
    "def list2encoded_vocab(lst):\n",
    "    return {word: idx for idx, word in enumerate(lst)}\n",
    "\n",
    "def filter_term(term, case_replacements, mode, spellcheck_replacements = {}):\n",
    "    if term in spellcheck_replacements.keys():\n",
    "        term = spellcheck_replacements[term]\n",
    "    if term in case_replacements.keys():\n",
    "        if case_replacements[term] == 'none':\n",
    "            return term\n",
    "        elif case_replacements[term] == 'lower':\n",
    "            return term.lower()\n",
    "        elif case_replacements[term] == 'capitalize':\n",
    "            return term.capitalize()\n",
    "        else:\n",
    "            return term.lower().capitalize()\n",
    "    else:\n",
    "        if term in string.punctuation:\n",
    "            return ''\n",
    "        else:\n",
    "            if mode == \"replace\":\n",
    "                return '#OOV#'\n",
    "            elif mode == \"remove\":\n",
    "                return ''\n",
    "            \n",
    "            \n",
    "def find_oov(vocab_encoded, vocab_embedding, spell_check_on = False):\n",
    "    \n",
    "    '''\n",
    "    Separates vocabulary into the words that are present in word embedding model and that are not.\n",
    "    Input:\n",
    "        vocab_encoded - existing vocabulary in a form {'cat': 0, 'hog': 1, ... }\n",
    "        vocab_embedding - the vocabulary of word embeddings models\n",
    "        spell_check_on - whether to try to find a correction so that the word will belong to vocabulary (optional)\n",
    "    Output:\n",
    "        vocab_clean - vocabulary with the words present in word embedding models in a form {'cat': 0, 'hog': 1, ... }\n",
    "        oov - list of unique out-of-vocabulary (in respect to word embedding model) words\n",
    "    '''\n",
    "    \n",
    "    print(spell_check_on)\n",
    "    \n",
    "    vocab_clean = set()\n",
    "    case_replacements = {}\n",
    "    if spell_check_on:\n",
    "        spellcheck_replacements = {}\n",
    "    oov = []\n",
    "    vocab_sorted = sorted(vocab_encoded.items(), key=operator.itemgetter(1))\n",
    "    for i, elem in enumerate(vocab_sorted):\n",
    "        print(\"Progress: %0.0f %%\" % (100*i/len(vocab_sorted)), end=\"\\r\")\n",
    "        sys.stdout.flush()\n",
    "        res = check_word_vocab(elem[0], vocab_embedding)\n",
    "        if res[0]:\n",
    "            vocab_clean.update(res[1])\n",
    "            case_replacements[elem[0]] = res[2]\n",
    "        else:\n",
    "            if spell_check_on:\n",
    "                is_continue = True\n",
    "                if len(elem[0]) > 3:\n",
    "                    for x in elem[0]:\n",
    "                        if x in string.punctuation:\n",
    "                            oov.append(elem[0])\n",
    "                            is_continue = False\n",
    "                    if is_continue:\n",
    "                        spell_check = d.suggest(elem[0])\n",
    "                        if len(spell_check) > 0:\n",
    "                            res = check_word_vocab(spell_check[0], vocab_embedding)\n",
    "                            if res[0]:\n",
    "                                vocab_clean.update(res[1])\n",
    "                                case_replacements[elem[0]] = res[2]\n",
    "                                spellcheck_replacements[elem[0]] = spell_check[0]\n",
    "                            else:\n",
    "                                oov.append(elem[0])\n",
    "                else:\n",
    "                    oov.append(elem[0])\n",
    "            else:\n",
    "                oov.append(elem[0])\n",
    "    vocab_clean = list2encoded_vocab(list(vocab_clean))\n",
    "    oov = list(set(oov))\n",
    "    if spell_check_on:\n",
    "         return vocab_clean, oov, case_replacements, spellcheck_replacements\n",
    "    return vocab_clean, oov, case_replacements, {}\n",
    "\n",
    "def pprint_oov(oov):\n",
    "    for a, b, c in zip(oov[200:290][::3], oov[200:290][1::3], oov[200:290][2::3]):\n",
    "        print('{:<30}{:<30}{:<}'.format(a, b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = pd.read_csv(DATA_PATH + 'preprocessed/train_ling.csv')\n",
    "test_clean = pd.read_csv(DATA_PATH + 'preprocessed/test_ling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model, vocab_embedding = load_pretrained(\"word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = generate_word_vocab([train_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vocab, open(\"../data/preprocessed/vocab_word2vec.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_clean, oov, case_replacements, spellcheck_replacements = find_oov(vocab, vocab_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint_oov(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vocab_clean, open(\"../data/preprocessed/vocab_clean_word2vec.p\", \"wb\"))\n",
    "pickle.dump(oov, open(\"../data/preprocessed/oov_word2vec.p\", \"wb\"))\n",
    "pickle.dump(case_replacements, open(\"../data/preprocessed/case_replacements_word2vec.p\", \"wb\"))\n",
    "pickle.dump(spellcheck_replacements, open(\"../data/preprocessed/spellcheck_replacements_word2vec.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = pickle.load(open(\"../ling_src/contractions.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'tis\": 'it is',\n",
       " \"'twas\": 'it was',\n",
       " \"I'd\": 'I had',\n",
       " \"I'll\": 'I shall',\n",
       " \"I'm\": 'I am',\n",
       " \"I'm'a\": 'I am going to',\n",
       " \"I've\": 'I have',\n",
       " \"ain't\": 'am not',\n",
       " \"amn't\": 'am not',\n",
       " \"aren't\": 'are not',\n",
       " \"can't \": 'cannot',\n",
       " \"could've\": 'could have',\n",
       " \"couldn't\": 'could not',\n",
       " \"daren't\": 'dare not',\n",
       " \"daresn't\": 'dare not',\n",
       " \"dasn't\": 'dare not',\n",
       " \"didn't\": 'did not',\n",
       " \"doesn't\": 'does not',\n",
       " \"don't\": 'do not',\n",
       " \"e'er\": 'ever',\n",
       " 'gonna': 'going to',\n",
       " 'gotta': 'got to',\n",
       " \"hadn't\": 'had not',\n",
       " \"hasn't\": 'has not',\n",
       " \"haven't\": 'have not',\n",
       " \"he'd\": 'he had',\n",
       " \"he'll\": 'he shall',\n",
       " \"he's\": 'he has',\n",
       " \"how'd\": 'how did',\n",
       " \"how'll\": 'how will',\n",
       " \"how's\": 'how has',\n",
       " \"isn't\": 'is not',\n",
       " \"it'd\": 'it would',\n",
       " \"it'll\": 'it shall',\n",
       " \"it's\": 'it has',\n",
       " \"let's\": 'let us',\n",
       " \"ma'am\": 'madam',\n",
       " \"may've\": 'may have',\n",
       " \"mayn't\": 'may not',\n",
       " \"might've\": 'might have',\n",
       " \"mightn't\": 'might not',\n",
       " \"must've\": 'must have',\n",
       " \"mustn't\": 'must not',\n",
       " \"ne'er\": 'never',\n",
       " \"needn't\": 'need not',\n",
       " \"noun're\": 'noun are ',\n",
       " \"noun's\": 'noun is ',\n",
       " \"o'clock\": 'of the clock',\n",
       " \"o'er\": 'over',\n",
       " \"ol'\": 'old',\n",
       " \"oughtn't\": 'ought not',\n",
       " \"shan't\": 'shall not',\n",
       " \"she'd\": 'she had',\n",
       " \"she'll\": 'she shall',\n",
       " \"she's\": 'she has',\n",
       " \"should've\": 'should have',\n",
       " \"shouldn't\": 'should not',\n",
       " \"somebody's\": 'somebody has',\n",
       " \"someone's\": 'someone has',\n",
       " \"something's\": 'something has',\n",
       " \"that'd\": 'that would',\n",
       " \"that'll\": 'that shall',\n",
       " \"that're\": 'that are',\n",
       " \"that's\": 'that has',\n",
       " \"there'd\": 'there had',\n",
       " \"there're\": 'there are',\n",
       " \"there's\": 'there has',\n",
       " \"these're\": 'these are',\n",
       " \"they'd\": 'they had',\n",
       " \"they'll\": 'they shall',\n",
       " \"they're\": 'they are',\n",
       " \"they've\": 'they have',\n",
       " \"this's\": 'this has',\n",
       " \"those're\": 'those are',\n",
       " \"wasn't\": 'was not',\n",
       " \"we'd\": 'we had',\n",
       " \"we'd've\": 'we would have',\n",
       " \"we'll\": 'we will',\n",
       " \"we're\": 'we are',\n",
       " \"we've\": 'we have',\n",
       " \"weren't\": 'were not',\n",
       " \"what'd\": 'what did',\n",
       " \"what'll\": 'what shall',\n",
       " \"what're\": 'what are',\n",
       " \"what's\": 'what has',\n",
       " \"what've\": 'what have',\n",
       " \"when's\": 'when has',\n",
       " \"where'd\": 'where did',\n",
       " \"where're\": 'where are',\n",
       " \"where's\": 'where has',\n",
       " \"where've\": 'where have',\n",
       " \"which's\": 'which has',\n",
       " \"who'd\": 'who would',\n",
       " \"who'd've\": 'who would have',\n",
       " \"who'll\": 'who shall',\n",
       " \"who're\": 'who are',\n",
       " \"who's\": 'who has',\n",
       " \"who've\": 'who have',\n",
       " \"why'd\": 'why did',\n",
       " \"why're\": 'why are',\n",
       " \"why's\": 'why has',\n",
       " \"won't\": 'will not',\n",
       " \"would've\": 'would have',\n",
       " \"wouldn't\": 'would not',\n",
       " \"y'all\": 'you all ',\n",
       " \"you'd\": 'you had',\n",
       " \"you'll\": 'you shall',\n",
       " \"you're\": 'you are',\n",
       " \"you've\": 'you have'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    for key, val in contractions.items():\n",
    "        text = text.replace(key, val)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I did not see him'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_contractions('I didn\\'t see him')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'] = train['comment_text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['comment_text'] = test['comment_text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(DATA_PATH + 'preprocessed/train_expanded_contractions.csv')\n",
    "test.to_csv(DATA_PATH + 'preprocessed/test_expanded_contractions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
