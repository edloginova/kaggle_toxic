{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Data Preprocessing</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('../ipython_notebook_toc_nonumbers.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('../ipython_notebook_toc_nonumbers.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from scipy.special import logit, expit\n",
    "import string\n",
    "import re    #for regex\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer   \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import collections\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import spacy\n",
    "import enchant\n",
    "import gensim\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/\"\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "d = enchant.Dict(\"en_US\")\n",
    "nlp = spacy.load('en')\n",
    "eng_stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_PATH + 'raw/train.csv').fillna(' ')\n",
    "test = pd.read_csv(DATA_PATH + 'raw/test.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0007e25b2121310b</td>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>001810bf8c45bf5f</td>\n",
       "      <td>You are gay or antisemmitian? \\n\\nArchangel WH...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>00190820581d90ce</td>\n",
       "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text  \\\n",
       "6   0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "12  0005c987bdfc9d4b  Hey... what is it..\\n@ | talk .\\nWhat is it......   \n",
       "16  0007e25b2121310b  Bye! \\n\\nDon't look, come or think of comming ...   \n",
       "42  001810bf8c45bf5f  You are gay or antisemmitian? \\n\\nArchangel WH...   \n",
       "43  00190820581d90ce           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!   \n",
       "\n",
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "6       1             1        1       0       1              0  \n",
       "12      1             0        0       0       0              0  \n",
       "16      1             0        0       0       0              0  \n",
       "42      1             0        1       0       1              1  \n",
       "43      1             0        1       0       1              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate toxic comments from clean ones:\n",
    "train_toxic = train[(train.toxic == 1) | (train.obscene == 1) | (train.threat == 1) | (train.insult == 1) | (train.identity_hate == 1)]\n",
    "train_toxic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of swear words in vocabulary:  1145\n"
     ]
    }
   ],
   "source": [
    "with open(\"../ling_src/obscene_words.txt\", \"r\") as f:\n",
    "    content = f.readlines()\n",
    "swear_words = set([x.strip() for x in content])\n",
    "print('Number of swear words in vocabulary: ', len(swear_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 5),\n",
    "    max_features=25000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_tfidf = hstack([train_char_features, train_word_features])\n",
    "test_features_tfidf = hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train_features_tfidf, open(DATA_PATH + \"train_features_tfidf.p\", \"wb\"))\n",
    "pickle.dump(test_features_tfidf, open(DATA_PATH + \"test_features_tfidf.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_simple_features(dataset_original):\n",
    "    dataset = dataset_original.copy(deep = True)\n",
    "    dataset['count_sent']=dataset[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "    #Word count in each comment:\n",
    "    dataset['count_word']=dataset[\"comment_text\"].apply(lambda x: len(str(x).split()))\n",
    "    #Unique word count\n",
    "    dataset['count_unique_word']=dataset[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "    #Letter count\n",
    "    dataset['count_letters']=dataset[\"comment_text\"].apply(lambda x: len(str(x)))\n",
    "    #punctuation count\n",
    "    dataset[\"count_punctuations\"] =dataset[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    #upper case words count\n",
    "    dataset[\"count_words_upper\"] = dataset[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    #title case words count\n",
    "    dataset[\"count_words_title\"] = dataset[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    #Number of stopwords\n",
    "    dataset[\"count_stopwords\"] = dataset[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "    #Average length of the words\n",
    "    dataset[\"mean_word_len\"] = dataset[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    #derived features\n",
    "    #Word count percent in each comment:\n",
    "    dataset['word_unique_percent']=dataset['count_unique_word']*100/dataset['count_word']\n",
    "    #derived features\n",
    "    #Punct percent in each comment:\n",
    "    dataset['punct_percent']=dataset['count_punctuations']*100/dataset['count_word']\n",
    "    dataset[\"count_swear_words\"] = dataset[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.lower() in swear_words]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train = add_simple_features(train)\n",
    "test = add_simple_features(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(DATA_PATH + 'preprocessed/train.csv')\n",
    "test.to_csv(DATA_PATH + 'preprocessed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### code for finding potential obscene words that were not in the original list\n",
    "\n",
    "# toxic_vocab = []\n",
    "# for idx, row in train_toxic.iterrows():\n",
    "#     text = str(row['comment_text']).lower()\n",
    "#     text = re.sub('([.,!?()])', r' \\1 ', text)\n",
    "#     text = re.sub('\\s{2,}', ' ', text)\n",
    "#     for w in text.split():\n",
    "#         if w not in eng_stopwords:\n",
    "#             toxic_vocab.append(w)\n",
    "\n",
    "# c = collections.Counter(toxic_vocab)\n",
    "\n",
    "# sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# for x in set(toxic_vocab):\n",
    "#     if x not in swear_words:\n",
    "#         score = sid.polarity_scores(x)\n",
    "#         if score['compound'] < 0:\n",
    "#             print(x)\n",
    "\n",
    "# for x in c.most_common(50):\n",
    "#     if x[0] not in swear_words:\n",
    "#         print(x[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### emoticons\n",
    "\n",
    "Replacing emoticons with their word meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons = {\":-)\": \"happy\", \":)\": \"happy\", \":-]\"\":]\": \"happy\",\":-3\": \"happy\",\":3\": \"happy\",\":->\": \"happy\",\":>\": \"happy\", \\\n",
    "                 \"8-)\": \"happy\",\"8)\": \"happy\",\":-}\": \"happy\",\":}\": \"happy\",\":o)\": \"happy\",\":c)\": \"happy\",\":^)\": \"happy\", \\\n",
    "                 \"=]\": \"happy\",\"=)\": \"happy\",\":-D\": \"happy\",\":D\": \"laugh\", \"8-D\": \"laugh\",\"8D\": \"laugh\", \"x-D\": \"laugh\", \\\n",
    "                 \"xD\": \"laugh\", \"X-D\": \"laugh\",\"XD\": \"laugh\", \"=D\": \"laugh\",\"=3\": \"happy\", \"B^D\": \"laugh\",\":-(\": \"sad\", \\\n",
    "                 \":(\": \"sad\",\":-c\": \"sad\",\":c\": \"sad\",\":-<\": \"sad\",\":<\": \"sad\",\":-[\": \"sad\",\":[\": \"sad\",\":-||\": \"sad\", \\\n",
    "                 \">:[\": \"angry\",\":{\": \"sad\",\":@\": \"sad\",\">:(\": \"angry\",\";-)\": \"wink\",\";)\": \"wink\",\"*-)\": \"wink\", \\\n",
    "                 \"*)\": \"wink\",\";-]\": \"wink\",\";]\": \"wink\",\";^)\": \"wink\",\":-,\": \"wink\",\";D\": \"laugh\", \\\n",
    "                 \":-/\": \"scepticism\",\":/\": \"scepticism\",\":-.\": \"scepticism\",\">:\\\\\": \"angry\",\">:/\": \"angry\", \\\n",
    "                 \":\\\\\": \"scepticism\",\"=/\": \"scepticism\",\"=\\\\\": \"scepticism\",\":L\": \"scepticism\",\"=L\": \"scepticism\", \\\n",
    "                 \":S\": \"scepticism\"}\n",
    "emoticons_re = {}\n",
    "for key, val in emoticons.items():\n",
    "    new_key = key\n",
    "    for c in new_key:\n",
    "        if c in ['[','\\\\','^','$','.','|','?','*','+','(',')']:\n",
    "            new_key = new_key.replace(c, \"\\\\\" + c)\n",
    "        new_key = new_key.replace(\"\\\\\\|\", \"\\\\|\")\n",
    "    regex = re.compile(new_key + \"+\")\n",
    "    emoticons_re[regex] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emoticons(text, tag = 0):\n",
    "    transformed_text = text\n",
    "    for emoticon in emoticons_re.keys():\n",
    "        if emoticon.search(text):\n",
    "            for m in emoticon.finditer(text):\n",
    "                if tag:\n",
    "                    placeholder = \" [EMOTICON:\" + emoticons_re[emoticon] + \"] \"\n",
    "                else:\n",
    "                    placeholder = \" \" + emoticons_re[emoticon] + \" \"\n",
    "                transformed_text = transformed_text.replace(m.group(), placeholder)\n",
    "    return transformed_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lowercase + tokens + lemmata + spellcheck\n",
    "\n",
    "Converting to lowercase, removing multiple spaces, usernames, ip, users. Tokenization, spellchecking, lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(comment):\n",
    "    \n",
    "    comment = comment.lower() #Convert to lower case\n",
    "    comment = re.sub(\"\\\\n\",\" \",comment)\n",
    "    comment = comment.strip() #remove \\n\n",
    "    comment = re.sub(' +',' ',comment)\n",
    "    comment = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)  # remove leaky elements like ip,user\n",
    "    comment = re.sub(\"\\[\\[.*\\]\",\"\",comment)     # removing usernames\n",
    "    comment = replace_emoticons(comment)\n",
    "    \n",
    "    tokens = [word.text for word in nlp(comment)]\n",
    "    original_tokens = tokens\n",
    "    tokens = [token for token in tokens if not token in eng_stopwords]\n",
    "    words_spellchecked = [d.suggest(token)[0] if not d.check(token) and len(d.suggest(token)) > 0 and not token in string.punctuation else token for token in tokens ]\n",
    "    lemmata = [word.lemma_ for word in nlp(comment)]\n",
    "    \n",
    "    clean_comment = \" \".join(tokens)\n",
    "    spellchecked_comment = \" \".join(words_spellchecked)\n",
    "    return clean_comment, spellchecked_comment, lemmata, tokens, original_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey... what is it..\n",
      "@ | talk .\n",
      "What is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\n",
      "\n",
      "Ask Sityush to clean up his behavior than issue me nonsensical warnings...\n",
      "hey ... .. @ | talk . ... exclusive group wp talibans ... good destroying , self - appointed purist gang one asks questions abt anti - social destructive ( non)-contribution wp ? ask sityush clean behavior issue nonsensical warnings ...\n",
      "['hey', '...', '..', '@', '|', 'talk', '.', '...', 'exclusive', 'group', 'wp', 'talibans', '...', 'good', 'destroying', ',', 'self', '-', 'appointed', 'purist', 'gang', 'one', 'asks', 'questions', 'abt', 'anti', '-', 'social', 'destructive', '(', 'non)-contribution', 'wp', '?', 'ask', 'sityush', 'clean', 'behavior', 'issue', 'nonsensical', 'warnings', '...']\n",
      "['hey', '...', 'what', 'is', 'it', '..', '@', '|', 'talk', '.', 'what', 'is', 'it', '...', 'an', 'exclusive', 'group', 'of', 'some', 'wp', 'talibans', '...', 'who', 'are', 'good', 'at', 'destroying', ',', 'self', '-', 'appointed', 'purist', 'who', 'gang', 'up', 'any', 'one', 'who', 'asks', 'them', 'questions', 'abt', 'their', 'anti', '-', 'social', 'and', 'destructive', '(', 'non)-contribution', 'at', 'wp', '?', 'ask', 'sityush', 'to', 'clean', 'up', 'his', 'behavior', 'than', 'issue', 'me', 'nonsensical', 'warnings', '...']\n",
      "['hey', '...', 'what', 'be', '-PRON-', '..', '@', '|', 'talk', '.', 'what', 'be', '-PRON-', '...', 'an', 'exclusive', 'group', 'of', 'some', 'wp', 'taliban', '...', 'who', 'be', 'good', 'at', 'destroy', ',', 'self', '-', 'appoint', 'purist', 'who', 'gang', 'up', 'any', 'one', 'who', 'ask', '-PRON-', 'question', 'abt', '-PRON-', 'anti', '-', 'social', 'and', 'destructive', '(', 'non)-contribution', 'at', 'wp', '?', 'ask', 'sityush', 'to', 'clean', 'up', '-PRON-', 'behavior', 'than', 'issue', '-PRON-', 'nonsensical', 'warning', '...']\n"
     ]
    }
   ],
   "source": [
    "example_text = train_toxic['comment_text'].values[1]\n",
    "print(example_text)\n",
    "\n",
    "clean_comment, spellchecked_comment, lemmata, tokens, original_tokens = clean(train_toxic['comment_text'].values[1])\n",
    "\n",
    "print(clean_comment)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "print(original_tokens)\n",
    "\n",
    "print(lemmata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey ... .. @ | talk . ... exclusive group WP tali bans ... good destroying , self - appointed purist gang one asks questions bat anti - social destructive ( non-contribution WP ? ask situs clean behavior issue nonsensical warnings ...\n"
     ]
    }
   ],
   "source": [
    "# Spellchecking doesn't make a lot of sense:\n",
    "print(spellchecked_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_linguistic_features(dataset_original):\n",
    "    dataset = dataset_original.copy(deep = True)\n",
    "    dataset.loc[:, 'lemmata'] = ''\n",
    "    dataset.loc[:, 'lemmata'] = dataset.loc[:, 'lemmata'].astype(object)\n",
    "    dataset.loc[:, 'tokens'] = ''\n",
    "    dataset.loc[:, 'tokens'] = dataset.loc[:, 'tokens'].astype(object)\n",
    "    dataset.loc[:, 'original_tokens'] = ''\n",
    "    dataset.loc[:, 'original_tokens'] = dataset.loc[:, 'original_tokens'].astype(object)\n",
    "    for idx, row in dataset.iterrows():\n",
    "        clean_comment, spellchecked_comment, lemmata, tokens, original_tokens = clean(row['comment_text'])\n",
    "        dataset.set_value(idx, 'clean_comment', clean_comment)\n",
    "        dataset.set_value(idx, 'spellchecked_comment', spellchecked_comment)\n",
    "        dataset.set_value(idx, 'lemmata', lemmata)\n",
    "        dataset.set_value(idx, 'tokens', tokens)\n",
    "        dataset.set_value(idx, 'original_tokens', original_tokens)\n",
    "    dataset[\"sentiment\"] = dataset[\"comment_text\"].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "    dataset[\"mean_sentiment\"] = dataset[\"comment_text\"].apply(lambda x: np.mean([sid.polarity_scores(w)['compound'] for w in x.split()])) \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_linguistic_features(train)\n",
    "test = add_linguistic_features(test)\n",
    "\n",
    "train.to_csv(DATA_PATH + 'preprocessed/train.csv')\n",
    "test.to_csv(DATA_PATH + 'preprocessed/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings & Out-of-vocabulary\n",
    "\n",
    "These files are too big for git, can be acquired here:\n",
    "* Glove https://nlp.stanford.edu/projects/glove/\n",
    "* word2vec https://code.google.com/archive/p/word2vec/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FOLDER = \"../../../embeddings/\"\n",
    "GLOVE_PATH = EMBEDDING_FOLDER + \"glove.6B.300d.txt\"\n",
    "WORD2VEC_PATH = EMBEDDING_FOLDER + \"GoogleNews-vectors-negative300.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "def load_pretrained(word_embedding_type):\n",
    "    if word_embedding_type == 'Glove':\n",
    "        embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(GLOVE_PATH))\n",
    "        embedding_model = np.stack(embeddings_index.values())\n",
    "        vocab_embedding = set(embeddings_index.keys())\n",
    "    elif word_embedding_type == 'word2vec':\n",
    "        embedding_model = gensim.models.KeyedVectors.load_word2vec_format(WORD2VEC_PATH, binary = True, unicode_errors = 'ignore')\n",
    "        vocab_embedding = embedding_model.vocab\n",
    "    return embedding_model, vocab_embedding\n",
    "\n",
    "def check_word_vocab(word, vocab):\n",
    "    if word in vocab:\n",
    "        return True, [word], 'none'\n",
    "    elif word.lower() in vocab:\n",
    "        return True, [word.lower()], 'lower'\n",
    "    elif word.capitalize() in vocab:\n",
    "        return True, [word.capitalize()], 'capitalize'\n",
    "    elif word.lower().capitalize() in vocab:\n",
    "        return True, [word.lower().capitalize()], 'lower+capitalize'\n",
    "    else:\n",
    "        return False, None, None\n",
    "\n",
    "def generate_word_vocab(datasets):\n",
    "    vocab = set()\n",
    "    cntr = 0\n",
    "    for dataset in datasets:\n",
    "        for idx, row in dataset.iterrows():\n",
    "            print(\"Progress: %0.0f %%\" % (100*cntr/len(dataset)), end = \"\\r\")\n",
    "            cntr += 1\n",
    "            sys.stdout.flush()\n",
    "            vocab.update([token.text for token in nlp(row['comment_text'])])\n",
    "    sys.stdout.flush()\n",
    "    vocab = list(vocab)\n",
    "    vocab.sort()\n",
    "    print(\"vocab size: \", len(vocab))\n",
    "    vocab_encoded = list2encoded_vocab(vocab)\n",
    "    return vocab_encoded\n",
    "\n",
    "def list2encoded_vocab(lst):\n",
    "    return {word: idx for idx, word in enumerate(lst)}\n",
    "\n",
    "def filter_term(term, case_replacements, mode, spellcheck_replacements = {}):\n",
    "    if term in spellcheck_replacements.keys():\n",
    "        term = spellcheck_replacements[term]\n",
    "    if term in case_replacements.keys():\n",
    "        if case_replacements[term] == 'none':\n",
    "            return term\n",
    "        elif case_replacements[term] == 'lower':\n",
    "            return term.lower()\n",
    "        elif case_replacements[term] == 'capitalize':\n",
    "            return term.capitalize()\n",
    "        else:\n",
    "            return term.lower().capitalize()\n",
    "    else:\n",
    "        if term in string.punctuation:\n",
    "            return ''\n",
    "        else:\n",
    "            if mode == \"replace\":\n",
    "                return '#OOV#'\n",
    "            elif mode == \"remove\":\n",
    "                return ''\n",
    "            \n",
    "            \n",
    "def find_oov(vocab_encoded, vocab_embedding, spell_check_on = False):\n",
    "    \n",
    "    '''\n",
    "    Separates vocabulary into the words that are present in word embedding model and that are not.\n",
    "    Input:\n",
    "        vocab_encoded - existing vocabulary in a form {'cat': 0, 'hog': 1, ... }\n",
    "        vocab_embedding - the vocabulary of word embeddings models\n",
    "        spell_check_on - whether to try to find a correction so that the word will belong to vocabulary (optional)\n",
    "    Output:\n",
    "        vocab_clean - vocabulary with the words present in word embedding models in a form {'cat': 0, 'hog': 1, ... }\n",
    "        oov - list of unique out-of-vocabulary (in respect to word embedding model) words\n",
    "    '''\n",
    "    \n",
    "    print(spell_check_on)\n",
    "    \n",
    "    vocab_clean = set()\n",
    "    case_replacements = {}\n",
    "    if spell_check_on:\n",
    "        spellcheck_replacements = {}\n",
    "    oov = []\n",
    "    vocab_sorted = sorted(vocab_encoded.items(), key=operator.itemgetter(1))\n",
    "    for i, elem in enumerate(vocab_sorted):\n",
    "        print(\"Progress: %0.0f %%\" % (100*i/len(vocab_sorted)), end=\"\\r\")\n",
    "        sys.stdout.flush()\n",
    "        res = check_word_vocab(elem[0], vocab_embedding)\n",
    "        if res[0]:\n",
    "            vocab_clean.update(res[1])\n",
    "            case_replacements[elem[0]] = res[2]\n",
    "        else:\n",
    "            if spell_check_on:\n",
    "                is_continue = True\n",
    "                if len(elem[0]) > 3:\n",
    "                    for x in elem[0]:\n",
    "                        if x in string.punctuation:\n",
    "                            oov.append(elem[0])\n",
    "                            is_continue = False\n",
    "                    if is_continue:\n",
    "                        spell_check = d.suggest(elem[0])\n",
    "                        if len(spell_check) > 0:\n",
    "                            res = check_word_vocab(spell_check[0], vocab_embedding)\n",
    "                            if res[0]:\n",
    "                                vocab_clean.update(res[1])\n",
    "                                case_replacements[elem[0]] = res[2]\n",
    "                                spellcheck_replacements[elem[0]] = spell_check[0]\n",
    "                            else:\n",
    "                                oov.append(elem[0])\n",
    "                else:\n",
    "                    oov.append(elem[0])\n",
    "            else:\n",
    "                oov.append(elem[0])\n",
    "    vocab_clean = list2encoded_vocab(list(vocab_clean))\n",
    "    oov = list(set(oov))\n",
    "    if spell_check_on:\n",
    "         return vocab_clean, oov, case_replacements, spellcheck_replacements\n",
    "    return vocab_clean, oov, case_replacements, {}\n",
    "\n",
    "def pprint_oov(oov):\n",
    "    for a, b, c in zip(oov[200:290][::3], oov[200:290][1::3], oov[200:290][2::3]):\n",
    "        print('{:<30}{:<30}{:<}'.format(a, b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = pd.read_csv(DATA_PATH + 'preprocessed/train_ling.csv')\n",
    "test_clean = pd.read_csv(DATA_PATH + 'preprocessed/test_ling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model, vocab_embedding = load_pretrained(\"word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  265198\n"
     ]
    }
   ],
   "source": [
    "vocab = generate_word_vocab([train_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vocab, open(\"../data/preprocessed/vocab_word2vec.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Progress: 100 %rogress: 24 %Progress: 25 %Progress: 27 %Progress: 27 %Progress: 28 %Progress: 28 %Progress: 36 %Progress: 37 %Progress: 38 %Progress: 39 %Progress: 40 %Progress: 42 %Progress: 43 %Progress: 45 %Progress: 47 %Progress: 48 %Progress: 49 %Progress: 50 %Progress: 51 %Progress: 52 %Progress: 56 %Progress: 58 %Progress: 59 %Progress: 59 %Progress: 60 %Progress: 61 %Progress: 62 %Progress: 63 %Progress: 65 %Progress: 67 %Progress: 70 %Progress: 71 %Progress: 72 %Progress: 73 %Progress: 74 %Progress: 76 %Progress: 76 %Progress: 78 %Progress: 80 %Progress: 81 %Progress: 81 %Progress: 82 %Progress: 82 %Progress: 85 %Progress: 88 %Progress: 90 %Progress: 92 %Progress: 92 %Progress: 93 %Progress: 95 %Progress: 96 %Progress: 99 %Progress: 100 %\r"
     ]
    }
   ],
   "source": [
    "vocab_clean, oov, case_replacements, spellcheck_replacements = find_oov(vocab, vocab_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hawkens1993                   1477                          A2-AB)/(A2-AB\n",
      "Ichiki                        Kiapiz                        CO/4302/2002TABDate\n",
      "Radio(Between                 autheticated                  http://www.stopwar.org.uk/index.php/usa-war-on-terror/2158-michael-moore-why-i-dont-support-the-troops-america-and-neither-do-you\n",
      "Cculber007                    -Wiggalama                    I.K.Gill\n",
      "CP30777                       94.4.32.46                    Stevertigo\n",
      "url}.                         eybaná                        theireven\n",
      "Wikivoyage                    http://books.google.co.in/books?ei=5RiUTYfiJMrWrQfh3JnzCw&ct;=result&id;=41MIAAAAQAAJ&dq;=abhira+afghanistan&q;=abhirasnewbiiee\n",
      "recreator                     5.6.5                         BlazikenMaster\n",
      "http://www.thekillersmusic.com/story/news/artwork_for_happy_birthday_guadalupeVGMusic                       pleeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaaassssssssssssssssseeeeeeeeeeeee!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!11111111111111111111111111oneoneoneoneoneoneoneoneoneoneoenone\n",
      "tougue                        pad|40px}}[[Cyril             WIKIPEDIA!DEATH\n",
      "Soapboxers                    22:37                         270lbs\n",
      "Evolian                       Law#Ergenekon                 1911–2000\n",
      "deffinetly                    24.255.11.149                 pizzled\n",
      "88.113.152.140                Nazirim==                     BLPNAME\n",
      "Nacedonia                     Pribina&curid;=4981041&diff;=208314304&oldid;=208291495virigin\n",
      "18:25                         falshi                        181.140.167.24\n",
      "910(f                         Rembaoud                      Yawn*.\n",
      "Zaporozhskaya                 possesions?89.7.100.151       agree—\n",
      "antipope                      1:2                           cut&pasted\n",
      "|4com=                        7:7                           I#Jinxmchue\n",
      "onepurposeonly                65.7.162.38                   help.72.0.36.36\n",
      "Hezbollahd                    FUCKINGL                      urev\n",
      "Spanise                       1832Summa                     50=1941\n",
      "exception-                    Wisconsin|in2=the             134.134.139.75\n",
      "2006-very                     magadhas                      Yuganta\n",
      "effetive                      97                            Maintitle.jpg\n",
      "pffff                         06:23                         Rahtlef\n",
      "myg                           15:23                         Contributions/88.112.189.85\n",
      "KOERAN                        pp.432                        50.150.181.230\n",
      "A5b                           Knowitwell                    protograf\n"
     ]
    }
   ],
   "source": [
    "pprint_oov(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vocab_clean, open(\"../data/preprocessed/vocab_clean_word2vec.p\", \"wb\"))\n",
    "pickle.dump(oov, open(\"../data/preprocessed/oov_word2vec.p\", \"wb\"))\n",
    "pickle.dump(case_replacements, open(\"../data/preprocessed/case_replacements_word2vec.p\", \"wb\"))\n",
    "pickle.dump(spellcheck_replacements, open(\"../data/preprocessed/spellcheck_replacements_word2vec.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = pickle.load(open(\"../ling_src/contractions.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    for key, val in contractions.items():\n",
    "        text = text.replace(key, val)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I did not see him'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_contractions('I didn\\'t see him')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'] = train['comment_text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['comment_text'] = test['comment_text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(DATA_PATH + 'preprocessed/train_expanded_contractions.csv')\n",
    "test.to_csv(DATA_PATH + 'preprocessed/test_expanded_contractions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
