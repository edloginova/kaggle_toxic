{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for datasets.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = \"../data/\"\n",
    "# X_train = pickle.load(open(DATA_PATH + \"X_train.p\", \"rb\"))\n",
    "# X_dev = pickle.load(open(DATA_PATH + \"X_dev.p\", \"rb\"))\n",
    "# y_train = pickle.load(open(DATA_PATH + \"y_train.p\", \"rb\"))\n",
    "# y_dev = pickle.load(open(DATA_PATH + \"y_dev.p\", \"rb\"))\n",
    "# X_train = X_train['comment_text'].values\n",
    "# X_train = [clean_str(sent) for sent in X_train]\n",
    "# X_train = [s.split(\" \") for s in X_train]\n",
    "# sentences_padded = pad_sentences(X_train)\n",
    "# vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "# pickle.dump(vocabulary, open(\"../data/preprocessed/cnn/vocabulary.p\", \"wb\"))\n",
    "# pickle.dump(vocabulary_inv, open(\"../data/preprocessed/cnn/vocabulary_inv.p\", \"wb\"))\n",
    "# x, y = build_input_data(sentences_padded, y_train, vocabulary)\n",
    "# X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.2, random_state=42)\n",
    "# pickle.dump(X_train, open(\"../data/preprocessed/cnn/X_train.p\", \"wb\"))\n",
    "# pickle.dump(X_test, open(\"../data/preprocessed/cnn/X_test.p\", \"wb\"))\n",
    "# pickle.dump(y_train, open(\"../data/preprocessed/cnn/y_train.p\", \"wb\"))\n",
    "# pickle.dump(y_test, open(\"../data/preprocessed/cnn/y_test.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pickle.load( open(\"../data/preprocessed/cnn/X_train.p\", \"rb\"))\n",
    "X_test = pickle.load( open(\"../data/preprocessed/cnn/X_test.p\", \"rb\"))\n",
    "y_train = pickle.load( open(\"../data/preprocessed/cnn/y_train.p\", \"rb\"))\n",
    "y_test = pickle.load( open(\"../data/preprocessed/cnn/y_test.p\", \"rb\"))\n",
    "vocabulary = pickle.load( open(\"../data/preprocessed/cnn/vocabulary.p\", \"rb\"))\n",
    "vocabulary_inv = pickle.load( open(\"../data/preprocessed/cnn/vocabulary_inv.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89359, 4948)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cnn_512filters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = X_train.shape[1] # 56\n",
    "vocabulary_size = len(vocabulary_inv) # 18765\n",
    "embedding_dim = 256\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Model...\")\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=6, activation='sigmoid')(dropout)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"../models/\" + model_name + \".h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 89359 samples, validate on 22340 samples\n",
      "Epoch 1/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9755Epoch 00001: val_acc improved from -inf to 0.98143, saving model to ../models/cnn_512filters.h5\n",
      "89359/89359 [==============================] - 1766s 20ms/step - loss: 0.0776 - acc: 0.9755 - val_loss: 0.0527 - val_acc: 0.9814\n",
      "Epoch 2/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9831Epoch 00002: val_acc improved from 0.98143 to 0.98261, saving model to ../models/cnn_512filters.h5\n",
      "89359/89359 [==============================] - 1766s 20ms/step - loss: 0.0462 - acc: 0.9831 - val_loss: 0.0472 - val_acc: 0.9826\n",
      "Epoch 3/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9852Epoch 00003: val_acc improved from 0.98261 to 0.98291, saving model to ../models/cnn_512filters.h5\n",
      "89359/89359 [==============================] - 1763s 20ms/step - loss: 0.0389 - acc: 0.9852 - val_loss: 0.0467 - val_acc: 0.9829\n",
      "Epoch 4/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9873Epoch 00004: val_acc improved from 0.98291 to 0.98298, saving model to ../models/cnn_512filters.h5\n",
      "89359/89359 [==============================] - 1739s 19ms/step - loss: 0.0329 - acc: 0.9873 - val_loss: 0.0480 - val_acc: 0.9830\n",
      "Epoch 5/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9894Epoch 00005: val_acc did not improve\n",
      "89359/89359 [==============================] - 1757s 20ms/step - loss: 0.0278 - acc: 0.9894 - val_loss: 0.0484 - val_acc: 0.9828\n",
      "Epoch 6/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9913Epoch 00006: val_acc did not improve\n",
      "89359/89359 [==============================] - 1761s 20ms/step - loss: 0.0227 - acc: 0.9913 - val_loss: 0.0520 - val_acc: 0.9823\n",
      "Epoch 7/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9932Epoch 00007: val_acc did not improve\n",
      "89359/89359 [==============================] - 1749s 20ms/step - loss: 0.0184 - acc: 0.9932 - val_loss: 0.0554 - val_acc: 0.9823\n",
      "Epoch 8/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9947Epoch 00008: val_acc did not improve\n",
      "89359/89359 [==============================] - 1707s 19ms/step - loss: 0.0145 - acc: 0.9947 - val_loss: 0.0610 - val_acc: 0.9817\n",
      "Epoch 9/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9960Epoch 00009: val_acc did not improve\n",
      "89359/89359 [==============================] - 1705s 19ms/step - loss: 0.0114 - acc: 0.9960 - val_loss: 0.0650 - val_acc: 0.9817\n",
      "Epoch 10/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9971Epoch 00010: val_acc did not improve\n",
      "89359/89359 [==============================] - 1705s 19ms/step - loss: 0.0088 - acc: 0.9971 - val_loss: 0.0729 - val_acc: 0.9813\n",
      "Epoch 11/20\n",
      "61670/89359 [===================>..........] - ETA: 8:09 - loss: 0.0070 - acc: 0.9977Epoch 00011: val_acc did not improve\n",
      "89359/89359 [==============================] - 1704s 19ms/step - loss: 0.0071 - acc: 0.9977 - val_loss: 0.0767 - val_acc: 0.9814\n",
      "Epoch 12/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9983Epoch 00012: val_acc did not improve\n",
      "89359/89359 [==============================] - 1706s 19ms/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.0842 - val_acc: 0.9808\n",
      "Epoch 13/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9986Epoch 00013: val_acc did not improve\n",
      "89359/89359 [==============================] - 1706s 19ms/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0877 - val_acc: 0.9806\n",
      "Epoch 14/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9988Epoch 00014: val_acc did not improve\n",
      "89359/89359 [==============================] - 1705s 19ms/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0908 - val_acc: 0.9805\n",
      "Epoch 15/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9990Epoch 00015: val_acc did not improve\n",
      "89359/89359 [==============================] - 1704s 19ms/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0982 - val_acc: 0.9806\n",
      "Epoch 16/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9992Epoch 00016: val_acc did not improve\n",
      "89359/89359 [==============================] - 1704s 19ms/step - loss: 0.0029 - acc: 0.9992 - val_loss: 0.1023 - val_acc: 0.9803\n",
      "Epoch 17/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9993Epoch 00017: val_acc did not improve\n",
      "89359/89359 [==============================] - 1705s 19ms/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.1038 - val_acc: 0.9802\n",
      "Epoch 18/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9993Epoch 00018: val_acc did not improve\n",
      "89359/89359 [==============================] - 1705s 19ms/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.1110 - val_acc: 0.9802\n",
      "Epoch 19/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9994Epoch 00019: val_acc did not improve\n",
      "89359/89359 [==============================] - 1704s 19ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.1092 - val_acc: 0.9800\n",
      "Epoch 20/20\n",
      "89350/89359 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9995Epoch 00020: val_acc did not improve\n",
      "89359/89359 [==============================] - 1702s 19ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.1142 - val_acc: 0.9798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc50a063d30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(X_test, y_test))  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22340/22340 [==============================] - 136s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11420533155799904, 0.9797821752707021]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_official_test = pd.read_csv(\"../data/raw/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_official_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_official_test = X_official_test['comment_text'].values\n",
    "X_official_test = [clean_str(sent) for sent in X_official_test]\n",
    "X_official_test = [s.split(\" \") for s in X_official_test]\n",
    "sentences_padded = pad_sentences(X_official_test)\n",
    "input_official_test = [[vocabulary[word] if word in vocabulary.keys() else vocabulary['<PAD/>'] for word in sentence][:sequence_length] for sentence in sentences_padded]\n",
    "input_official_test = np.array(input_official_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(input_official_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00, 7.6079491e-04, 9.9977845e-01, 4.5479645e-04,\n",
       "        6.5788597e-01, 2.8980246e-03],\n",
       "       [4.1717476e-07, 2.0273018e-10, 2.5410711e-07, 6.9606576e-10,\n",
       "        9.4658432e-08, 1.0131702e-08],\n",
       "       [2.4840146e-02, 3.7809627e-05, 5.5967597e-04, 2.2267770e-05,\n",
       "        3.3064093e-04, 3.0477891e-05],\n",
       "       [4.9160352e-08, 7.8039498e-08, 3.3403830e-06, 4.7232027e-07,\n",
       "        2.6196167e-06, 7.4032030e-08],\n",
       "       [1.7843114e-12, 1.6204430e-09, 6.9018929e-10, 9.2310645e-11,\n",
       "        2.3779442e-10, 1.8485312e-11],\n",
       "       [2.8221211e-06, 7.3520727e-07, 2.0043837e-05, 5.2830787e-06,\n",
       "        3.8845370e-05, 7.1646940e-07],\n",
       "       [2.9325328e-08, 1.7341399e-09, 1.1999602e-07, 1.0083566e-07,\n",
       "        1.3109195e-08, 5.0725123e-07],\n",
       "       [4.0071312e-01, 4.7665526e-07, 3.3666592e-04, 2.1422446e-07,\n",
       "        1.8792100e-04, 6.1540923e-08],\n",
       "       [4.7383113e-12, 5.0560233e-12, 9.1178835e-11, 1.2671983e-12,\n",
       "        1.2710340e-10, 4.4644449e-12],\n",
       "       [6.5379454e-18, 5.0153257e-13, 6.8185334e-13, 3.4809590e-13,\n",
       "        6.4823586e-13, 3.3105695e-12]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"../models/\" + model_name + \".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../submissions/sample_submission.csv')\n",
    "sample_submission[list_classes] = pred\n",
    "sample_submission.to_csv(\"../submissions/\" + model_name + \".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
