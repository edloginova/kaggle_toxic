{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/preprocessed/train.csv')\n",
    "test = pd.read_csv('../data/preprocessed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['count_sent',\n",
    "            'count_word',\n",
    "            'count_letters',\n",
    "            'count_punctuations',\n",
    "            'mean_word_len',\n",
    "            'word_unique_percent',\n",
    "            'uppercase_percent',\n",
    "            'stopwords_perent']\n",
    "\n",
    "target = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_avg_auc(Y,predictions):\n",
    "    nb_classes = Y.shape[1]\n",
    "    cols = Y.columns\n",
    "    score = 0\n",
    "    for cl in range(nb_classes):\n",
    "        score += roc_auc_score(Y[cols[cl]],predictions[:,cl])\n",
    "    score = score/nb_classes\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(train[features],train[target], random_state=997)\n",
    "X = test[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit = [None]*len(target)\n",
    "for cl in range(len(target)):\n",
    "    logit[cl] = LogisticRegression(multi_class = 'ovr', class_weight = 'balanced')\n",
    "    logit[cl].fit(X_train,Y_train[list(Y_train.columns)[cl]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC for a set of single-target logit models is 0.620174187236\n"
     ]
    }
   ],
   "source": [
    "predictions = np.zeros((X_test.shape[0],len(target)))\n",
    "for cl in range(len(target)):\n",
    "    predictions[:,cl] = logit[cl].predict(X_test)\n",
    "score = calc_avg_auc(Y_test,predictions)\n",
    "print(\"Mean AUC for a set of single-target logit models is \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_logit = np.zeros((X.shape[0],len(target)))\n",
    "for cl in range(Y_train.shape[1]):\n",
    "    submission_logit[:,cl] = logit[cl].predict(X)\n",
    "submission_logit = pd.DataFrame(submission_logit, columns = target)\n",
    "submission_logit['id'] = test['id']\n",
    "submission_logit=submission_logit[['id']+target]\n",
    "submission_logit.to_csv('../submissions/logit.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import *\n",
    "from sklearn.ensemble import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One RF per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = [None]*len(target)\n",
    "for cl in range(Y_train.shape[1]):\n",
    "    rf[cl] = RandomForestClassifier(n_estimators=130, max_features=None,random_state=997,min_samples_leaf=100)\n",
    "    rf[cl].fit(X_train,Y_train[list(Y_train.columns)[cl]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC for a set of single-target RF models is 0.770897555977\n"
     ]
    }
   ],
   "source": [
    "predictions = np.zeros((X_test.shape[0],len(target)))\n",
    "for cl in range(len(target)):\n",
    "    predictions[:,cl] = rf[cl].predict_proba(X_test)[:,1]\n",
    "score = calc_avg_auc(Y_test,predictions)\n",
    "print(\"Mean AUC for a set of single-target RF models is \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_rf = np.zeros((X.shape[0],len(target)))\n",
    "for cl in range(Y_train.shape[1]):\n",
    "    submission_rf[:,cl] = rf[cl].predict_proba(X)[:,1]\n",
    "    joblib.dump(rf[cl],'../models/rf'+target[cl]+'.pkl')\n",
    "submission_rf = pd.DataFrame(submission_rf, columns = target)\n",
    "submission_rf['id'] = test['id']\n",
    "submission_rf=submission_rf[['id']+target]\n",
    "submission_rf.to_csv('../submissions/rf.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single multi-output RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC for a single multi-target RF model is 0.737785683318\n"
     ]
    }
   ],
   "source": [
    "rf_joint = RandomForestClassifier(n_estimators=130,criterion='gini',random_state=997,class_weight='balanced',min_samples_leaf=50)\n",
    "rf_joint.fit(X_train,Y_train)\n",
    "predictions = rf_joint.predict_proba(X_test)\n",
    "predictions_matrix = np.zeros((X_test.shape[0],len(target)))\n",
    "for targ in range(len(target)):\n",
    "    predictions_matrix[:,targ] = predictions[targ][:,1]\n",
    "score = calc_avg_auc(Y_test,predictions_matrix)\n",
    "print(\"Mean AUC for a single multi-target RF model is \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class toxic, AUC = 0.702739793752\n",
      "Class severe_toxic, AUC = 0.807153090431\n",
      "Class obscene, AUC = 0.735892097635\n",
      "Class threat, AUC = 0.730944133417\n",
      "Class insult, AUC = 0.728980487772\n",
      "Class identity_hate, AUC = 0.721004496899\n"
     ]
    }
   ],
   "source": [
    "for cl in range(Y_test.shape[1]):\n",
    "    cols = Y_test.columns\n",
    "    auc = roc_auc_score(Y_test[cols[cl]],predictions_matrix[:,cl])\n",
    "    print(\"Class \"+cols[cl]+\", AUC = \"+str(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/rf_joint_50samples_entropy.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = rf_joint.predict_proba(X)\n",
    "submission_rf_joint = np.zeros((X.shape[0],len(target)))\n",
    "for targ in range(len(target)):\n",
    "    submission_rf_joint[:,targ] = probabilities[targ][:,1]\n",
    "submission_rf_joint = pd.DataFrame(submission_rf_joint, columns = target)\n",
    "submission_rf_joint['id'] = test['id']\n",
    "submission_rf_joint=submission_rf_joint[['id']+target]\n",
    "submission_rf_joint.to_csv('../submissions/evgeniya/rf_joint_50samples.csv', index = False)\n",
    "joblib.dump(rf_joint,'../models/rf_joint_50samples.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ada = [None]*len(target)\n",
    "base_estimator = DecisionTreeClassifier(min_samples_leaf=100)\n",
    "for cl in range(Y_train.shape[1]):\n",
    "    ada[cl] = AdaBoostClassifier(base_estimator=base_estimator)\n",
    "    ada[cl].fit(X_train,Y_train[list(Y_train.columns)[cl]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC for a set of AdaBoost models is 0.716101547481\n"
     ]
    }
   ],
   "source": [
    "predictions = np.zeros((X_test.shape[0],len(target)))\n",
    "for cl in range(len(target)):\n",
    "    predictions[:,cl] = ada[cl].predict_proba(X_test)[:,1]\n",
    "    joblib.dump(rf[cl],'../models/ada'+target[cl]+'.pkl')\n",
    "score = calc_avg_auc(Y_test,predictions)\n",
    "print(\"Mean AUC for a set of AdaBoost models is \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-42-897d76b2958f>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-42-897d76b2958f>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    submission_ada[:,cl] = ada[cl].predict_proba(X){:,1}\u001b[0m\n\u001b[1;37m                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "submission_ada = np.zeros((X.shape[0],len(target)))\n",
    "for cl in range(Y_train.shape[1]):\n",
    "    submission_ada[:,cl] = ada[cl].predict_proba(X){:,1}\n",
    "submission_ada = pd.DataFrame(submission_ada, columns = target)\n",
    "submission_ada['id'] = test['id']\n",
    "submission_ada=submission_ada[['id']+target]\n",
    "submission_ada.to_csv('../submissions/ada.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named xgboost",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-9b31cfdb821c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named xgboost"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgboost = [None]*len(target)\n",
    "for cl in range(Y_train.shape[1]):\n",
    "    xgboost[cl] =XGBBoostClassifier()\n",
    "    xgboost[cl].fit(X_train,Y_train[list(Y_train.columns)[cl]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
