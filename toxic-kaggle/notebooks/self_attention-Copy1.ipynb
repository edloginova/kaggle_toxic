{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "import keras\n",
    "from keras import backend as K, activations, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "more_than_2_sequential_characters = re.compile(r'(.)\\1{3,}', flags=re.IGNORECASE)\n",
    "def preprocess(x):\n",
    "    return x.fillna(\"fillna\") \\\n",
    "    .values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../../../embeddings/crawl-300d-2M.vec'\n",
    "\n",
    "train = pd.read_csv('../data/preprocessed/train.csv')\n",
    "test = pd.read_csv('../data/preprocessed/test.csv')\n",
    "submission = pd.read_csv('../submissions/sample_submission.csv')\n",
    "\n",
    "X_train = preprocess(train[\"clean_text\"])\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = preprocess(test[\"clean_text\"])\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    x = np.array([[embeddings_index[vocabulary_inv[vocabulary['word']]] if word in vocabulary.keys() else len(vocabulary) - 1 for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>nlp</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmata</th>\n",
       "      <th>no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>['Explanation', '\\n', 'Why', 'the', 'edits', '...</td>\n",
       "      <td>['explanation', '\\n', 'why', 'the', 'edit', 'm...</td>\n",
       "      <td>['Explanation', '\\n', 'Why', 'edits', 'made', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>[\"D'aww\", '!', 'He', 'matches', 'this', 'backg...</td>\n",
       "      <td>[\"d'aww\", '!', '-PRON-', 'match', 'this', 'bac...</td>\n",
       "      <td>[\"D'aww\", '!', 'He', 'matches', 'background', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>['Hey', 'man', ',', 'I', \"'m\", 'really', 'not'...</td>\n",
       "      <td>['hey', 'man', ',', '-PRON-', 'be', 'really', ...</td>\n",
       "      <td>['Hey', 'man', ',', 'I', \"'m\", 'really', 'tryi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" more i can't make any real suggestions on im...</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>['\"', '\\n', 'More', '\\n', 'I', 'ca', \"n't\", 'm...</td>\n",
       "      <td>['\"', '\\n', 'more', '\\n', '-PRON-', 'can', 'no...</td>\n",
       "      <td>['\"', '\\n', 'More', '\\n', 'I', 'ca', \"n't\", 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>['You', ',', 'sir', ',', 'are', 'my', 'hero', ...</td>\n",
       "      <td>['-PRON-', ',', 'sir', ',', 'be', '-PRON-', 'h...</td>\n",
       "      <td>['You', ',', 'sir', ',', 'hero', '.', 'Any', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0           0  0000997932d777bf   \n",
       "1           1  000103f0d9cfb60f   \n",
       "2           2  000113f07ec002fd   \n",
       "3           3  0001b41b1c6bb37e   \n",
       "4           4  0001d958c54c6e35   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \\\n",
       "0        0       0       0              0   \n",
       "1        0       0       0              0   \n",
       "2        0       0       0              0   \n",
       "3        0       0       0              0   \n",
       "4        0       0       0              0   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  explanation why the edits made under my userna...   \n",
       "1  d'aww! he matches this background colour i'm s...   \n",
       "2  hey man, i'm really not trying to edit war. it...   \n",
       "3  \" more i can't make any real suggestions on im...   \n",
       "4  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                                 nlp  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  D'aww! He matches this background colour I'm s...   \n",
       "2  Hey man, I'm really not trying to edit war. It...   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  You, sir, are my hero. Any chance you remember...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['Explanation', '\\n', 'Why', 'the', 'edits', '...   \n",
       "1  [\"D'aww\", '!', 'He', 'matches', 'this', 'backg...   \n",
       "2  ['Hey', 'man', ',', 'I', \"'m\", 'really', 'not'...   \n",
       "3  ['\"', '\\n', 'More', '\\n', 'I', 'ca', \"n't\", 'm...   \n",
       "4  ['You', ',', 'sir', ',', 'are', 'my', 'hero', ...   \n",
       "\n",
       "                                             lemmata  \\\n",
       "0  ['explanation', '\\n', 'why', 'the', 'edit', 'm...   \n",
       "1  [\"d'aww\", '!', '-PRON-', 'match', 'this', 'bac...   \n",
       "2  ['hey', 'man', ',', '-PRON-', 'be', 'really', ...   \n",
       "3  ['\"', '\\n', 'more', '\\n', '-PRON-', 'can', 'no...   \n",
       "4  ['-PRON-', ',', 'sir', ',', 'be', '-PRON-', 'h...   \n",
       "\n",
       "                                        no_stopwords  \n",
       "0  ['Explanation', '\\n', 'Why', 'edits', 'made', ...  \n",
       "1  [\"D'aww\", '!', 'He', 'matches', 'background', ...  \n",
       "2  ['Hey', 'man', ',', 'I', \"'m\", 'really', 'tryi...  \n",
       "3  ['\"', '\\n', 'More', '\\n', 'I', 'ca', \"n't\", 'm...  \n",
       "4  ['You', ',', 'sir', ',', 'hero', '.', 'Any', '...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "maxlen = 100\n",
    "embed_size = 300\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train_sequences, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test_sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(Layer):\n",
    "    @interfaces.legacy_dense_support\n",
    "    def __init__(self, units,\n",
    "                 activation='tanh',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=2)\n",
    "        self.supports_masking = False\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.ws1 = self.add_weight(shape=( self.units, input_shape[-1]),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='ws1',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint, trainable=True)\n",
    "\n",
    "        self.ws2 = self.add_weight(shape=(1, self.units,),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='ws2',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint, trainable=True)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print('H', inputs.shape)\n",
    "#         print('H^T', K.transpose(inputs).shape)\n",
    "        print('W_s1', self.ws1.shape)\n",
    "        print('W_s2', self.ws2.shape)\n",
    "        print('inputs.shape[0]', inputs.shape[0])\n",
    "        shape = K.shape(inputs)\n",
    "        inputs_t_shape = tf.stack([shape[0], shape[2], shape[1]])\n",
    "        print('shape', shape)\n",
    "        input_reshaped = K.reshape(inputs, inputs_t_shape)\n",
    "        print('input_reshaped', input_reshaped.shape)\n",
    "        ws1_repeated = K.repeat(self.ws1, shape[0])\n",
    "        print('ws1_repeated', ws1_repeated.shape)\n",
    "        ws1_permuted = K.permute_dimensions(ws1_repeated,(1,0,2))\n",
    "        print('ws1_permuted', ws1_permuted.shape)\n",
    "        inputs_permuted = K.permute_dimensions(inputs,(0,2,1,))\n",
    "        print('inputs_permuted', inputs_permuted.shape)\n",
    "        output = K.batch_dot(inputs_permuted , ws1_permuted, axes=[1,2]) #will result in (?,500,500)\n",
    "        print('output', output.shape)\n",
    "#         #we also need to transpose x[1]:\n",
    "#         x1T = K.permute_dimensions(x[1],(0,2,1))\n",
    "\n",
    "#         #and the second multiplication:\n",
    "#         result = K.batch_dot(firstMul, x1T, axes=[1,2])\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "            \n",
    "        print('output', output.shape)\n",
    "        print('self.ws2', self.ws2.shape)\n",
    "        ws2_repeated = K.repeat(self.ws2, shape[0])\n",
    "        print('ws2_repeated', ws2_repeated.shape)\n",
    "        ws2_permuted = K.permute_dimensions(ws2_repeated,(1,0,2))\n",
    "        print('ws2_permuted', ws2_permuted.shape)\n",
    "        ws2_permuted = K.permute_dimensions(ws2_permuted,(0, 2, 1))\n",
    "        print('ws2_permuted', ws2_permuted.shape)\n",
    "        output = K.batch_dot(ws2_permuted, output, axes=[1,2])\n",
    "        print('output', output.shape)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[1]\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'activation': activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'v_kernel_initializer': initializers.serialize(self.v_kernel_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'v_kernel_regularizer': regularizers.serialize(self.v_kernel_regularizer),\n",
    "            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint),\n",
    "            'v_kernel_constraint':constraints.serialize(self.v_kernel_constraint)\n",
    "        }\n",
    "        base_config = super(SelfAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d_a$ = 13\n",
    "$n$ = 100\n",
    "$u$ = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "    lstm = Bidirectional(CuDNNLSTM(80, return_sequences=True))(x)\n",
    "    sa = SelfAttention(100)(lstm)\n",
    "#     att_1 = TimeDistributed(Dense(13))(lstm) # tanh(W_{s_1} * H^T)\n",
    "#     att_2 = TimeDistributed(Dense(1, activation = 'softmax'))(att_1) # A = softmax(w_{s_2}*tanh(W_s * H^T)\n",
    "#     att_3 = Multiply()([att_2, lstm]) # AH\n",
    "#     flat = Flatten()(att_3)\n",
    "#     dense = Dense(units=1000, activation='sigmoid')(sa)\n",
    "    output = Dense(units=6, activation='sigmoid')(sa)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func = get_model\n",
    "epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H (?, 100, 600)\n",
      "W_s1 (350, 600)\n",
      "W_s2 (1, 350)\n",
      "inputs.shape[0] ?\n",
      "shape Tensor(\"self_attention_3/Shape:0\", shape=(3,), dtype=int32)\n",
      "input_reshaped (?, ?, ?)\n",
      "ws1_repeated (350, ?, 600)\n",
      "ws1_permuted (?, 350, 600)\n",
      "inputs_permuted (?, 600, 100)\n",
      "output (?, 100, 350)\n",
      "output (?, 100, 350)\n",
      "self.ws2 (1, 350)\n",
      "ws2_repeated (1, ?, 350)\n",
      "ws2_permuted (?, 1, 350)\n",
      "ws2_permuted (?, 350, 1)\n",
      "output (?, 1, 100)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 100, 300)          9000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 100, 600)          1444800   \n",
      "_________________________________________________________________\n",
      "self_attention_3 (SelfAttent (None, 100)               210350    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 10,655,756\n",
      "Trainable params: 10,655,756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_func()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "##### RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7979/7979 [==============================] - 22s 3ms/step\n",
      "0.5104167399261529\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_val_pred = model.predict(X_val, batch_size=2, verbose=1)\n",
    "score = roc_auc_score(y_val, y_val_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_roc_auc(y_true, y_pred):\n",
    "    roc_auc_scores = []\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        roc_auc_scores.append(roc_auc_score(y_true[:, i], y_pred[:, i]))\n",
    "    print(roc_auc_scores)\n",
    "    return np.mean(roc_auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.503690036900369, 0.5126370437155089, 0.5067567567567568, 0.5001887504718763, 0.5504925501430831, 0.48873530156932293]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5104167399261529"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_roc_auc(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val_pred = ensemble_model.predict(X_val, batch_size=4, verbose=1)\n",
    "# score = roc_auc_score(y_val, y_val_pred)\n",
    "# print(score)\n",
    "\n",
    "# y_pred = ensemble_model.predict(x_test, batch_size=4, verbose=1)\n",
    "# submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "# submission.to_csv('clean_text_lstm_7_ensemble_adamax005.csv', index=False)\n",
    "\n",
    "# print(x_test.shape, y_pred.shape, y_val.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
