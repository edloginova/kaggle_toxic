{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "import keras\n",
    "from keras import backend as K, activations, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['OMP_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "more_than_2_sequential_characters = re.compile(r'(.)\\1{3,}', flags=re.IGNORECASE)\n",
    "def preprocess(x):\n",
    "    return x.fillna(\"fillna\") \\\n",
    "    .values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../../../embeddings/crawl-300d-2M.vec'\n",
    "\n",
    "train = pd.read_csv('../data/preprocessed/train.csv')\n",
    "test = pd.read_csv('../data/preprocessed/test.csv')\n",
    "submission = pd.read_csv('../submissions/sample_submission.csv')\n",
    "\n",
    "X_train = preprocess(train[\"clean_text\"])\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = preprocess(test[\"clean_text\"])\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    x = np.array([[embeddings_index[vocabulary_inv[vocabulary['word']]] if word in vocabulary.keys() else len(vocabulary) - 1 for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>nlp</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmata</th>\n",
       "      <th>no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>['Explanation', '\\n', 'Why', 'the', 'edits', '...</td>\n",
       "      <td>['explanation', '\\n', 'why', 'the', 'edit', 'm...</td>\n",
       "      <td>['Explanation', '\\n', 'Why', 'edits', 'made', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>[\"D'aww\", '!', 'He', 'matches', 'this', 'backg...</td>\n",
       "      <td>[\"d'aww\", '!', '-PRON-', 'match', 'this', 'bac...</td>\n",
       "      <td>[\"D'aww\", '!', 'He', 'matches', 'background', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>['Hey', 'man', ',', 'I', \"'m\", 'really', 'not'...</td>\n",
       "      <td>['hey', 'man', ',', '-PRON-', 'be', 'really', ...</td>\n",
       "      <td>['Hey', 'man', ',', 'I', \"'m\", 'really', 'tryi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" more i can't make any real suggestions on im...</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>['\"', '\\n', 'More', '\\n', 'I', 'ca', \"n't\", 'm...</td>\n",
       "      <td>['\"', '\\n', 'more', '\\n', '-PRON-', 'can', 'no...</td>\n",
       "      <td>['\"', '\\n', 'More', '\\n', 'I', 'ca', \"n't\", 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>['You', ',', 'sir', ',', 'are', 'my', 'hero', ...</td>\n",
       "      <td>['-PRON-', ',', 'sir', ',', 'be', '-PRON-', 'h...</td>\n",
       "      <td>['You', ',', 'sir', ',', 'hero', '.', 'Any', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0           0  0000997932d777bf   \n",
       "1           1  000103f0d9cfb60f   \n",
       "2           2  000113f07ec002fd   \n",
       "3           3  0001b41b1c6bb37e   \n",
       "4           4  0001d958c54c6e35   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \\\n",
       "0        0       0       0              0   \n",
       "1        0       0       0              0   \n",
       "2        0       0       0              0   \n",
       "3        0       0       0              0   \n",
       "4        0       0       0              0   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  explanation why the edits made under my userna...   \n",
       "1  d'aww! he matches this background colour i'm s...   \n",
       "2  hey man, i'm really not trying to edit war. it...   \n",
       "3  \" more i can't make any real suggestions on im...   \n",
       "4  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                                 nlp  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  D'aww! He matches this background colour I'm s...   \n",
       "2  Hey man, I'm really not trying to edit war. It...   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  You, sir, are my hero. Any chance you remember...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['Explanation', '\\n', 'Why', 'the', 'edits', '...   \n",
       "1  [\"D'aww\", '!', 'He', 'matches', 'this', 'backg...   \n",
       "2  ['Hey', 'man', ',', 'I', \"'m\", 'really', 'not'...   \n",
       "3  ['\"', '\\n', 'More', '\\n', 'I', 'ca', \"n't\", 'm...   \n",
       "4  ['You', ',', 'sir', ',', 'are', 'my', 'hero', ...   \n",
       "\n",
       "                                             lemmata  \\\n",
       "0  ['explanation', '\\n', 'why', 'the', 'edit', 'm...   \n",
       "1  [\"d'aww\", '!', '-PRON-', 'match', 'this', 'bac...   \n",
       "2  ['hey', 'man', ',', '-PRON-', 'be', 'really', ...   \n",
       "3  ['\"', '\\n', 'more', '\\n', '-PRON-', 'can', 'no...   \n",
       "4  ['-PRON-', ',', 'sir', ',', 'be', '-PRON-', 'h...   \n",
       "\n",
       "                                        no_stopwords  \n",
       "0  ['Explanation', '\\n', 'Why', 'edits', 'made', ...  \n",
       "1  [\"D'aww\", '!', 'He', 'matches', 'background', ...  \n",
       "2  ['Hey', 'man', ',', 'I', \"'m\", 'really', 'tryi...  \n",
       "3  ['\"', '\\n', 'More', '\\n', 'I', 'ca', \"n't\", 'm...  \n",
       "4  ['You', ',', 'sir', ',', 'hero', '.', 'Any', '...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "maxlen = 100\n",
    "embed_size = 300\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train_sequences, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test_sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(Layer):\n",
    "    @interfaces.legacy_dense_support\n",
    "    def __init__(self, units,\n",
    "                 activation='tanh',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 v_kernel_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 v_kernel_regularizer = None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 v_kernel_constraint=None,\n",
    "                 **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.v_kernel_initializer = initializers.get(v_kernel_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.v_kernel_regularizer = regularizers.get(v_kernel_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.v_kernel_constraint =  constraints.get(v_kernel_constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=2)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1],),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "\n",
    "        self.v_kernel = self.add_weight(shape=(1, self.units),\n",
    "                                      initializer=self.v_kernel_initializer,\n",
    "                                      name='v_kernel',\n",
    "                                      regularizer=self.v_kernel_regularizer,\n",
    "                                      constraint=self.v_kernel_constraint)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "#         self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "#         reshape_layer = K.reshape(inputs,(-1,100,160))\n",
    "#         print(reshape_layer.shape)\n",
    "#         output = K.dot(self.kernel, reshape_layer)%%\n",
    "        print('H', inputs.shape)\n",
    "#         inputs = K.reshape(inputs, (-1, 160))\n",
    "#         print('H reshape', inputs.shape)\n",
    "        print('H^T', K.transpose(inputs).shape)\n",
    "        print('W_s1', self.kernel.shape)\n",
    "#         output = dot_product(self.kernel, K.transpose(inputs))\n",
    "#         BatchM = K.repeat_elements(x=self.kernel,rep=1,axis=0)\n",
    "        tmp = K.batch_dot(inputs, self.kernel, axes=[1,2])\n",
    "\n",
    "        #we also need to transpose x[1]:\n",
    "        inputs1T = K.permute_dimensions(inputs[1],(0,2,1))\n",
    "\n",
    "        #and the second multiplication:\n",
    "        output = K.batch_dot(tmp, inputs1T, axes=[1,2])\n",
    "#         output = K.sum(self.kernel * inputs,axis=-1,keepdims=True)\n",
    "#         if self.use_bias:\n",
    "#             output = K.bias_add(output, self.bias)\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        print('output', output.shape)\n",
    "        v_kernel_output = dot_product(self.v_kernel, output)\n",
    "        return dot_product(activations.softmax(v_kernel_output), inputs)\n",
    "\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'activation': activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'v_kernel_initializer': initializers.serialize(self.v_kernel_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'v_kernel_regularizer': regularizers.serialize(self.v_kernel_regularizer),\n",
    "            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint),\n",
    "            'v_kernel_constraint':constraints.serialize(self.v_kernel_constraint)\n",
    "        }\n",
    "        base_config = super(SelfAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d_a$ = 13\n",
    "$n$ = 100\n",
    "$u$ = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "    lstm = Bidirectional(CuDNNLSTM(80, return_sequences=True))(x)\n",
    "    att_1 = TimeDistributed(Dense(13))(lstm) # tanh(W_{s_1} * H^T)\n",
    "    att_2 = TimeDistributed(Dense(1, activation = 'softmax'))(att_1) # A = softmax(w_{s_2}*tanh(W_s * H^T)\n",
    "    att_3 = Multiply()([att_2, lstm]) # AH\n",
    "    flat = Flatten()(att_3)\n",
    "    output = Dense(units=6, activation='sigmoid')(flat)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_func = get_model\n",
    "epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_53 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_53 (Embedding)        (None, 100, 300)     9000000     input_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_53 (Bidirectional (None, 100, 160)     244480      embedding_53[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 100, 13)      2093        bidirectional_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 100, 1)       14          time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 100, 160)     0           time_distributed_6[0][0]         \n",
      "                                                                 bidirectional_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 16000)        0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 6)            96006       flatten_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 9,342,593\n",
      "Trainable params: 9,342,593\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_func()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/5\n",
      "151592/151592 [==============================] - 142s 935us/step - loss: 0.0549 - acc: 0.9809 - val_loss: 0.0529 - val_acc: 0.9815\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.982803 \n",
      "\n",
      "Epoch 2/5\n",
      "151592/151592 [==============================] - 140s 924us/step - loss: 0.0387 - acc: 0.9854 - val_loss: 0.0516 - val_acc: 0.9813\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.981408 \n",
      "\n",
      "Epoch 3/5\n",
      "151592/151592 [==============================] - 141s 927us/step - loss: 0.0249 - acc: 0.9911 - val_loss: 0.0636 - val_acc: 0.9798\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.974178 \n",
      "\n",
      "Epoch 4/5\n",
      "151592/151592 [==============================] - 140s 927us/step - loss: 0.0164 - acc: 0.9947 - val_loss: 0.0775 - val_acc: 0.9795\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.968631 \n",
      "\n",
      "Epoch 5/5\n",
      "151592/151592 [==============================] - 141s 927us/step - loss: 0.0124 - acc: 0.9965 - val_loss: 0.0918 - val_acc: 0.9786\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.966636 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7979/7979 [==============================] - 24s 3ms/step\n",
      "0.9666361492889011\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_val_pred = model.predict(X_val, batch_size=2, verbose=1)\n",
    "score = roc_auc_score(y_val, y_val_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_roc_auc(y_true, y_pred):\n",
    "    roc_auc_scores = []\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        roc_auc_scores.append(roc_auc_score(y_true[:, i], y_pred[:, i]))\n",
    "    print(roc_auc_scores)\n",
    "    return np.mean(roc_auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9554555662776835, 0.98057301288454, 0.9673196255313045, 0.9714082358122563, 0.9651489811370993, 0.9599114740905229]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9666361492889011"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_roc_auc(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val_pred = ensemble_model.predict(X_val, batch_size=4, verbose=1)\n",
    "# score = roc_auc_score(y_val, y_val_pred)\n",
    "# print(score)\n",
    "\n",
    "# y_pred = ensemble_model.predict(x_test, batch_size=4, verbose=1)\n",
    "# submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "# submission.to_csv('clean_text_lstm_7_ensemble_adamax005.csv', index=False)\n",
    "\n",
    "# print(x_test.shape, y_pred.shape, y_val.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
