{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ling = pd.read_csv(\"../data/\" + \"preprocessed/train_ling.csv\")\n",
    "\n",
    "EMBEDDING_FILE = '../../../embeddings/glove.42B.300d.txt'\n",
    "\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "submission = pd.read_csv('../submissions/sample_submission.csv')\n",
    "\n",
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values\n",
    "\n",
    "meta_features = [\n",
    "#     'count_sent', 'count_word', 'count_unique_word', 'count_letters',\n",
    "#        'count_punctuations',\n",
    "#     'count_words_upper',\n",
    "#     'count_words_title',\n",
    "#        'count_stopwords', 'mean_word_len', 'word_unique_percent',\n",
    "#        'punct_percent'\n",
    "#     ,\n",
    "#                  'count_swear_words'\n",
    "]\n",
    "\n",
    "X_meta_features = train_ling[meta_features]\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "X_meta_features = minmax_scale(X_meta_features)\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    x = np.array([[embeddings_index[vocabulary_inv[vocabulary['word']]] if word in vocabulary.keys() else len(vocabulary) - 1 for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_features = 30000\n",
    "maxlen = 100\n",
    "embed_size = 300\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/3\n",
      " - 42s - loss: 0.0550 - acc: 0.9804 - val_loss: 0.0512 - val_acc: 0.9810\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.984961 \n",
      "\n",
      "Epoch 2/3\n",
      " - 42s - loss: 0.0392 - acc: 0.9850 - val_loss: 0.0485 - val_acc: 0.9818\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.982992 \n",
      "\n",
      "Epoch 3/3\n",
      " - 41s - loss: 0.0305 - acc: 0.9882 - val_loss: 0.0533 - val_acc: 0.9811\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.980961 \n",
      "\n",
      "0.9809607034655509\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "def test_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    inp_meta = Input(shape=(len(meta_features),))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "    lstm1 = Bidirectional(CuDNNLSTM(80, return_sequences=True))(x)\n",
    "#     meta_dense = Dense(64)(inp_meta)\n",
    "    flatten = Flatten()(lstm1)\n",
    "    conc = concatenate([inp_meta, flatten])\n",
    "#     dense = Dense(7)(conc)\n",
    "#     lstm2 = Bidirectional(CuDNNLSTM(80, return_sequences=True))(dense)\n",
    "    output = Dense(units=6, activation='sigmoid')(conc)\n",
    "    model = Model(inputs=[inp, inp_meta], outputs=output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',\n",
    "#                   optimizer=Adam(lr = 0.001, \n",
    "# #                                  decay=0.2\n",
    "#                                 ),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def ensemble(model_f, num_ensembles, input_length):\n",
    "    models = [model_f() for i in range(0,num_ensembles)]\n",
    "    ensemble_input = Input(shape=(input_length,))\n",
    "    ensemble_meta_input = Input(shape=(len(meta_features),))\n",
    "    averaged = average([model([ensemble_input, ensemble_meta_input]) for model in models])\n",
    "    ensemble = Model(inputs=[ensemble_input, ensemble_meta_input], outputs=[averaged])\n",
    "    ensemble.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])    \n",
    "    return models, ensemble\n",
    "\n",
    "\n",
    "def simple_lstm_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "    lstm1 = Bidirectional(CuDNNLSTM(80, return_sequences=True))(x )\n",
    "#     lstm2 = Bidirectional(CuDNNLSTM(20, return_sequences=True))(lstm1)\n",
    "    avg_pool = GlobalAveragePooling1D()(lstm1)\n",
    "    max_pool = GlobalMaxPooling1D()(lstm1)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "#     dense = Dense(128)(conc)\n",
    "    output = Dense(units=6, activation='sigmoid')(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(CuDNNGRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model_func = test_model\n",
    "ensemble_it = False\n",
    "num_ensembles = 7\n",
    "train_ensemble_jointly_after = True\n",
    "epochs = 3\n",
    "batch_size = 128\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "X_train_meta, X_val_meta, y_tra2,y_val2 = train_test_split(X_meta_features, y_train, train_size=0.95, random_state=233)\n",
    "    \n",
    "if ensemble_it:\n",
    "    models, ensemble_model = ensemble(model_func, num_ensembles, maxlen)   \n",
    "    for model in models:\n",
    "        callbacks=[\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5),\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto'),\n",
    "        RocAucEvaluation(validation_data=([X_val, X_val_meta], y_val), interval=1)\n",
    "        ]\n",
    "\n",
    "        hist = model.fit([X_tra, X_train_meta], y_tra, batch_size=batch_size, epochs=epochs, validation_data=([X_val, X_val_meta], y_val),\n",
    "                         callbacks=callbacks, verbose=2)\n",
    "    y_val_pred = ensemble_model.predict([X_val, X_val_meta], batch_size=batch_size)\n",
    "    score = roc_auc_score(y_val, y_val_pred)\n",
    "    print(score)\n",
    "\n",
    "else:\n",
    "    model = model_func()\n",
    "    callbacks=[\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto'),\n",
    "    RocAucEvaluation(validation_data=([X_val, X_val_meta], y_val), interval=1)\n",
    "    ]\n",
    "    hist = model.fit([X_tra, X_train_meta], y_tra, batch_size=batch_size, epochs=epochs, validation_data=([X_val, X_val_meta], y_val),\n",
    "                     callbacks=callbacks, verbose=2)\n",
    "    y_val_pred = model.predict([X_val, X_val_meta], batch_size=batch_size)\n",
    "    score = roc_auc_score(y_val, y_val_pred)\n",
    "    print(score)\n",
    "\n",
    "if ensemble_it and train_ensemble_jointly_after:\n",
    "    epochs=1\n",
    "    callbacks=[\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5),\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto'),\n",
    "        RocAucEvaluation(validation_data=([X_val, X_val_meta], y_val), interval=1)\n",
    "        ]\n",
    "    hist = ensemble_model.fit([X_tra, X_train_meta], y_tra, batch_size=batch_size, epochs=epochs, validation_data=([X_val, X_val_meta], y_val),\n",
    "                     callbacks=[callbacks], verbose=2)\n",
    "    y_val_pred = model.predict([X_val, X_val_meta], batch_size=2)\n",
    "    score = roc_auc_score(y_val, y_val_pred)\n",
    "    print(score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks=[\n",
    "# keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5),\n",
    "# keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto'),\n",
    "# RocAucEvaluation(validation_data=([X_val, X_val_meta], y_val), interval=1)\n",
    "# ]\n",
    "# hist = model.fit([X_tra, X_train_meta], y_tra, batch_size=batch_size, epochs=100, validation_data=([X_val, X_val_meta], y_val),\n",
    "#                  callbacks=callbacks, verbose=2)\n",
    "# y_val_pred = model.predict([X_val, X_val_meta], batch_size=batch_size)\n",
    "# score = roc_auc_score(y_val, y_val_pred)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-802bce9d0271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(len(meta_features))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model.layers[4].get_weights()[0].mean()\n",
    "# print(len(meta_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size=128)\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = ensemble_model.predict(X_val, batch_size=128)\n",
    "score = roc_auc_score(y_val, y_val_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape, y_pred.shape, y_val.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
