{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "more_than_2_sequential_characters = re.compile(r'(.)\\1{3,}', flags=re.IGNORECASE)\n",
    "def preprocess(x):\n",
    "    return x.fillna(\"fillna\") \\\n",
    "    .map(lambda string: string.lower()) \\\n",
    "    .map(lambda string: more_than_2_sequential_characters.sub(r'\\1\\1', string)) \\\n",
    "    .map(lambda string: string.replace(\"f*ck\", \"fuck\")) \\\n",
    "    .map(lambda string: string.replace(\"b*tch\", \"bitch\")) \\\n",
    "    .map(lambda string: string.replace(\"c*nt\", \"cunt\")) \\\n",
    "    .values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ling = pd.read_csv(\"../data/\" + \"preprocessed/train_ling.csv\")\n",
    "\n",
    "EMBEDDING_FILE = '../../../embeddings/glove.42B.300d.txt'\n",
    "\n",
    "# train = pd.read_csv('../data/train.csv')\n",
    "# test = pd.read_csv('../data/test.csv')\n",
    "train = pd.read_csv('../data/preprocessed/train.csv')\n",
    "test = pd.read_csv('../data/preprocessed/test.csv')\n",
    "submission = pd.read_csv('../submissions/sample_submission.csv')\n",
    "\n",
    "X_train = preprocess(train[\"clean_text\"])\n",
    "# X_train_nostopwords = train[\"no_stopwords\"].apply(\" \".join).values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = preprocess(test[\"clean_text\"])\n",
    "# X_test_nostopwords = test[\"no_stopwords\"].apply(\" \".join).values\n",
    "\n",
    "# meta_features = ['count_sent', 'count_word', 'count_unique_word', 'count_letters',\n",
    "#        'count_punctuations', 'count_words_upper', 'count_words_title',\n",
    "#        'count_stopwords', 'mean_word_len', 'word_unique_percent',\n",
    "#        'punct_percent', 'count_swear_words']\n",
    "\n",
    "# X_meta_features = train_ling[meta_features]\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    x = np.array([[embeddings_index[vocabulary_inv[vocabulary['word']]] if word in vocabulary.keys() else len(vocabulary) - 1 for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>nlp</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmata</th>\n",
       "      <th>no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>['Explanation', '\\n', 'Why', 'the', 'edits', '...</td>\n",
       "      <td>['explanation', '\\n', 'why', 'the', 'edit', 'm...</td>\n",
       "      <td>['Explanation', '\\n', 'Why', 'edits', 'made', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>[\"D'aww\", '!', 'He', 'matches', 'this', 'backg...</td>\n",
       "      <td>[\"d'aww\", '!', '-PRON-', 'match', 'this', 'bac...</td>\n",
       "      <td>[\"D'aww\", '!', 'He', 'matches', 'background', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>['Hey', 'man', ',', 'I', \"'m\", 'really', 'not'...</td>\n",
       "      <td>['hey', 'man', ',', '-PRON-', 'be', 'really', ...</td>\n",
       "      <td>['Hey', 'man', ',', 'I', \"'m\", 'really', 'tryi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" more i can't make any real suggestions on im...</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>['\"', '\\n', 'More', '\\n', 'I', 'ca', \"n't\", 'm...</td>\n",
       "      <td>['\"', '\\n', 'more', '\\n', '-PRON-', 'can', 'no...</td>\n",
       "      <td>['\"', '\\n', 'More', '\\n', 'I', 'ca', \"n't\", 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>['You', ',', 'sir', ',', 'are', 'my', 'hero', ...</td>\n",
       "      <td>['-PRON-', ',', 'sir', ',', 'be', '-PRON-', 'h...</td>\n",
       "      <td>['You', ',', 'sir', ',', 'hero', '.', 'Any', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0           0  0000997932d777bf   \n",
       "1           1  000103f0d9cfb60f   \n",
       "2           2  000113f07ec002fd   \n",
       "3           3  0001b41b1c6bb37e   \n",
       "4           4  0001d958c54c6e35   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \\\n",
       "0        0       0       0              0   \n",
       "1        0       0       0              0   \n",
       "2        0       0       0              0   \n",
       "3        0       0       0              0   \n",
       "4        0       0       0              0   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  explanation why the edits made under my userna...   \n",
       "1  d'aww! he matches this background colour i'm s...   \n",
       "2  hey man, i'm really not trying to edit war. it...   \n",
       "3  \" more i can't make any real suggestions on im...   \n",
       "4  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                                 nlp  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  D'aww! He matches this background colour I'm s...   \n",
       "2  Hey man, I'm really not trying to edit war. It...   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  You, sir, are my hero. Any chance you remember...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['Explanation', '\\n', 'Why', 'the', 'edits', '...   \n",
       "1  [\"D'aww\", '!', 'He', 'matches', 'this', 'backg...   \n",
       "2  ['Hey', 'man', ',', 'I', \"'m\", 'really', 'not'...   \n",
       "3  ['\"', '\\n', 'More', '\\n', 'I', 'ca', \"n't\", 'm...   \n",
       "4  ['You', ',', 'sir', ',', 'are', 'my', 'hero', ...   \n",
       "\n",
       "                                             lemmata  \\\n",
       "0  ['explanation', '\\n', 'why', 'the', 'edit', 'm...   \n",
       "1  [\"d'aww\", '!', '-PRON-', 'match', 'this', 'bac...   \n",
       "2  ['hey', 'man', ',', '-PRON-', 'be', 'really', ...   \n",
       "3  ['\"', '\\n', 'more', '\\n', '-PRON-', 'can', 'no...   \n",
       "4  ['-PRON-', ',', 'sir', ',', 'be', '-PRON-', 'h...   \n",
       "\n",
       "                                        no_stopwords  \n",
       "0  ['Explanation', '\\n', 'Why', 'edits', 'made', ...  \n",
       "1  [\"D'aww\", '!', 'He', 'matches', 'background', ...  \n",
       "2  ['Hey', 'man', ',', 'I', \"'m\", 'really', 'tryi...  \n",
       "3  ['\"', '\\n', 'More', '\\n', 'I', 'ca', \"n't\", 'm...  \n",
       "4  ['You', ',', 'sir', ',', 'hero', '.', 'Any', '...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_features = 30000\n",
    "# max_features = 5159 #only this many with stopwords\n",
    "maxlen = 100\n",
    "embed_size = 300\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train_sequences, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test_sequences, maxlen=maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/2\n",
      "151592/151592 [==============================] - 195s 1ms/step - loss: 0.0478 - acc: 0.9824 - val_loss: 0.0456 - val_acc: 0.9831\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.986411 \n",
      "\n",
      "Epoch 2/2\n",
      "151592/151592 [==============================] - 194s 1ms/step - loss: 0.0371 - acc: 0.9854 - val_loss: 0.0462 - val_acc: 0.9829\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.986454 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/2\n",
      "151592/151592 [==============================] - 196s 1ms/step - loss: 0.0470 - acc: 0.9826 - val_loss: 0.0469 - val_acc: 0.9823\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.986523 \n",
      "\n",
      "Epoch 2/2\n",
      "151592/151592 [==============================] - 195s 1ms/step - loss: 0.0371 - acc: 0.9855 - val_loss: 0.0446 - val_acc: 0.9830\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.986579 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/2\n",
      "151592/151592 [==============================] - 195s 1ms/step - loss: 0.0474 - acc: 0.9826 - val_loss: 0.0456 - val_acc: 0.9827\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.985863 \n",
      "\n",
      "Epoch 2/2\n",
      "151592/151592 [==============================] - 194s 1ms/step - loss: 0.0371 - acc: 0.9855 - val_loss: 0.0443 - val_acc: 0.9831\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.986642 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/2\n",
      "151592/151592 [==============================] - 195s 1ms/step - loss: 0.0475 - acc: 0.9825 - val_loss: 0.0457 - val_acc: 0.9827\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.985982 \n",
      "\n",
      "Epoch 2/2\n",
      "151592/151592 [==============================] - 194s 1ms/step - loss: 0.0370 - acc: 0.9855 - val_loss: 0.0443 - val_acc: 0.9834\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.986489 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/2\n",
      "151592/151592 [==============================] - 195s 1ms/step - loss: 0.0473 - acc: 0.9825 - val_loss: 0.0452 - val_acc: 0.9831\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.985323 \n",
      "\n",
      "Epoch 2/2\n",
      "151592/151592 [==============================] - 194s 1ms/step - loss: 0.0371 - acc: 0.9855 - val_loss: 0.0464 - val_acc: 0.9825\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.986011 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/2\n",
      "151592/151592 [==============================] - 195s 1ms/step - loss: 0.0473 - acc: 0.9826 - val_loss: 0.0457 - val_acc: 0.9827\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.985694 \n",
      "\n",
      "Epoch 2/2\n",
      "151592/151592 [==============================] - 194s 1ms/step - loss: 0.0369 - acc: 0.9855 - val_loss: 0.0474 - val_acc: 0.9827\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.986792 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/2\n",
      "151592/151592 [==============================] - 196s 1ms/step - loss: 0.0474 - acc: 0.9825 - val_loss: 0.0460 - val_acc: 0.9825\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.986146 \n",
      "\n",
      "Epoch 2/2\n",
      "151592/151592 [==============================] - 194s 1ms/step - loss: 0.0370 - acc: 0.9855 - val_loss: 0.0465 - val_acc: 0.9819\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.985743 \n",
      "\n",
      "7979/7979 [==============================] - 137s 17ms/step\n",
      "0.9875603046662279\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "    lstm1 = Bidirectional(CuDNNLSTM(80, return_sequences=True))(x)\n",
    "#     attention = AttentionWithContext()(lstm1)\n",
    "#     lstm2 = Bidirectional(CuDNNLSTM(20, return_sequences=True))(lstm1)\n",
    "#     conv1 = Conv1D(128, 2, activation='relu')(lstm1)\n",
    "#     pool1 = MaxPool1D(2)(conv1)\n",
    "#     lstm2 = Bidirectional(CuDNNLSTM(80, return_sequences=True))(pool1)\n",
    "#     avg_pool = GlobalAveragePooling1D()(attention)\n",
    "#     max_pool = GlobalMaxPooling1D()(attention)\n",
    "#     conc = concatenate([avg_pool, max_pool])\n",
    "#     dense = Dense(128)(conc)\n",
    "    flatten = Flatten()(lstm1)\n",
    "    output = Dense(units=6, activation='sigmoid')(flatten)\n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def ensemble(model_f, num_ensembles, input_length):\n",
    "    models = [model_f() for i in range(0,num_ensembles)]\n",
    "    ensemble_input = Input(shape=(input_length,))\n",
    "    averaged = average([m(ensemble_input) for m in models])\n",
    "    ensemble = Model(inputs=[ensemble_input], outputs=[averaged])\n",
    "    ensemble.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])    \n",
    "    return models, ensemble\n",
    "\n",
    "\n",
    "def simple_lstm_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "    lstm1 = Bidirectional(CuDNNLSTM(80, return_sequences=True))(x )\n",
    "#     lstm2 = Bidirectional(CuDNNLSTM(20, return_sequences=True))(lstm1)\n",
    "    avg_pool = GlobalAveragePooling1D()(lstm1)\n",
    "    max_pool = GlobalMaxPooling1D()(lstm1)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "#     dense = Dense(128)(conc)\n",
    "    output = Dense(units=6, activation='sigmoid')(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(CuDNNGRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model_func = simple_lstm_model\n",
    "ensemble_it = True\n",
    "num_ensembles = 7\n",
    "train_ensemble_jointly_after = False\n",
    "epochs = 2\n",
    "batch_size = 16\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "\n",
    "if ensemble_it:\n",
    "    models, ensemble_model = ensemble(model_func, num_ensembles, maxlen)\n",
    "    for model in models:\n",
    "\n",
    "        RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "        hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                         callbacks=[RocAuc], verbose=1)\n",
    "    y_val_pred = ensemble_model.predict(X_val, batch_size=2, verbose=1)\n",
    "    score = roc_auc_score(y_val, y_val_pred)\n",
    "    print(score)\n",
    "\n",
    "else:\n",
    "    model = model_func()\n",
    "    RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "    hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                     callbacks=[RocAuc], verbose=1)\n",
    "    y_val_pred = model.predict(X_val, batch_size=2, verbose=1)\n",
    "    score = roc_auc_score(y_val, y_val_pred)\n",
    "    print(score)\n",
    "\n",
    "if ensemble_it and train_ensemble_jointly_after:\n",
    "    epochs=1\n",
    "    RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "    hist = ensemble_model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                     callbacks=[RocAuc], verbose=1)\n",
    "    y_val_pred = model.predict(X_val, batch_size=1, verbose=1)\n",
    "    score = roc_auc_score(y_val, y_val_pred)\n",
    "    print(score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "hist = ensemble_model.fit(X_tra, y_tra, batch_size=2, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=1, verbose=1)\n",
    "y_val_pred = model.predict(X_val, batch_size=1, verbose=1)\n",
    "score = roc_auc_score(y_val, y_val_pred)\n",
    "print(score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7979/7979 [==============================] - 69s 9ms/step\n",
      "0.9875603046662279\n"
     ]
    }
   ],
   "source": [
    "y_val_pred = ensemble_model.predict(X_val, batch_size=4, verbose=1)\n",
    "score = roc_auc_score(y_val, y_val_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ensemble_model.predict(x_test, batch_size=4, verbose=1)\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "submission.to_csv('clean_text_lstm_7_ensemble_98799valauc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape, y_pred.shape, y_val.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
