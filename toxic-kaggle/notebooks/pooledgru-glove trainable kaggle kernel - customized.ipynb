{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "# set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "from keras import backend as K, activations, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "import numpy as np\n",
    "\n",
    "class SelfAttention(Layer):\n",
    "\n",
    "    \"\"\"Just your regular densely-connected NN layer.\n",
    "\n",
    "    `Dense` implements the operation:\n",
    "    `output = activation(dot(input, kernel) + bias)`\n",
    "    where `activation` is the element-wise activation function\n",
    "    passed as the `activation` argument, `kernel` is a weights matrix\n",
    "    created by the layer, and `bias` is a bias vector created by the layer\n",
    "    (only applicable if `use_bias` is `True`).\n",
    "\n",
    "    Note: if the input to the layer has a rank greater than 2, then\n",
    "    it is flattened prior to the initial dot product with `kernel`.\n",
    "\n",
    "    # Example\n",
    "\n",
    "    ```python\n",
    "        # as first layer in a sequential model:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_shape=(16,)))\n",
    "        # now the model will take as input arrays of shape (*, 16)\n",
    "        # and output arrays of shape (*, 32)\n",
    "\n",
    "        # after the first layer, you don't need to specify\n",
    "        # the size of the input anymore:\n",
    "        model.add(Dense(32))\n",
    "    ```\n",
    "\n",
    "    # Arguments\n",
    "        units: Positive integer, dimensionality of the output space.\n",
    "        activation: Activation function to use\n",
    "            (see [activations](../activations.md)).\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. \"linear\" activation: `a(x) = x`).\n",
    "        use_bias: Boolean, whether the layer uses a bias vector.\n",
    "        kernel_initializer: Initializer for the `kernel` weights matrix\n",
    "            (see [initializers](../initializers.md)).\n",
    "        bias_initializer: Initializer for the bias vector\n",
    "            (see [initializers](../initializers.md)).\n",
    "        kernel_regularizer: Regularizer function applied to\n",
    "            the `kernel` weights matrix\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        bias_regularizer: Regularizer function applied to the bias vector\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        activity_regularizer: Regularizer function applied to\n",
    "            the output of the layer (its \"activation\").\n",
    "            (see [regularizer](../regularizers.md)).\n",
    "        kernel_constraint: Constraint function applied to\n",
    "            the `kernel` weights matrix\n",
    "            (see [constraints](../constraints.md)).\n",
    "        bias_constraint: Constraint function applied to the bias vector\n",
    "            (see [constraints](../constraints.md)).\n",
    "\n",
    "    # Input shape\n",
    "        nD tensor with shape: `(batch_size, ..., input_dim)`.\n",
    "        The most common situation would be\n",
    "        a 2D input with shape `(batch_size, input_dim)`.\n",
    "\n",
    "    # Output shape\n",
    "        nD tensor with shape: `(batch_size, ..., units)`.\n",
    "        For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
    "        the output would have shape `(batch_size, units)`.\n",
    "    \"\"\"\n",
    "\n",
    "    @interfaces.legacy_dense_support\n",
    "    def __init__(self, units,\n",
    "                 activation='tanh',\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 v_kernel_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 v_kernel_regularizer = None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 v_kernel_constraint=None,\n",
    "                 **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.v_kernel_initializer = initializers.get(v_kernel_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.v_kernel_regularizer = regularizers.get(v_kernel_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.v_kernel_constraint =  constraints.get(v_kernel_constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=2)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        self.kernel = self.add_weight(shape=(self.units, input_dim),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "\n",
    "        self.v_kernel = self.add_weight(shape=(input_dim, self.units),\n",
    "                                      initializer=self.v_kernel_initializer,\n",
    "                                      name='v_kernel',\n",
    "                                      regularizer=self.v_kernel_regularizer,\n",
    "                                      constraint=self.v_kernel_constraint)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(self.kernel.shape, inputs.shape, flush=True)\n",
    "        newdim = tuple([x for x in inputs.shape.as_list() if x != 1 and x is not None])\n",
    "        #newdim is now (15, 36). Reshape does not take batch size as an input dimension.\n",
    "        reshape_layer = Reshape(newdim) (inputs)\n",
    "        print(reshape_layer.shape)\n",
    "        output = K.dot(self.kernel, reshape_layer)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias)\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        v_kernel_output = K.dot(output, self.v_kernel)\n",
    "        return K.dot(inputs, activations.softmax(v_kernel_output))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) >= 2\n",
    "        assert input_shape[-1]\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] = self.units\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'activation': activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'v_kernel_initializer': initializers.serialize(self.v_kernel_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'v_kernel_regularizer': regularizers.serialize(self.v_kernel_regularizer),\n",
    "            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint),\n",
    "            'v_kernel_constraint':constraints.serialize(self.v_kernel_constraint)\n",
    "        }\n",
    "        base_config = super(SelfAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "more_than_2_sequential_characters = re.compile(r'(.)\\1{3,}', flags=re.IGNORECASE)\n",
    "def preprocess(x):\n",
    "    return x.fillna(\"fillna\") \\\n",
    "    .values\n",
    "#     .map(lambda string: string.lower()) \\\n",
    "#     .map(lambda string: more_than_2_sequential_characters.sub(r'\\1\\1', string)) \\\n",
    "#     .map(lambda string: string.replace(\"f*ck\", \"fuck\")) \\\n",
    "#     .map(lambda string: string.replace(\"b*tch\", \"bitch\")) \\\n",
    "#     .map(lambda string: string.replace(\"c*nt\", \"cunt\")) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ling = pd.read_csv(\"../data/\" + \"preprocessed/train_ling.csv\")\n",
    "\n",
    "EMBEDDING_FILE = '../../../embeddings/glove.42B.300d.txt'\n",
    "# EMBEDDING_FILE = '../../../embeddings/crawl-300d-2M.vec'\n",
    "\n",
    "# train = pd.read_csv('../data/train.csv')\n",
    "# test = pd.read_csv('../data/test.csv')\n",
    "train = pd.read_csv('../data/preprocessed/train.csv')\n",
    "test = pd.read_csv('../data/preprocessed/test.csv')\n",
    "submission = pd.read_csv('../submissions/sample_submission.csv')\n",
    "\n",
    "X_train = preprocess(train[\"comment_text\"])\n",
    "# X_train = preprocess(train[\"clean_text\"])\n",
    "# X_train_nostopwords = train[\"no_stopwords\"].apply(\" \".join).values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = preprocess(test[\"comment_text\"])\n",
    "# X_test = preprocess(test[\"clean_text\"])\n",
    "# X_test_nostopwords = test[\"no_stopwords\"].apply(\" \".join).values\n",
    "\n",
    "# meta_features = ['count_sent', 'count_word', 'count_unique_word', 'count_letters',\n",
    "#        'count_punctuations', 'count_words_upper', 'count_words_title',\n",
    "#        'count_stopwords', 'mean_word_len', 'word_unique_percent',\n",
    "#        'punct_percent', 'count_swear_words']\n",
    "\n",
    "# X_meta_features = train_ling[meta_features]\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    x = np.array([[embeddings_index[vocabulary_inv[vocabulary['word']]] if word in vocabulary.keys() else len(vocabulary) - 1 for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>nlp</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmata</th>\n",
       "      <th>no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>['Explanation', '\\n', 'Why', 'the', 'edits', '...</td>\n",
       "      <td>['explanation', '\\n', 'why', 'the', 'edit', 'm...</td>\n",
       "      <td>['Explanation', '\\n', 'Why', 'edits', 'made', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>[\"D'aww\", '!', 'He', 'matches', 'this', 'backg...</td>\n",
       "      <td>[\"d'aww\", '!', '-PRON-', 'match', 'this', 'bac...</td>\n",
       "      <td>[\"D'aww\", '!', 'He', 'matches', 'background', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>['Hey', 'man', ',', 'I', \"'m\", 'really', 'not'...</td>\n",
       "      <td>['hey', 'man', ',', '-PRON-', 'be', 'really', ...</td>\n",
       "      <td>['Hey', 'man', ',', 'I', \"'m\", 'really', 'tryi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" more i can't make any real suggestions on im...</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>['\"', '\\n', 'More', '\\n', 'I', 'ca', \"n't\", 'm...</td>\n",
       "      <td>['\"', '\\n', 'more', '\\n', '-PRON-', 'can', 'no...</td>\n",
       "      <td>['\"', '\\n', 'More', '\\n', 'I', 'ca', \"n't\", 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>['You', ',', 'sir', ',', 'are', 'my', 'hero', ...</td>\n",
       "      <td>['-PRON-', ',', 'sir', ',', 'be', '-PRON-', 'h...</td>\n",
       "      <td>['You', ',', 'sir', ',', 'hero', '.', 'Any', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0           0  0000997932d777bf   \n",
       "1           1  000103f0d9cfb60f   \n",
       "2           2  000113f07ec002fd   \n",
       "3           3  0001b41b1c6bb37e   \n",
       "4           4  0001d958c54c6e35   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \\\n",
       "0        0       0       0              0   \n",
       "1        0       0       0              0   \n",
       "2        0       0       0              0   \n",
       "3        0       0       0              0   \n",
       "4        0       0       0              0   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  explanation why the edits made under my userna...   \n",
       "1  d'aww! he matches this background colour i'm s...   \n",
       "2  hey man, i'm really not trying to edit war. it...   \n",
       "3  \" more i can't make any real suggestions on im...   \n",
       "4  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                                                 nlp  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...   \n",
       "1  D'aww! He matches this background colour I'm s...   \n",
       "2  Hey man, I'm really not trying to edit war. It...   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  You, sir, are my hero. Any chance you remember...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['Explanation', '\\n', 'Why', 'the', 'edits', '...   \n",
       "1  [\"D'aww\", '!', 'He', 'matches', 'this', 'backg...   \n",
       "2  ['Hey', 'man', ',', 'I', \"'m\", 'really', 'not'...   \n",
       "3  ['\"', '\\n', 'More', '\\n', 'I', 'ca', \"n't\", 'm...   \n",
       "4  ['You', ',', 'sir', ',', 'are', 'my', 'hero', ...   \n",
       "\n",
       "                                             lemmata  \\\n",
       "0  ['explanation', '\\n', 'why', 'the', 'edit', 'm...   \n",
       "1  [\"d'aww\", '!', '-PRON-', 'match', 'this', 'bac...   \n",
       "2  ['hey', 'man', ',', '-PRON-', 'be', 'really', ...   \n",
       "3  ['\"', '\\n', 'more', '\\n', '-PRON-', 'can', 'no...   \n",
       "4  ['-PRON-', ',', 'sir', ',', 'be', '-PRON-', 'h...   \n",
       "\n",
       "                                        no_stopwords  \n",
       "0  ['Explanation', '\\n', 'Why', 'edits', 'made', ...  \n",
       "1  [\"D'aww\", '!', 'He', 'matches', 'background', ...  \n",
       "2  ['Hey', 'man', ',', 'I', \"'m\", 'really', 'tryi...  \n",
       "3  ['\"', '\\n', 'More', '\\n', 'I', 'ca', \"n't\", 'm...  \n",
       "4  ['You', ',', 'sir', ',', 'hero', '.', 'Any', '...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_features = 30000\n",
    "# max_features = 5159 #only this many with stopwords\n",
    "maxlen = 100\n",
    "embed_size = 300\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train_sequences, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test_sequences, maxlen=maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove embeddings\n",
    "# def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "# embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "# all_embs = np.stack(embeddings_index.values())\n",
    "# emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "# word_index = tokenizer.word_index\n",
    "# nb_words = min(max_features, len(word_index))\n",
    "# embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "# for word, i in word_index.items():\n",
    "#     if i >= max_features: continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "\n",
    "#fasttest embeddings\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "\n",
    "\n",
    "class MinOrMax(Merge):\n",
    "    \"\"\"Layer that averages a list of inputs.\n",
    "    It takes as input a list of tensors,\n",
    "    all of the same shape, and returns\n",
    "    a single tensor (also of the same shape).\n",
    "    \"\"\"\n",
    "    \n",
    "    def _merge_function(self, inputs):\n",
    "        tf.c\n",
    "        output = inputs[0]\n",
    "        for i in range(0, len(inputs[0])):\n",
    "            higher_than_05 = tf.nn.relu(inputs[:,0] - 0.5)\n",
    "            nr_higher_than_05 = tf.count_nonzero(higher_than_05)\n",
    "            if nr_higher_than_05 > len(inputs[:,0]): #take max\n",
    "                \n",
    "            output += inputs[i]\n",
    "            return output / len(inputs)\n",
    "    \n",
    "def min_or_max(inputs, **kwargs):\n",
    "    return Median(**kwargs)(inputs)\n",
    "\n",
    "def reduce_var(x, axis=None, keepdims=False):\n",
    "    \"\"\"Variance of a tensor, alongside the specified axis.\n",
    "\n",
    "    # Arguments\n",
    "        x: A tensor or variable.\n",
    "        axis: An integer, the axis to compute the variance.\n",
    "        keepdims: A boolean, whether to keep the dimensions or not.\n",
    "            If `keepdims` is `False`, the rank of the tensor is reduced\n",
    "            by 1. If `keepdims` is `True`,\n",
    "            the reduced dimension is retained with length 1.\n",
    "\n",
    "    # Returns\n",
    "        A tensor with the variance of elements of `x`.\n",
    "    \"\"\"\n",
    "    m = tf.reduce_mean(x, axis=axis, keep_dims=True)\n",
    "    devs_squared = tf.square(x - m)\n",
    "    return tf.reduce_mean(devs_squared, axis=axis, keep_dims=keepdims)\n",
    "\n",
    "def reduce_std(x, axis=None, keepdims=False):\n",
    "    \"\"\"Standard deviation of a tensor, alongside the specified axis.\n",
    "\n",
    "    # Arguments\n",
    "        x: A tensor or variable.\n",
    "        axis: An integer, the axis to compute the standard deviation.\n",
    "        keepdims: A boolean, whether to keep the dimensions or not.\n",
    "            If `keepdims` is `False`, the rank of the tensor is reduced\n",
    "            by 1. If `keepdims` is `True`,\n",
    "            the reduced dimension is retained with length 1.\n",
    "\n",
    "    # Returns\n",
    "        A tensor with the standard deviation of elements of `x`.\n",
    "    \"\"\"\n",
    "    return tf.sqrt(reduce_var(x, axis=axis, keepdims=keepdims))\n",
    "\n",
    "\n",
    "# class MLGaussian(Merge):\n",
    "    \n",
    "#     def _merge_function(self, inputs):\n",
    "#         std = reduce_std(inputs)\n",
    "#         mean = average(inputs)\n",
    "        \n",
    "    \n",
    "\n",
    "class Median(Merge):\n",
    "    \"\"\"Layer that averages a list of inputs.\n",
    "    It takes as input a list of tensors,\n",
    "    all of the same shape, and returns\n",
    "    a single tensor (also of the same shape).\n",
    "    \"\"\"\n",
    "\n",
    "    def _merge_function(self, inputs):\n",
    "        return tf.contrib.distributions.percentile(inputs, 50.0)\n",
    "    \n",
    "def median(inputs, **kwargs):\n",
    "    return Median(**kwargs)(inputs)\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/2\n",
      "151592/151592 [==============================] - 133s 876us/step - loss: 0.0489 - acc: 0.9821 - val_loss: 0.0459 - val_acc: 0.9827\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.985191 \n",
      "\n",
      "Epoch 2/2\n",
      "151592/151592 [==============================] - 131s 865us/step - loss: 0.0395 - acc: 0.9847 - val_loss: 0.0441 - val_acc: 0.9835\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.986591 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/2\n",
      "151592/151592 [==============================] - 132s 871us/step - loss: 0.0490 - acc: 0.9820 - val_loss: 0.0462 - val_acc: 0.9827\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.986279 \n",
      "\n",
      "Epoch 2/2\n",
      "151592/151592 [==============================] - 131s 867us/step - loss: 0.0390 - acc: 0.9850 - val_loss: 0.0448 - val_acc: 0.9828\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.987510 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/2\n",
      "151592/151592 [==============================] - 132s 872us/step - loss: 0.0485 - acc: 0.9823 - val_loss: 0.0465 - val_acc: 0.9825\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.985828 \n",
      "\n",
      "Epoch 2/2\n",
      "151592/151592 [==============================] - 132s 869us/step - loss: 0.0391 - acc: 0.9847 - val_loss: 0.0448 - val_acc: 0.9831\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.987234 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/2\n",
      "151592/151592 [==============================] - 133s 874us/step - loss: 0.0488 - acc: 0.9820 - val_loss: 0.0454 - val_acc: 0.9829\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.985981 \n",
      "\n",
      "Epoch 2/2\n",
      "151592/151592 [==============================] - 132s 869us/step - loss: 0.0389 - acc: 0.9849 - val_loss: 0.0445 - val_acc: 0.9833\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.987500 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/2\n",
      "151592/151592 [==============================] - 132s 873us/step - loss: 0.0496 - acc: 0.9819 - val_loss: 0.0464 - val_acc: 0.9822\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.984691 \n",
      "\n",
      "Epoch 2/2\n",
      "151592/151592 [==============================] - 132s 868us/step - loss: 0.0398 - acc: 0.9845 - val_loss: 0.0451 - val_acc: 0.9831\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.986752 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/2\n",
      "151592/151592 [==============================] - 132s 873us/step - loss: 0.0502 - acc: 0.9817 - val_loss: 0.0465 - val_acc: 0.9824\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.984469 \n",
      "\n",
      "Epoch 2/2\n",
      "151592/151592 [==============================] - 131s 866us/step - loss: 0.0396 - acc: 0.9847 - val_loss: 0.0451 - val_acc: 0.9830\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.986208 \n",
      "\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/2\n",
      "151592/151592 [==============================] - 132s 870us/step - loss: 0.0491 - acc: 0.9820 - val_loss: 0.0480 - val_acc: 0.9823\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.985398 \n",
      "\n",
      "Epoch 2/2\n",
      " 42016/151592 [=======>......................] - ETA: 1:33 - loss: 0.0386 - acc: 0.9854"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_model():    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "    lstm1 = Bidirectional(CuDNNLSTM(85, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(lstm1)\n",
    "    max_pool = GlobalMaxPooling1D()(lstm1)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    output = Dense(units=6, activation='sigmoid')(conc)\n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adamax(lr=0.005),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def ensemble(model_f, num_ensembles, input_length):\n",
    "    models = [model_f() for i in range(0,num_ensembles)]\n",
    "    ensemble_input = Input(shape=(input_length,))\n",
    "#     averaged = average([m(ensemble_input) for m in models])\n",
    "    averaged = median([m(ensemble_input) for m in models])\n",
    "    ensemble = Model(inputs=[ensemble_input], outputs=[averaged])\n",
    "    ensemble.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])    \n",
    "    return models, ensemble\n",
    "\n",
    "\n",
    "def simple_lstm_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "    lstm1 = Bidirectional(CuDNNLSTM(80, return_sequences=True))(x )\n",
    "    att = SelfAttention(50)(lstm1)\n",
    "    output = Dense(units=6, activation='sigmoid')(att)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def highest_current_model(): # 7 ensemble, glove.42B.300d, seed42, 2 epochs,batch size 32 with median ensemble val auc 0.9877301793160443\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "    lstm1 = Bidirectional(CuDNNLSTM(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(lstm1)\n",
    "    max_pool = GlobalMaxPooling1D()(lstm1)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    output = Dense(units=6, activation='sigmoid')(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adamax(lr=0.005),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_func = test_model\n",
    "ensemble_it = True\n",
    "num_ensembles = 7\n",
    "train_ensemble_jointly_after = False\n",
    "epochs = 2\n",
    "batch_size = 32\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "\n",
    "if ensemble_it:\n",
    "    models, ensemble_model = ensemble(model_func, num_ensembles, maxlen)\n",
    "    for model in models:\n",
    "\n",
    "        RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "        hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                         callbacks=[RocAuc], verbose=1)\n",
    "    y_val_pred = ensemble_model.predict(X_val, batch_size=2, verbose=1)\n",
    "    score = roc_auc_score(y_val, y_val_pred)\n",
    "    print(score)\n",
    "\n",
    "else:\n",
    "    model = model_func()\n",
    "    model.summary()\n",
    "    RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "    hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                     callbacks=[RocAuc], verbose=1)\n",
    "    y_val_pred = model.predict(X_val, batch_size=2, verbose=1)\n",
    "    score = roc_auc_score(y_val, y_val_pred)\n",
    "    print(score)\n",
    "\n",
    "if ensemble_it and train_ensemble_jointly_after:\n",
    "    epochs=1\n",
    "    RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "    hist = ensemble_model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                     callbacks=[RocAuc], verbose=1)\n",
    "    y_val_pred = model.predict(X_val, batch_size=1, verbose=1)\n",
    "    score = roc_auc_score(y_val, y_val_pred)\n",
    "    print(score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs=1\n",
    "# RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "# hist = ensemble_model.fit(X_tra, y_tra, batch_size=2, epochs=epochs, validation_data=(X_val, y_val),\n",
    "#                  callbacks=[RocAuc], verbose=1, verbose=1)\n",
    "# y_val_pred = model.predict(X_val, batch_size=1, verbose=1)\n",
    "# score = roc_auc_score(y_val, y_val_pred)\n",
    "# print(score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_val_pred = ensemble_model.predict(X_val, batch_size=1, verbose=1)\n",
    "score = roc_auc_score(y_val, y_val_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission = True\n",
    "if create_submission:\n",
    "    y_pred = ensemble_model.predict(x_test, batch_size=16, verbose=1)\n",
    "    submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "    submission.to_csv('clean_text_lstm_17_ensemble_adamax005.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape, y_pred.shape, y_val.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
