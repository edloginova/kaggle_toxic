{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.utils import class_weight\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, Activation, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../models/\"\n",
    "DATA_PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_PATH + \"preprocessed/train_ling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features = ['count_sent', 'count_word', 'count_unique_word', 'count_letters',\n",
    "       'count_punctuations', 'count_words_upper', 'count_words_title',\n",
    "       'count_stopwords', 'mean_word_len', 'word_unique_percent',\n",
    "       'punct_percent', 'count_swear_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>...</th>\n",
       "      <th>count_unique_word</th>\n",
       "      <th>count_letters</th>\n",
       "      <th>count_punctuations</th>\n",
       "      <th>count_words_upper</th>\n",
       "      <th>count_words_title</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>word_unique_percent</th>\n",
       "      <th>punct_percent</th>\n",
       "      <th>count_swear_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>264</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>5.162791</td>\n",
       "      <td>95.348837</td>\n",
       "      <td>23.255814</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>112</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5.588235</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>70.588235</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>233</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>92.857143</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>622</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>55</td>\n",
       "      <td>4.486726</td>\n",
       "      <td>72.566372</td>\n",
       "      <td>18.584071</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4.230769</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>38.461538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0           0  0000997932d777bf   \n",
       "1           1  000103f0d9cfb60f   \n",
       "2           2  000113f07ec002fd   \n",
       "3           3  0001b41b1c6bb37e   \n",
       "4           4  0001d958c54c6e35   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  count_sent        ...          \\\n",
       "0        0       0       0              0           2        ...           \n",
       "1        0       0       0              0           1        ...           \n",
       "2        0       0       0              0           1        ...           \n",
       "3        0       0       0              0           5        ...           \n",
       "4        0       0       0              0           1        ...           \n",
       "\n",
       "   count_unique_word  count_letters  count_punctuations  count_words_upper  \\\n",
       "0                 41            264                  10                  2   \n",
       "1                 17            112                  12                  1   \n",
       "2                 39            233                   6                  0   \n",
       "3                 82            622                  21                  5   \n",
       "4                 13             67                   5                  0   \n",
       "\n",
       "   count_words_title  count_stopwords  mean_word_len  word_unique_percent  \\\n",
       "0                 11               16       5.162791            95.348837   \n",
       "1                  3                2       5.588235           100.000000   \n",
       "2                  2               19       4.571429            92.857143   \n",
       "3                  7               55       4.486726            72.566372   \n",
       "4                  2                5       4.230769           100.000000   \n",
       "\n",
       "   punct_percent  count_swear_words  \n",
       "0      23.255814                  0  \n",
       "1      70.588235                  0  \n",
       "2      14.285714                  0  \n",
       "3      18.584071                  0  \n",
       "4      38.461538                  0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    x = np.array([[embeddings_index[vocabulary_inv[vocabulary['word']]] if word in vocabulary.keys() else len(vocabulary) - 1 for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=18400)\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "sequences = tokenizer.texts_to_sequences(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = tokenizer.word_index\n",
    "vocabulary_inv = {v:k for k, v in vocabulary.items()}\n",
    "embeddings_index = {}\n",
    "EMBEDDING_DIM = 100\n",
    "f = open(\"../../../embeddings/glove.6B.\" + str(EMBEDDING_DIM) + \"d.txt\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "embedding_matrix = np.zeros((len(vocabulary) + 1, EMBEDDING_DIM))\n",
    "embedding_matrix[-1] = np.random.rand(EMBEDDING_DIM) # oov-vector\n",
    "for word, i in vocabulary.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i - 1] = embedding_vector\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=200, trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_dev, y_train, y_dev = train_test_split(train, targets, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200\n",
    "\n",
    "x_train_texts = tokenizer.texts_to_sequences(x_train['comment_text'])\n",
    "x_train_texts = pad_sequences(x_train_texts, maxlen=max_length, padding='post')\n",
    "\n",
    "x_dev_texts = tokenizer.texts_to_sequences(x_dev['comment_text'])\n",
    "x_dev_texts = pad_sequences(x_dev_texts, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 100)     21033800    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        (None, 10)           4480        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 12)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 22)           0           cu_dnnlstm_1[0][0]               \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           1150        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5)            255         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 5)            255         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 5)            255         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 5)            255         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 5)            255         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 5)            255         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_1 (Dense)                (None, 1)            6           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_2 (Dense)                (None, 1)            6           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_3 (Dense)                (None, 1)            6           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_4 (Dense)                (None, 1)            6           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_5 (Dense)                (None, 1)            6           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_6 (Dense)                (None, 1)            6           dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 21,040,996\n",
      "Trainable params: 7,196\n",
      "Non-trainable params: 21,033,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(max_length,), dtype='float32')\n",
    "metadata_input = Input(shape=(len(meta_features),), dtype='float32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "lstm = keras.layers.CuDNNLSTM(10, return_sequences=False)(embedded_sequences)\n",
    "concatenated_data = Concatenate(axis=1)([lstm, metadata_input])\n",
    "dense_1 = Dense(50, activation='relu')(concatenated_data)\n",
    "dense_output_1 = Dense(5, activation='relu')(dense_1)\n",
    "dense_output_2 = Dense(5, activation='relu')(dense_1)\n",
    "dense_output_3 = Dense(5, activation='relu')(dense_1)\n",
    "dense_output_4 = Dense(5, activation='relu')(dense_1)\n",
    "dense_output_5 = Dense(5, activation='relu')(dense_1)\n",
    "dense_output_6 = Dense(5, activation='relu')(dense_1)\n",
    "output_1 = Dense(units=1, activation='sigmoid', name = 'output_1')(dense_output_1)\n",
    "output_2 = Dense(units=1, activation='sigmoid', name = 'output_2')(dense_output_2)\n",
    "output_3 = Dense(units=1, activation='sigmoid', name = 'output_3')(dense_output_3)\n",
    "output_4 = Dense(units=1, activation='sigmoid', name = 'output_4')(dense_output_4)\n",
    "output_5 = Dense(units=1, activation='sigmoid', name = 'output_5')(dense_output_5)\n",
    "output_6 = Dense(units=1, activation='sigmoid', name = 'output_6')(dense_output_6)\n",
    "model = Model(inputs=[sequence_input,metadata_input], outputs=[output_1, output_2, output_3, output_4, output_5, output_6])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_targets_train = [y_train[:, i] for i in range(0, y_train.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_targets_dev = [y_dev[:, i] for i in range(0, y_dev.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_class_weights = [class_weight.compute_class_weight('balanced', np.unique(separate_targets_train[i]),separate_targets_train[i]) for i in range(0, len(separate_targets_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_class_weights_dict = [{0:x[0], 1: x[1]} for x in separate_class_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 0.5530365294542862, 1: 5.213732262882749},\n",
       " {0: 0.5050140157337915, 1: 50.360234445446345},\n",
       " {0: 0.527848137156683, 1: 9.477261157305277},\n",
       " {0: 0.5015356017134083, 1: 163.30263157894737},\n",
       " {0: 0.5258353654517893, 1: 10.176658163265307},\n",
       " {0: 0.5043937286635478, 1: 57.39928057553957}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separate_class_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.55303653, 5.21373226]),\n",
       " array([ 0.50501402, 50.36023445]),\n",
       " array([0.52784814, 9.47726116]),\n",
       " array([  0.5015356 , 163.30263158]),\n",
       " array([ 0.52583537, 10.17665816]),\n",
       " array([ 0.50439373, 57.39928058])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separate_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_class_weights = {'output_' + str(i + 1): x for i, x in enumerate(separate_class_weights_dict)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_targets_train_ = [y_train[i, :] for i in range(0, y_train.shape[0])]\n",
    "separate_targets_dev_ = [y_dev[i, :] for i in range(0, y_dev.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111699"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"../models/contextual_lstm_specialized_dense-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 111699 samples, validate on 47872 samples\n",
      "Epoch 1/20\n",
      "111699/111699 [==============================] - 76s 681us/step - loss: 3.2291 - output_1_loss: 0.5018 - output_2_loss: 0.4365 - output_3_loss: 0.5059 - output_4_loss: 0.5864 - output_5_loss: 0.4580 - output_6_loss: 0.7405 - output_1_acc: 0.8594 - output_2_acc: 0.8911 - output_3_acc: 0.8503 - output_4_acc: 0.8332 - output_5_acc: 0.8752 - output_6_acc: 0.1657 - val_loss: 2.3942 - val_output_1_loss: 0.3881 - val_output_2_loss: 0.1453 - val_output_3_loss: 0.2474 - val_output_4_loss: 0.5902 - val_output_5_loss: 0.3307 - val_output_6_loss: 0.6925 - val_output_1_acc: 0.9373 - val_output_2_acc: 0.9665 - val_output_3_acc: 0.9427 - val_output_4_acc: 0.8906 - val_output_5_acc: 0.9501 - val_output_6_acc: 0.0124\n",
      "\n",
      "Epoch 00001: val_loss improved from -inf to 2.39423, saving model to ../models/contextual_lstm_specialized_dense-01-2.39.hdf5\n",
      "Epoch 2/20\n",
      "111699/111699 [==============================] - 76s 681us/step - loss: 3.2622 - output_1_loss: 0.5057 - output_2_loss: 0.5045 - output_3_loss: 0.4955 - output_4_loss: 0.5810 - output_5_loss: 0.4515 - output_6_loss: 0.7239 - output_1_acc: 0.8610 - output_2_acc: 0.8836 - output_3_acc: 0.8710 - output_4_acc: 0.8400 - output_5_acc: 0.8881 - output_6_acc: 0.3670 - val_loss: 2.9050 - val_output_1_loss: 0.4353 - val_output_2_loss: 0.2969 - val_output_3_loss: 0.4637 - val_output_4_loss: 0.6249 - val_output_5_loss: 0.4022 - val_output_6_loss: 0.6819 - val_output_1_acc: 0.8988 - val_output_2_acc: 0.9348 - val_output_3_acc: 0.9125 - val_output_4_acc: 0.7662 - val_output_5_acc: 0.9269 - val_output_6_acc: 0.9910\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.39423 to 2.90502, saving model to ../models/contextual_lstm_specialized_dense-02-2.91.hdf5\n",
      "Epoch 3/20\n",
      "111699/111699 [==============================] - 76s 680us/step - loss: 2.9902 - output_1_loss: 0.4874 - output_2_loss: 0.3588 - output_3_loss: 0.4701 - output_4_loss: 0.5167 - output_5_loss: 0.4320 - output_6_loss: 0.7251 - output_1_acc: 0.8685 - output_2_acc: 0.9049 - output_3_acc: 0.8822 - output_4_acc: 0.8698 - output_5_acc: 0.8976 - output_6_acc: 0.2643 - val_loss: 2.4035 - val_output_1_loss: 0.4244 - val_output_2_loss: 0.1636 - val_output_3_loss: 0.2268 - val_output_4_loss: 0.4504 - val_output_5_loss: 0.4566 - val_output_6_loss: 0.6817 - val_output_1_acc: 0.8665 - val_output_2_acc: 0.9599 - val_output_3_acc: 0.9494 - val_output_4_acc: 0.9394 - val_output_5_acc: 0.9265 - val_output_6_acc: 0.9910\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/20\n",
      "111699/111699 [==============================] - 76s 680us/step - loss: 3.0518 - output_1_loss: 0.4818 - output_2_loss: 0.4683 - output_3_loss: 0.4652 - output_4_loss: 0.4845 - output_5_loss: 0.4284 - output_6_loss: 0.7236 - output_1_acc: 0.8642 - output_2_acc: 0.8654 - output_3_acc: 0.8753 - output_4_acc: 0.8800 - output_5_acc: 0.9015 - output_6_acc: 0.5026 - val_loss: 2.6258 - val_output_1_loss: 0.4169 - val_output_2_loss: 0.4568 - val_output_3_loss: 0.2943 - val_output_4_loss: 0.4235 - val_output_5_loss: 0.3432 - val_output_6_loss: 0.6911 - val_output_1_acc: 0.9226 - val_output_2_acc: 0.9401 - val_output_3_acc: 0.9343 - val_output_4_acc: 0.9416 - val_output_5_acc: 0.9384 - val_output_6_acc: 0.9910\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/20\n",
      "111699/111699 [==============================] - 76s 679us/step - loss: 2.7605 - output_1_loss: 0.4657 - output_2_loss: 0.2757 - output_3_loss: 0.4272 - output_4_loss: 0.4797 - output_5_loss: 0.4190 - output_6_loss: 0.6933 - output_1_acc: 0.8691 - output_2_acc: 0.9277 - output_3_acc: 0.8968 - output_4_acc: 0.8848 - output_5_acc: 0.9042 - output_6_acc: 0.3433 - val_loss: 2.4962 - val_output_1_loss: 0.4526 - val_output_2_loss: 0.2933 - val_output_3_loss: 0.2733 - val_output_4_loss: 0.4043 - val_output_5_loss: 0.3842 - val_output_6_loss: 0.6885 - val_output_1_acc: 0.8763 - val_output_2_acc: 0.9260 - val_output_3_acc: 0.9175 - val_output_4_acc: 0.9414 - val_output_5_acc: 0.9068 - val_output_6_acc: 0.9910\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/20\n",
      "111699/111699 [==============================] - 76s 680us/step - loss: 2.7688 - output_1_loss: 0.4754 - output_2_loss: 0.2886 - output_3_loss: 0.4257 - output_4_loss: 0.4662 - output_5_loss: 0.4189 - output_6_loss: 0.6941 - output_1_acc: 0.8767 - output_2_acc: 0.9173 - output_3_acc: 0.8851 - output_4_acc: 0.8923 - output_5_acc: 0.9063 - output_6_acc: 0.6437 - val_loss: 2.5670 - val_output_1_loss: 0.4291 - val_output_2_loss: 0.4274 - val_output_3_loss: 0.3061 - val_output_4_loss: 0.3695 - val_output_5_loss: 0.3400 - val_output_6_loss: 0.6951 - val_output_1_acc: 0.9014 - val_output_2_acc: 0.9398 - val_output_3_acc: 0.9327 - val_output_4_acc: 0.9408 - val_output_5_acc: 0.9370 - val_output_6_acc: 0.0155\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/20\n",
      "111699/111699 [==============================] - 76s 681us/step - loss: 2.7211 - output_1_loss: 0.4566 - output_2_loss: 0.2837 - output_3_loss: 0.3894 - output_4_loss: 0.4894 - output_5_loss: 0.4087 - output_6_loss: 0.6933 - output_1_acc: 0.8782 - output_2_acc: 0.9290 - output_3_acc: 0.9088 - output_4_acc: 0.8857 - output_5_acc: 0.9146 - output_6_acc: 0.4581 - val_loss: 2.0022 - val_output_1_loss: 0.3766 - val_output_2_loss: 0.1452 - val_output_3_loss: 0.2517 - val_output_4_loss: 0.2410 - val_output_5_loss: 0.2899 - val_output_6_loss: 0.6979 - val_output_1_acc: 0.9241 - val_output_2_acc: 0.9643 - val_output_3_acc: 0.9402 - val_output_4_acc: 0.9531 - val_output_5_acc: 0.9427 - val_output_6_acc: 0.0094\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/20\n",
      "111699/111699 [==============================] - 76s 680us/step - loss: 2.6608 - output_1_loss: 0.4548 - output_2_loss: 0.2580 - output_3_loss: 0.3795 - output_4_loss: 0.4575 - output_5_loss: 0.4091 - output_6_loss: 0.7020 - output_1_acc: 0.8762 - output_2_acc: 0.9338 - output_3_acc: 0.9094 - output_4_acc: 0.8830 - output_5_acc: 0.9120 - output_6_acc: 0.2309 - val_loss: 2.4131 - val_output_1_loss: 0.4419 - val_output_2_loss: 0.2463 - val_output_3_loss: 0.3397 - val_output_4_loss: 0.3200 - val_output_5_loss: 0.3685 - val_output_6_loss: 0.6966 - val_output_1_acc: 0.8624 - val_output_2_acc: 0.9420 - val_output_3_acc: 0.9260 - val_output_4_acc: 0.9279 - val_output_5_acc: 0.9219 - val_output_6_acc: 0.0130\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/20\n",
      "111699/111699 [==============================] - 76s 680us/step - loss: 2.5646 - output_1_loss: 0.4380 - output_2_loss: 0.2454 - output_3_loss: 0.3606 - output_4_loss: 0.4297 - output_5_loss: 0.3892 - output_6_loss: 0.7019 - output_1_acc: 0.8674 - output_2_acc: 0.9359 - output_3_acc: 0.8986 - output_4_acc: 0.8822 - output_5_acc: 0.8968 - output_6_acc: 0.1451 - val_loss: 2.8910 - val_output_1_loss: 0.4821 - val_output_2_loss: 0.3348 - val_output_3_loss: 0.4509 - val_output_4_loss: 0.4780 - val_output_5_loss: 0.4476 - val_output_6_loss: 0.6977 - val_output_1_acc: 0.8140 - val_output_2_acc: 0.8807 - val_output_3_acc: 0.8247 - val_output_4_acc: 0.8677 - val_output_5_acc: 0.8464 - val_output_6_acc: 0.0090\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/20\n",
      "111699/111699 [==============================] - 76s 680us/step - loss: 2.4467 - output_1_loss: 0.4246 - output_2_loss: 0.2342 - output_3_loss: 0.3396 - output_4_loss: 0.3826 - output_5_loss: 0.3700 - output_6_loss: 0.6956 - output_1_acc: 0.8595 - output_2_acc: 0.9323 - output_3_acc: 0.8883 - output_4_acc: 0.8883 - output_5_acc: 0.8910 - output_6_acc: 0.3966 - val_loss: 2.8996 - val_output_1_loss: 0.5384 - val_output_2_loss: 0.2281 - val_output_3_loss: 0.4178 - val_output_4_loss: 0.5124 - val_output_5_loss: 0.5078 - val_output_6_loss: 0.6952 - val_output_1_acc: 0.7878 - val_output_2_acc: 0.9317 - val_output_3_acc: 0.8371 - val_output_4_acc: 0.8245 - val_output_5_acc: 0.8065 - val_output_6_acc: 0.0522\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n",
      "111699/111699 [==============================] - 76s 681us/step - loss: 2.3151 - output_1_loss: 0.4112 - output_2_loss: 0.2059 - output_3_loss: 0.3328 - output_4_loss: 0.3842 - output_5_loss: 0.3591 - output_6_loss: 0.6218 - output_1_acc: 0.8573 - output_2_acc: 0.9247 - output_3_acc: 0.8845 - output_4_acc: 0.8767 - output_5_acc: 0.8860 - output_6_acc: 0.5175 - val_loss: 2.4924 - val_output_1_loss: 0.5322 - val_output_2_loss: 0.2658 - val_output_3_loss: 0.4556 - val_output_4_loss: 0.4528 - val_output_5_loss: 0.4262 - val_output_6_loss: 0.3599 - val_output_1_acc: 0.7961 - val_output_2_acc: 0.9043 - val_output_3_acc: 0.8130 - val_output_4_acc: 0.8236 - val_output_5_acc: 0.8389 - val_output_6_acc: 0.8687\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/20\n",
      "111699/111699 [==============================] - 76s 681us/step - loss: 2.2057 - output_1_loss: 0.4087 - output_2_loss: 0.1932 - output_3_loss: 0.3266 - output_4_loss: 0.4097 - output_5_loss: 0.3572 - output_6_loss: 0.5103 - output_1_acc: 0.8668 - output_2_acc: 0.9220 - output_3_acc: 0.8931 - output_4_acc: 0.8805 - output_5_acc: 0.8896 - output_6_acc: 0.8699 - val_loss: 1.6910 - val_output_1_loss: 0.3780 - val_output_2_loss: 0.1916 - val_output_3_loss: 0.2426 - val_output_4_loss: 0.2540 - val_output_5_loss: 0.3430 - val_output_6_loss: 0.2818 - val_output_1_acc: 0.8605 - val_output_2_acc: 0.9372 - val_output_3_acc: 0.9229 - val_output_4_acc: 0.9273 - val_output_5_acc: 0.8825 - val_output_6_acc: 0.8912\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/20\n",
      "111699/111699 [==============================] - 76s 679us/step - loss: 2.1479 - output_1_loss: 0.3999 - output_2_loss: 0.1909 - output_3_loss: 0.3199 - output_4_loss: 0.4023 - output_5_loss: 0.3464 - output_6_loss: 0.4884 - output_1_acc: 0.8698 - output_2_acc: 0.9297 - output_3_acc: 0.8944 - output_4_acc: 0.8855 - output_5_acc: 0.8903 - output_6_acc: 0.8640 - val_loss: 1.9803 - val_output_1_loss: 0.4380 - val_output_2_loss: 0.1817 - val_output_3_loss: 0.4029 - val_output_4_loss: 0.3723 - val_output_5_loss: 0.3369 - val_output_6_loss: 0.2487 - val_output_1_acc: 0.8380 - val_output_2_acc: 0.9390 - val_output_3_acc: 0.8411 - val_output_4_acc: 0.8932 - val_output_5_acc: 0.8818 - val_output_6_acc: 0.8961\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n",
      "111699/111699 [==============================] - 76s 681us/step - loss: 1.9759 - output_1_loss: 0.3928 - output_2_loss: 0.1894 - output_3_loss: 0.3104 - output_4_loss: 0.3211 - output_5_loss: 0.3383 - output_6_loss: 0.4239 - output_1_acc: 0.8727 - output_2_acc: 0.9351 - output_3_acc: 0.8950 - output_4_acc: 0.8962 - output_5_acc: 0.8857 - output_6_acc: 0.8677 - val_loss: 3.2352 - val_output_1_loss: 0.5029 - val_output_2_loss: 0.3055 - val_output_3_loss: 0.4681 - val_output_4_loss: 0.6865 - val_output_5_loss: 0.5298 - val_output_6_loss: 0.7424 - val_output_1_acc: 0.8100 - val_output_2_acc: 0.8903 - val_output_3_acc: 0.8214 - val_output_4_acc: 0.7366 - val_output_5_acc: 0.7978 - val_output_6_acc: 0.6260\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.90502 to 3.23516, saving model to ../models/contextual_lstm_specialized_dense-14-3.24.hdf5\n",
      "Epoch 15/20\n",
      "111699/111699 [==============================] - 76s 680us/step - loss: 1.8988 - output_1_loss: 0.3828 - output_2_loss: 0.1740 - output_3_loss: 0.3005 - output_4_loss: 0.3162 - output_5_loss: 0.3294 - output_6_loss: 0.3959 - output_1_acc: 0.8774 - output_2_acc: 0.9299 - output_3_acc: 0.8970 - output_4_acc: 0.8836 - output_5_acc: 0.8909 - output_6_acc: 0.8609 - val_loss: 1.5071 - val_output_1_loss: 0.3023 - val_output_2_loss: 0.1921 - val_output_3_loss: 0.2341 - val_output_4_loss: 0.2251 - val_output_5_loss: 0.2628 - val_output_6_loss: 0.2907 - val_output_1_acc: 0.9055 - val_output_2_acc: 0.9300 - val_output_3_acc: 0.9259 - val_output_4_acc: 0.9170 - val_output_5_acc: 0.9179 - val_output_6_acc: 0.8824\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "111699/111699 [==============================] - 76s 681us/step - loss: 1.9170 - output_1_loss: 0.3823 - output_2_loss: 0.1753 - output_3_loss: 0.3029 - output_4_loss: 0.3482 - output_5_loss: 0.3271 - output_6_loss: 0.3812 - output_1_acc: 0.8784 - output_2_acc: 0.9302 - output_3_acc: 0.8995 - output_4_acc: 0.8830 - output_5_acc: 0.8912 - output_6_acc: 0.8668 - val_loss: 2.6900 - val_output_1_loss: 0.5694 - val_output_2_loss: 0.2365 - val_output_3_loss: 0.4690 - val_output_4_loss: 0.4942 - val_output_5_loss: 0.5081 - val_output_6_loss: 0.4127 - val_output_1_acc: 0.7520 - val_output_2_acc: 0.9090 - val_output_3_acc: 0.7859 - val_output_4_acc: 0.7936 - val_output_5_acc: 0.7736 - val_output_6_acc: 0.7925\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n",
      "111699/111699 [==============================] - 76s 679us/step - loss: 1.8721 - output_1_loss: 0.3805 - output_2_loss: 0.1679 - output_3_loss: 0.3006 - output_4_loss: 0.3146 - output_5_loss: 0.3229 - output_6_loss: 0.3855 - output_1_acc: 0.8759 - output_2_acc: 0.9305 - output_3_acc: 0.8956 - output_4_acc: 0.8837 - output_5_acc: 0.8893 - output_6_acc: 0.8654 - val_loss: 2.0213 - val_output_1_loss: 0.3845 - val_output_2_loss: 0.1953 - val_output_3_loss: 0.3197 - val_output_4_loss: 0.3342 - val_output_5_loss: 0.3608 - val_output_6_loss: 0.4268 - val_output_1_acc: 0.8660 - val_output_2_acc: 0.9319 - val_output_3_acc: 0.8883 - val_output_4_acc: 0.8852 - val_output_5_acc: 0.8761 - val_output_6_acc: 0.8288\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "111699/111699 [==============================] - 76s 681us/step - loss: 1.7784 - output_1_loss: 0.3749 - output_2_loss: 0.1582 - output_3_loss: 0.2909 - output_4_loss: 0.2723 - output_5_loss: 0.3159 - output_6_loss: 0.3663 - output_1_acc: 0.8833 - output_2_acc: 0.9324 - output_3_acc: 0.9019 - output_4_acc: 0.8914 - output_5_acc: 0.8952 - output_6_acc: 0.8682 - val_loss: 1.6091 - val_output_1_loss: 0.3485 - val_output_2_loss: 0.1573 - val_output_3_loss: 0.2459 - val_output_4_loss: 0.2848 - val_output_5_loss: 0.2631 - val_output_6_loss: 0.3095 - val_output_1_acc: 0.8843 - val_output_2_acc: 0.9429 - val_output_3_acc: 0.9131 - val_output_4_acc: 0.8838 - val_output_5_acc: 0.9120 - val_output_6_acc: 0.8766\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "111699/111699 [==============================] - 76s 681us/step - loss: 1.6990 - output_1_loss: 0.3660 - output_2_loss: 0.1600 - output_3_loss: 0.2862 - output_4_loss: 0.2486 - output_5_loss: 0.3077 - output_6_loss: 0.3304 - output_1_acc: 0.8821 - output_2_acc: 0.9344 - output_3_acc: 0.9025 - output_4_acc: 0.8936 - output_5_acc: 0.8966 - output_6_acc: 0.8729 - val_loss: 1.4429 - val_output_1_loss: 0.2813 - val_output_2_loss: 0.1245 - val_output_3_loss: 0.2123 - val_output_4_loss: 0.2500 - val_output_5_loss: 0.2658 - val_output_6_loss: 0.3089 - val_output_1_acc: 0.9084 - val_output_2_acc: 0.9557 - val_output_3_acc: 0.9239 - val_output_4_acc: 0.8970 - val_output_5_acc: 0.9078 - val_output_6_acc: 0.8688\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n",
      "111699/111699 [==============================] - 76s 680us/step - loss: 1.6776 - output_1_loss: 0.3659 - output_2_loss: 0.1589 - output_3_loss: 0.2850 - output_4_loss: 0.2368 - output_5_loss: 0.3074 - output_6_loss: 0.3237 - output_1_acc: 0.8840 - output_2_acc: 0.9320 - output_3_acc: 0.9029 - output_4_acc: 0.8956 - output_5_acc: 0.8973 - output_6_acc: 0.8764 - val_loss: 1.6128 - val_output_1_loss: 0.3567 - val_output_2_loss: 0.1705 - val_output_3_loss: 0.2632 - val_output_4_loss: 0.2386 - val_output_5_loss: 0.3198 - val_output_6_loss: 0.2641 - val_output_1_acc: 0.8724 - val_output_2_acc: 0.9343 - val_output_3_acc: 0.8999 - val_output_4_acc: 0.8881 - val_output_5_acc: 0.8831 - val_output_6_acc: 0.8850\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f16fecd25c0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train_texts, x_train[meta_features]], separate_targets_train, validation_data=([x_dev_texts, x_dev[meta_features]], separate_targets_dev),\n",
    "          epochs=20, batch_size=50, class_weight = multiple_class_weights, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dev = model.predict([x_dev_texts, x_dev[meta_features]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dev = np.hstack(pred_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_roc_auc(y_true, y_pred):\n",
    "    roc_auc_scores = []\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        roc_auc_scores.append(metrics.roc_auc_score(y_true[:, i], y_pred[:, i]))\n",
    "    print(roc_auc_scores)\n",
    "    return np.mean(roc_auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9150969263256474, 0.975256100888613, 0.9490004618584489, 0.9503111167795424, 0.9422322860715479, 0.9315207123227781]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9439029340410964"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_roc_auc(y_dev, pred_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(MODEL_PATH + \"keras_contextual_lstm_specialized_dense_classification_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(MODEL_PATH + \"keras_contextual_lstm_specialized_dense_classification_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
