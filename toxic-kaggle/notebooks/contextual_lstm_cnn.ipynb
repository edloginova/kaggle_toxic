{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.utils import class_weight\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, Activation, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../models/\"\n",
    "DATA_PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_PATH + \"preprocessed/train_ling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features = ['count_sent', 'count_word', 'count_unique_word', 'count_letters',\n",
    "       'count_punctuations', 'count_words_upper', 'count_words_title',\n",
    "       'count_stopwords', 'mean_word_len', 'word_unique_percent',\n",
    "       'punct_percent', 'count_swear_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>...</th>\n",
       "      <th>count_unique_word</th>\n",
       "      <th>count_letters</th>\n",
       "      <th>count_punctuations</th>\n",
       "      <th>count_words_upper</th>\n",
       "      <th>count_words_title</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>word_unique_percent</th>\n",
       "      <th>punct_percent</th>\n",
       "      <th>count_swear_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>264</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>5.162791</td>\n",
       "      <td>95.348837</td>\n",
       "      <td>23.255814</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>112</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5.588235</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>70.588235</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>233</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>92.857143</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>622</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>55</td>\n",
       "      <td>4.486726</td>\n",
       "      <td>72.566372</td>\n",
       "      <td>18.584071</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4.230769</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>38.461538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0           0  0000997932d777bf   \n",
       "1           1  000103f0d9cfb60f   \n",
       "2           2  000113f07ec002fd   \n",
       "3           3  0001b41b1c6bb37e   \n",
       "4           4  0001d958c54c6e35   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  count_sent        ...          \\\n",
       "0        0       0       0              0           2        ...           \n",
       "1        0       0       0              0           1        ...           \n",
       "2        0       0       0              0           1        ...           \n",
       "3        0       0       0              0           5        ...           \n",
       "4        0       0       0              0           1        ...           \n",
       "\n",
       "   count_unique_word  count_letters  count_punctuations  count_words_upper  \\\n",
       "0                 41            264                  10                  2   \n",
       "1                 17            112                  12                  1   \n",
       "2                 39            233                   6                  0   \n",
       "3                 82            622                  21                  5   \n",
       "4                 13             67                   5                  0   \n",
       "\n",
       "   count_words_title  count_stopwords  mean_word_len  word_unique_percent  \\\n",
       "0                 11               16       5.162791            95.348837   \n",
       "1                  3                2       5.588235           100.000000   \n",
       "2                  2               19       4.571429            92.857143   \n",
       "3                  7               55       4.486726            72.566372   \n",
       "4                  2                5       4.230769           100.000000   \n",
       "\n",
       "   punct_percent  count_swear_words  \n",
       "0      23.255814                  0  \n",
       "1      70.588235                  0  \n",
       "2      14.285714                  0  \n",
       "3      18.584071                  0  \n",
       "4      38.461538                  0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    x = np.array([[embeddings_index[vocabulary_inv[vocabulary['word']]] if word in vocabulary.keys() else len(vocabulary) - 1 for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=18400)\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "sequences = tokenizer.texts_to_sequences(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = tokenizer.word_index\n",
    "vocabulary_inv = {v:k for k, v in vocabulary.items()}\n",
    "embeddings_index = {}\n",
    "EMBEDDING_DIM = 100\n",
    "f = open(\"../../../embeddings/glove.6B.\" + str(EMBEDDING_DIM) + \"d.txt\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "embedding_matrix = np.zeros((len(vocabulary) + 1, EMBEDDING_DIM))\n",
    "embedding_matrix[-1] = np.random.rand(EMBEDDING_DIM) # oov-vector\n",
    "for word, i in vocabulary.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i - 1] = embedding_vector\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=200, trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_dev, y_train, y_dev = train_test_split(train, targets, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200\n",
    "\n",
    "x_train_texts = tokenizer.texts_to_sequences(x_train['comment_text'])\n",
    "x_train_texts = pad_sequences(x_train_texts, maxlen=max_length, padding='post')\n",
    "\n",
    "x_dev_texts = tokenizer.texts_to_sequences(x_dev['comment_text'])\n",
    "x_dev_texts = pad_sequences(x_dev_texts, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 100)     21033800    input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_14 (CuDNNLSTM)       (None, 10)           4480        embedding_1[7][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 12)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 22)           0           cu_dnnlstm_14[0][0]              \n",
      "                                                                 input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 22, 1)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_15 (CuDNNLSTM)       (None, 10)           520         reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 10, 1)        0           cu_dnnlstm_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 9, 100)       300         reshape_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 4, 100)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 400)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 50)           20050       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_1 (Dense)                (None, 1)            51          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_2 (Dense)                (None, 1)            51          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_3 (Dense)                (None, 1)            51          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_4 (Dense)                (None, 1)            51          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_5 (Dense)                (None, 1)            51          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_6 (Dense)                (None, 1)            51          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 21,059,456\n",
      "Trainable params: 25,656\n",
      "Non-trainable params: 21,033,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(max_length,), dtype='float32')\n",
    "metadata_input = Input(shape=(len(meta_features),), dtype='float32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "lstm = keras.layers.CuDNNLSTM(10, return_sequences=False)(embedded_sequences)\n",
    "concatenated_data = Concatenate(axis=1)([lstm, metadata_input])\n",
    "reshape = keras.layers.Reshape((-1, 1))(concatenated_data)\n",
    "lstm_2 = keras.layers.CuDNNLSTM(10, return_sequences=False)(reshape)\n",
    "reshape_2 = keras.layers.Reshape((-1, 1))(lstm_2)\n",
    "cnn = keras.layers.Conv1D(filters = 100, kernel_size = 2, activation = 'relu')(reshape_2)\n",
    "pool = keras.layers.MaxPooling1D(pool_size=2)(cnn)\n",
    "flatten = keras.layers.Flatten()(pool)\n",
    "dense_1 = Dense(50, activation='relu')(flatten)\n",
    "output_1 = Dense(units=1, activation='sigmoid', name = 'output_1')(dense_1)\n",
    "output_2 = Dense(units=1, activation='sigmoid', name = 'output_2')(dense_1)\n",
    "output_3 = Dense(units=1, activation='sigmoid', name = 'output_3')(dense_1)\n",
    "output_4 = Dense(units=1, activation='sigmoid', name = 'output_4')(dense_1)\n",
    "output_5 = Dense(units=1, activation='sigmoid', name = 'output_5')(dense_1)\n",
    "output_6 = Dense(units=1, activation='sigmoid', name = 'output_6')(dense_1)\n",
    "model = Model(inputs=[sequence_input,metadata_input], outputs=[output_1, output_2, output_3, output_4, output_5, output_6])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_targets_train = [y_train[:, i] for i in range(0, y_train.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_targets_dev = [y_dev[:, i] for i in range(0, y_dev.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_class_weights = [class_weight.compute_class_weight('balanced', np.unique(separate_targets_train[i]),separate_targets_train[i]) for i in range(0, len(separate_targets_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_class_weights_dict = [{0:x[0], 1: x[1]} for x in separate_class_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 0.5530365294542862, 1: 5.213732262882749},\n",
       " {0: 0.5050140157337915, 1: 50.360234445446345},\n",
       " {0: 0.527848137156683, 1: 9.477261157305277},\n",
       " {0: 0.5015356017134083, 1: 163.30263157894737},\n",
       " {0: 0.5258353654517893, 1: 10.176658163265307},\n",
       " {0: 0.5043937286635478, 1: 57.39928057553957}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separate_class_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.55303653, 5.21373226]),\n",
       " array([ 0.50501402, 50.36023445]),\n",
       " array([0.52784814, 9.47726116]),\n",
       " array([  0.5015356 , 163.30263158]),\n",
       " array([ 0.52583537, 10.17665816]),\n",
       " array([ 0.50439373, 57.39928058])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separate_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_class_weights = {'output_' + str(i + 1): x for i, x in enumerate(separate_class_weights_dict)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_targets_train_ = [y_train[i, :] for i in range(0, y_train.shape[0])]\n",
    "separate_targets_dev_ = [y_dev[i, :] for i in range(0, y_dev.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111699"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"../models/contextual_lstm_cnn-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 111699 samples, validate on 47872 samples\n",
      "Epoch 1/20\n",
      "111699/111699 [==============================] - 204s 2ms/step - loss: 2.4346 - output_1_loss: 0.4836 - output_2_loss: 0.2917 - output_3_loss: 0.3790 - output_4_loss: 0.4431 - output_5_loss: 0.4269 - output_6_loss: 0.4103 - output_1_acc: 0.8540 - output_2_acc: 0.9062 - output_3_acc: 0.8842 - output_4_acc: 0.8830 - output_5_acc: 0.8953 - output_6_acc: 0.8780 - val_loss: 2.2089 - val_output_1_loss: 0.4729 - val_output_2_loss: 0.2680 - val_output_3_loss: 0.3090 - val_output_4_loss: 0.3481 - val_output_5_loss: 0.4133 - val_output_6_loss: 0.3976 - val_output_1_acc: 0.8785 - val_output_2_acc: 0.9243 - val_output_3_acc: 0.9294 - val_output_4_acc: 0.9134 - val_output_5_acc: 0.9173 - val_output_6_acc: 0.9078\n",
      "\n",
      "Epoch 00001: val_loss improved from -inf to 2.20889, saving model to ../models/contextual_lstm_cnn-01-2.21.hdf5\n",
      "Epoch 2/20\n",
      "111699/111699 [==============================] - 201s 2ms/step - loss: 2.2977 - output_1_loss: 0.4645 - output_2_loss: 0.2645 - output_3_loss: 0.3575 - output_4_loss: 0.4105 - output_5_loss: 0.4049 - output_6_loss: 0.3958 - output_1_acc: 0.9015 - output_2_acc: 0.9280 - output_3_acc: 0.9272 - output_4_acc: 0.8975 - output_5_acc: 0.9173 - output_6_acc: 0.9101 - val_loss: 2.1229 - val_output_1_loss: 0.4352 - val_output_2_loss: 0.2693 - val_output_3_loss: 0.3215 - val_output_4_loss: 0.3161 - val_output_5_loss: 0.3813 - val_output_6_loss: 0.3995 - val_output_1_acc: 0.8831 - val_output_2_acc: 0.9254 - val_output_3_acc: 0.9307 - val_output_4_acc: 0.9152 - val_output_5_acc: 0.9139 - val_output_6_acc: 0.9090\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/20\n",
      "111699/111699 [==============================] - 201s 2ms/step - loss: 2.2829 - output_1_loss: 0.4608 - output_2_loss: 0.2568 - output_3_loss: 0.3535 - output_4_loss: 0.4153 - output_5_loss: 0.4014 - output_6_loss: 0.3951 - output_1_acc: 0.9033 - output_2_acc: 0.9297 - output_3_acc: 0.9283 - output_4_acc: 0.9041 - output_5_acc: 0.9195 - output_6_acc: 0.9095 - val_loss: 1.8606 - val_output_1_loss: 0.3992 - val_output_2_loss: 0.2470 - val_output_3_loss: 0.2935 - val_output_4_loss: 0.3040 - val_output_5_loss: 0.3209 - val_output_6_loss: 0.2959 - val_output_1_acc: 0.9070 - val_output_2_acc: 0.9291 - val_output_3_acc: 0.9329 - val_output_4_acc: 0.9167 - val_output_5_acc: 0.9292 - val_output_6_acc: 0.9171\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/20\n",
      "111699/111699 [==============================] - 200s 2ms/step - loss: 2.2693 - output_1_loss: 0.4587 - output_2_loss: 0.2529 - output_3_loss: 0.3520 - output_4_loss: 0.4136 - output_5_loss: 0.4001 - output_6_loss: 0.3920 - output_1_acc: 0.9021 - output_2_acc: 0.9305 - output_3_acc: 0.9276 - output_4_acc: 0.9075 - output_5_acc: 0.9195 - output_6_acc: 0.9083 - val_loss: 1.6112 - val_output_1_loss: 0.3806 - val_output_2_loss: 0.1428 - val_output_3_loss: 0.2523 - val_output_4_loss: 0.2693 - val_output_5_loss: 0.3098 - val_output_6_loss: 0.2564 - val_output_1_acc: 0.9185 - val_output_2_acc: 0.9493 - val_output_3_acc: 0.9385 - val_output_4_acc: 0.9256 - val_output_5_acc: 0.9330 - val_output_6_acc: 0.9269\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/20\n",
      "111699/111699 [==============================] - 201s 2ms/step - loss: 2.2567 - output_1_loss: 0.4576 - output_2_loss: 0.2517 - output_3_loss: 0.3502 - output_4_loss: 0.4077 - output_5_loss: 0.3987 - output_6_loss: 0.3908 - output_1_acc: 0.9025 - output_2_acc: 0.9323 - output_3_acc: 0.9279 - output_4_acc: 0.9060 - output_5_acc: 0.9183 - output_6_acc: 0.9097 - val_loss: 2.2369 - val_output_1_loss: 0.4437 - val_output_2_loss: 0.2454 - val_output_3_loss: 0.3316 - val_output_4_loss: 0.4432 - val_output_5_loss: 0.3918 - val_output_6_loss: 0.3812 - val_output_1_acc: 0.9177 - val_output_2_acc: 0.9295 - val_output_3_acc: 0.9294 - val_output_4_acc: 0.9057 - val_output_5_acc: 0.9230 - val_output_6_acc: 0.9098\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.20889 to 2.23693, saving model to ../models/contextual_lstm_cnn-05-2.24.hdf5\n",
      "Epoch 6/20\n",
      "111699/111699 [==============================] - 200s 2ms/step - loss: 2.2370 - output_1_loss: 0.4554 - output_2_loss: 0.2417 - output_3_loss: 0.3478 - output_4_loss: 0.4040 - output_5_loss: 0.3959 - output_6_loss: 0.3922 - output_1_acc: 0.9058 - output_2_acc: 0.9338 - output_3_acc: 0.9300 - output_4_acc: 0.9056 - output_5_acc: 0.9216 - output_6_acc: 0.9112 - val_loss: 1.9650 - val_output_1_loss: 0.3767 - val_output_2_loss: 0.2495 - val_output_3_loss: 0.2818 - val_output_4_loss: 0.3761 - val_output_5_loss: 0.3262 - val_output_6_loss: 0.3547 - val_output_1_acc: 0.9050 - val_output_2_acc: 0.9325 - val_output_3_acc: 0.9366 - val_output_4_acc: 0.9122 - val_output_5_acc: 0.9298 - val_output_6_acc: 0.9161\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/20\n",
      "111699/111699 [==============================] - 218s 2ms/step - loss: 2.2206 - output_1_loss: 0.4533 - output_2_loss: 0.2416 - output_3_loss: 0.3473 - output_4_loss: 0.3965 - output_5_loss: 0.3934 - output_6_loss: 0.3885 - output_1_acc: 0.8992 - output_2_acc: 0.9353 - output_3_acc: 0.9287 - output_4_acc: 0.9021 - output_5_acc: 0.9199 - output_6_acc: 0.9110 - val_loss: 2.1545 - val_output_1_loss: 0.4333 - val_output_2_loss: 0.2628 - val_output_3_loss: 0.3354 - val_output_4_loss: 0.3649 - val_output_5_loss: 0.3953 - val_output_6_loss: 0.3630 - val_output_1_acc: 0.8758 - val_output_2_acc: 0.9241 - val_output_3_acc: 0.9234 - val_output_4_acc: 0.9054 - val_output_5_acc: 0.9130 - val_output_6_acc: 0.9041loss: 0.3511 - output_4_loss: 0.3720 - output_5_loss: 0.3877 - output_6_loss: 0.3808 - output_1_acc: 0.9004 - output_2_acc: 0.9349 - output_3_acc: 0.9275 - output_4_acc: 0.9125 - output_5_acc: 0.9206 - ETA: 1:35 - loss: 2.1946 - output_1_loss: 0.4538 - output_2_loss: 0.2550 - output_3_loss: 0.3499 - output_4_loss: 0.3690 - output_5_loss: 0.3873 - output_6_loss: 0.3797 - output_1_acc: 0.9007 - output_2_acc: 0.9347 - output_3_acc: 0.9277 - output_4_acc: 0.9125 - output_5_acc: 0.9207 - output - ETA: 1:35 - loss: 2.1917 - output_1_loss: 0.4542 - output_2_loss: 0.2544 - output - ETA: 29s - loss: 2.2217 - output_1_loss: 0.4538 - output_2_loss: 0.2409 - output_3_loss: 0.3474 - output_4_loss: 0. - ETA: 23s - loss: 2.2281 - output_1_loss: 0.4545 - output_2_loss: 0.2416 - output_3_loss: 0.3474 - output_4_loss: 0.4016 - output_5_loss: 0.3922 - output_6_loss: 0.3908 - output_1_acc: 0.9005 - output_2_a - ETA: 19s - loss: 2.2306 - output_1_loss: 0.4545 - output_2_loss: 0.2422 - output_3_loss: 0.3482 - output_4_loss: 0.4007 - output_5_loss - ETA: 14s - loss: 2.2232 - ou\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/20\n",
      "111699/111699 [==============================] - 220s 2ms/step - loss: 2.1204 - output_1_loss: 0.4371 - output_2_loss: 0.2272 - output_3_loss: 0.3364 - output_4_loss: 0.3698 - output_5_loss: 0.3737 - output_6_loss: 0.3763 - output_1_acc: 0.8731 - output_2_acc: 0.9313 - output_3_acc: 0.9176 - output_4_acc: 0.8829 - output_5_acc: 0.8996 - output_6_acc: 0.9004 - val_loss: 1.6339 - val_output_1_loss: 0.3404 - val_output_2_loss: 0.1871 - val_output_3_loss: 0.2570 - val_output_4_loss: 0.2509 - val_output_5_loss: 0.2810 - val_output_6_loss: 0.3175 - val_output_1_acc: 0.9025 - val_output_2_acc: 0.9404 - val_output_3_acc: 0.9308 - val_output_4_acc: 0.9152 - val_output_5_acc: 0.9198 - val_output_6_acc: 0.9098loss: 0.3772 - output_5_loss: 0.3739 - output_6_loss: 0.3683 - output_1_acc: 0.8742 - output_2_acc: 0.9318 - output_3_acc: 0.9190 - output_4_acc - ETA: 6s - loss: 2.1187 - output_1_loss: 0.4372 - output_2_loss: 0.2269 - output_3_loss: 0.3366 - output_4_loss: 0.3747 - output_5_loss: 0.3737 - output_6_loss: 0.3695 - output_1_acc: 0.8739 - output_2_acc: 0.\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111699/111699 [==============================] - 221s 2ms/step - loss: 2.0605 - output_1_loss: 0.4273 - output_2_loss: 0.2201 - output_3_loss: 0.3308 - output_4_loss: 0.3484 - output_5_loss: 0.3632 - output_6_loss: 0.3707 - output_1_acc: 0.8673 - output_2_acc: 0.9301 - output_3_acc: 0.9148 - output_4_acc: 0.8680 - output_5_acc: 0.8885 - output_6_acc: 0.8906 - val_loss: 1.4786 - val_output_1_loss: 0.3208 - val_output_2_loss: 0.1541 - val_output_3_loss: 0.2467 - val_output_4_loss: 0.2528 - val_output_5_loss: 0.2550 - val_output_6_loss: 0.2492 - val_output_1_acc: 0.8980 - val_output_2_acc: 0.9457 - val_output_3_acc: 0.9276 - val_output_4_acc: 0.9064 - val_output_5_acc: 0.9189 - val_output_6_acc: 0.9164 - output_5 - ETA: 2:44 - loss: 2.0570 - output_1_loss: 0.4212 - output_2_loss: 0.2089 - output_3_loss: 0.3316 - output_4_loss: 0.3460 - output_5_loss: 0.3721 - output_6_loss: 0.3771 - output_1_acc: 0.8677 - output_2_acc: 0.9289 - output_3_acc: 0.9124 - output_4_acc: 0.883 - ETA: 2:40 - loss: 2.0439 - output_1_lo - ETA: 2:01 - loss: 2.0431 - output_1_loss: 0.4251 - output_2_loss: 0.2028 - output_3_loss: 0.3218 - output_4_loss: 0.3462  - ETA: 1:49 - loss: 2.0698 - output_1_loss: 0.4275 - output_2_loss: 0.2041 - output_3_loss: 0.3252 - output_4_loss:\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/20\n",
      "111699/111699 [==============================] - 221s 2ms/step - loss: 2.0009 - output_1_loss: 0.4204 - output_2_loss: 0.2089 - output_3_loss: 0.3248 - output_4_loss: 0.3251 - output_5_loss: 0.3568 - output_6_loss: 0.3650 - output_1_acc: 0.8636 - output_2_acc: 0.9246 - output_3_acc: 0.9049 - output_4_acc: 0.8612 - output_5_acc: 0.8828 - output_6_acc: 0.8812 - val_loss: 2.3312 - val_output_1_loss: 0.4495 - val_output_2_loss: 0.2683 - val_output_3_loss: 0.3543 - val_output_4_loss: 0.4244 - val_output_5_loss: 0.4081 - val_output_6_loss: 0.4266 - val_output_1_acc: 0.7967 - val_output_2_acc: 0.8980 - val_output_3_acc: 0.8719 - val_output_4_acc: 0.7807 - val_output_5_acc: 0.8165 - val_output_6_acc: 0.8170ss: 0.4063 - output_5_loss: 0.3602 - output_6 - ETA: 5s - loss: 2.0029 - output_1_loss: 0.4203 - output_2_loss: 0.2099 - output_3_loss: 0.3245 - output_4_loss: 0.3253 - output_5_loss: 0.3565 - output_6_loss: 0.3663 - output_1_acc: 0.8646 - output_2_acc: 0.9248 - output_3_acc\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.23693 to 2.33120, saving model to ../models/contextual_lstm_cnn-10-2.33.hdf5\n",
      "Epoch 11/20\n",
      "111699/111699 [==============================] - 218s 2ms/step - loss: 1.9787 - output_1_loss: 0.4144 - output_2_loss: 0.1988 - output_3_loss: 0.3216 - output_4_loss: 0.3334 - output_5_loss: 0.3495 - output_6_loss: 0.3609 - output_1_acc: 0.8659 - output_2_acc: 0.9200 - output_3_acc: 0.9033 - output_4_acc: 0.8689 - output_5_acc: 0.8856 - output_6_acc: 0.8789 - val_loss: 2.1001 - val_output_1_loss: 0.4056 - val_output_2_loss: 0.2475 - val_output_3_loss: 0.3071 - val_output_4_loss: 0.3888 - val_output_5_loss: 0.3356 - val_output_6_loss: 0.4153 - val_output_1_acc: 0.8547 - val_output_2_acc: 0.9028 - val_output_3_acc: 0.8899 - val_output_4_acc: 0.8447 - val_output_5_acc: 0.8729 - val_output_6_acc: 0.856661 - output_4_loss: 0.3176 - output_5_loss: 0.3565 - output_6_loss: 0.3611 - output_1_acc: 0.8613 - output_2_acc: 0.9292 - output_3_acc: 0.8996 - output_4_acc: 0.8668 - outp - ETA: 2:17 - loss: 1.9790 - output_1_loss: 0.4190 - output_2_loss: 0.1837 - output_3_loss: 0.3352 - output_4_loss: 0.3225 - output_5_loss: 0.3602 - output_6_loss: 0.3583 - output_1_acc: 0.8605 - output_2_acc: 0.9276 - output_3 - ETA: 2:14 - loss: 1.9712 - output_1_loss: 0.4179 - output_2_loss: 0.1824 - output_3_loss: 0.3332 - output_4_loss: 0.3296 - output_5_loss: 0.3582 - output_6_loss: 0.3498 - output_1_acc: 0.8617 - output_2_acc: 0.9268 - output_3_acc: 0.9009 - output_4_acc: 0.8618 - output_5_acc: 0.8813 - output_6_acc - ETA: 2:13 - loss: 1.9729 - output_1_loss: 0.4180 - output_2_loss: 0.1863 - output_3_loss: 0.3336 - output_4_loss: 0.3280 - output_5_loss: 0.3585 - o - ETA: 2:05 - loss: 2.0139 - output_1_loss: 0.4250 - output_2_loss: 0.1920 - output_3_loss: 0.3409 - output_4_loss: 0.3321 - output_5_loss: 0.3655 - output_6_loss: 0.3582 - output_1_acc: 0.8574 - output_2_acc: 0.9216 - output_3_acc: 0.8965 - output_4_acc: 0.8571 - output_5_acc: 0.8779 - outp - ETA: 2:04 - loss: 2.0172 - output_1_loss: 0.4247 - output_2_loss: 0.1914 - output_3_loss: 0.3400 - output_4_loss: 0.3308 - output_5_loss: 0.3655 - ou - ETA: 4s - loss: 1.9770 - output_1_loss: 0.4144 - output_2_loss: 0.1991 - output_3_loss: 0.3212 - output_4_loss: 0.3302 - output_5_loss: 0.3506 - output_6_loss: 0.3615 - output_1_acc: 0.8658 - output_2_acc: 0.9202 - output_3_acc: 0.9032 - output_4_acc: 0.8689 - output_5_acc - ETA: 2s - loss: 1.9781 - output_1_loss: 0.4143 - output_2_loss: 0.1985 - output_3_loss: 0.3218 - output_4_loss: 0.3336 - output_5_loss: 0.3495 - output_6_loss: 0.3604 - output_1_acc: 0.8661 - output_2_acc: 0.9202 - output_3_acc: 0.9035 - output_4_acc: 0.8692 - output_5_\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/20\n",
      "111699/111699 [==============================] - 221s 2ms/step - loss: 1.8897 - output_1_loss: 0.4047 - output_2_loss: 0.1897 - output_3_loss: 0.3145 - output_4_loss: 0.2891 - output_5_loss: 0.3420 - output_6_loss: 0.3497 - output_1_acc: 0.8655 - output_2_acc: 0.9190 - output_3_acc: 0.9010 - output_4_acc: 0.8733 - output_5_acc: 0.8847 - output_6_acc: 0.8773 - val_loss: 1.5791 - val_output_1_loss: 0.3292 - val_output_2_loss: 0.1879 - val_output_3_loss: 0.2483 - val_output_4_loss: 0.2299 - val_output_5_loss: 0.2832 - val_output_6_loss: 0.3006 - val_output_1_acc: 0.9090 - val_output_2_acc: 0.9227 - val_output_3_acc: 0.9222 - val_output_4_acc: 0.9103 - val_output_5_acc: 0.9116 - val_output_6_acc: 0.9018\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/20\n",
      "111699/111699 [==============================] - 222s 2ms/step - loss: 1.8777 - output_1_loss: 0.3997 - output_2_loss: 0.1859 - output_3_loss: 0.3109 - output_4_loss: 0.3013 - output_5_loss: 0.3352 - output_6_loss: 0.3447 - output_1_acc: 0.8660 - output_2_acc: 0.9178 - output_3_acc: 0.9014 - output_4_acc: 0.8753 - output_5_acc: 0.8844 - output_6_acc: 0.8769 - val_loss: 2.0566 - val_output_1_loss: 0.3665 - val_output_2_loss: 0.2752 - val_output_3_loss: 0.2906 - val_output_4_loss: 0.3830 - val_output_5_loss: 0.3312 - val_output_6_loss: 0.4102 - val_output_1_acc: 0.8564 - val_output_2_acc: 0.8847 - val_output_3_acc: 0.8856 - val_output_4_acc: 0.8403 - val_output_5_acc: 0.8664 - val_output_6_acc: 0.8394\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111699/111699 [==============================] - 225s 2ms/step - loss: 1.8275 - output_1_loss: 0.3948 - output_2_loss: 0.1818 - output_3_loss: 0.3076 - output_4_loss: 0.2754 - output_5_loss: 0.3297 - output_6_loss: 0.3383 - output_1_acc: 0.8725 - output_2_acc: 0.9155 - output_3_acc: 0.9014 - output_4_acc: 0.8821 - output_5_acc: 0.8867 - output_6_acc: 0.8748 - val_loss: 2.1190 - val_output_1_loss: 0.4351 - val_output_2_loss: 0.2182 - val_output_3_loss: 0.3322 - val_output_4_loss: 0.3909 - val_output_5_loss: 0.3719 - val_output_6_loss: 0.3707 - val_output_1_acc: 0.8399 - val_output_2_acc: 0.9137 - val_output_3_acc: 0.8850 - val_output_4_acc: 0.8278 - val_output_5_acc: 0.8600 - val_output_6_acc: 0.85563929 - output_2_loss: 0.1853 - output_3_loss: 0.3039 - output_4_loss: 0.2558 - output_5_loss: 0.3266 - output_6_loss: 0.3381 - output_1_acc: 0.8758 - output_2_acc:  - ETA: 57s - loss: 1.8029 - output_1_loss: 0.3922 - output_2_loss: 0.1857 - output_3_loss: 0.3035 - output_4_loss: 0.2532 - output_5_loss - ETA: 51s - loss: 1.7957 - output_1_loss: 0.3911 - output_2_loss: 0.1839 - output_3_loss: 0.3027 - output_4_loss: 0.2537 - output_5_loss: 0.3251 - output_6_loss: 0.3392 - output_1_acc: 0.8759 - output_2_acc: 0.9150 - output_3_acc: 0.9030 - output_4_acc: 0.8928 - output_5_acc: 0.8898 - output_6_a - ETA: 50s - loss: 1.7945 - output_1_loss: 0.3909 - output_2_loss: 0.1837 - output_3_loss: 0.3029 - output_4_loss: 0.2535 - output_5_ - ETA: 44s - loss: 1.7979 - output_1_loss: 0.3922 - output_2_loss: 0.1841 - output_3_loss: 0.3\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n",
      "111699/111699 [==============================] - 221s 2ms/step - loss: 1.7561 - output_1_loss: 0.3912 - output_2_loss: 0.1699 - output_3_loss: 0.2998 - output_4_loss: 0.2446 - output_5_loss: 0.3233 - output_6_loss: 0.3272 - output_1_acc: 0.8685 - output_2_acc: 0.9215 - output_3_acc: 0.9001 - output_4_acc: 0.8785 - output_5_acc: 0.8855 - output_6_acc: 0.8740 - val_loss: 1.5658 - val_output_1_loss: 0.3401 - val_output_2_loss: 0.1829 - val_output_3_loss: 0.2475 - val_output_4_loss: 0.2241 - val_output_5_loss: 0.2898 - val_output_6_loss: 0.2814 - val_output_1_acc: 0.8710 - val_output_2_acc: 0.9258 - val_output_3_acc: 0.9091 - val_output_4_acc: 0.9043 - val_output_5_acc: 0.8889 - val_output_6_acc: 0.8874 output_5_loss: 0.3068 - output_6_loss: 0.3605 - output_1_acc: 0.8663 - output_2_acc: 0.9197 - output_3_acc: 0.8959 - output_4_acc: 0.8792 - output_5_acc: 0.8860 - output_6_ac - ETA: 2:27 - loss: 1.7429 - output_1_loss: 0.3896 - output_2_loss: 0.1588 - output_3_loss: 0.3029 - output_4_loss: 0.2238 - output_5_loss: 0.3057 - output_6_loss: 0.3621 - output_1_acc: 0.8663 - output_2_acc: 0.9198 - output_3_acc: 0 - ETA: 2:22 - loss: 1.7410 - output_1_loss: 0.3904 - output_2_loss: 0.1563 - output_3_loss: 0.3032 - output_4_loss: 0.2246 - output_5_loss: 0.3095 - output_6_loss: 0.3569 - output_1_acc: 0.8668 - output_2_acc: 0.9198 - output_3_acc: 0.8952 - output_4_acc: 0 - ETA: 2:18 - loss: 1.7267 - output_1_loss: 0.3901 - output_2_loss: 0.1521 - output_3_loss: 0.2993 - output_4_loss: 0.2185 - output_5_loss: 0.3113 - output_6_loss: 0.3553 - output_1_acc: 0.8678 - output_2_acc: 0.9209 - out - ETA: 2:12 - loss: 1.7491 - output_1_loss: 0.3937 - output_2_loss: 0.1587 - ou - ETA: 15s - loss: 1.7779 - outp - ETA: 1s - loss: 1.7576 - output_1_loss: 0.3913 - output_2_loss: 0.1702 - output_3_loss: 0.3005 - output_4_loss: 0.2451 - output_5_loss: 0.3232 - output_6_loss: 0.3273 - output_1_acc: 0.8686 - output_2_acc: 0.9214 - output_3_acc: 0.9000 - output_4_acc: 0.8784 - output_5_acc: 0.8\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "111699/111699 [==============================] - 224s 2ms/step - loss: 1.7266 - output_1_loss: 0.3829 - output_2_loss: 0.1684 - output_3_loss: 0.2963 - output_4_loss: 0.2398 - output_5_loss: 0.3160 - output_6_loss: 0.3231 - output_1_acc: 0.8660 - output_2_acc: 0.9248 - output_3_acc: 0.8992 - output_4_acc: 0.8819 - output_5_acc: 0.8830 - output_6_acc: 0.8727 - val_loss: 1.1099 - val_output_1_loss: 0.2463 - val_output_2_loss: 0.1307 - val_output_3_loss: 0.1979 - val_output_4_loss: 0.1293 - val_output_5_loss: 0.1954 - val_output_6_loss: 0.2103 - val_output_1_acc: 0.9225 - val_output_2_acc: 0.9450 - val_output_3_acc: 0.9324 - val_output_4_acc: 0.9402 - val_output_5_acc: 0.9337 - val_output_6_acc: 0.9168 output_3_loss: 0 - ETA: 1:06 - loss: - ETA: 53s - loss: 1.7375 - output_1_loss: 0.3819 - output_2_loss: 0.1617 - output_3_loss: 0.2974 - output_4_loss: 0.2508 - outpu\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n",
      "111699/111699 [==============================] - 220s 2ms/step - loss: 1.6667 - output_1_loss: 0.3787 - output_2_loss: 0.1607 - output_3_loss: 0.2915 - output_4_loss: 0.2151 - output_5_loss: 0.3113 - output_6_loss: 0.3093 - output_1_acc: 0.8722 - output_2_acc: 0.9235 - output_3_acc: 0.9034 - output_4_acc: 0.8853 - output_5_acc: 0.8887 - output_6_acc: 0.8727 - val_loss: 1.4849 - val_output_1_loss: 0.3184 - val_output_2_loss: 0.1722 - val_output_3_loss: 0.2476 - val_output_4_loss: 0.2114 - val_output_5_loss: 0.2700 - val_output_6_loss: 0.2653 - val_output_1_acc: 0.8858 - val_output_2_acc: 0.9312 - val_output_3_acc: 0.9164 - val_output_4_acc: 0.8998 - val_output_5_acc: 0.9024 - val_output_6_acc: 0.8930_5_loss: 0.3162 - output_6_loss: 0.3197 - output_1_acc: 0.8787 - - ETA: 13s - loss: 1.6625 - output_1_loss: 0.3789 - output_2_loss: 0.1623 - output_3_loss: 0.2912 - output_4_loss: 0.2111 - output_5_loss: 0.3113 - output_6_lo - ETA: 7s - loss: 1.6692 - output_1_loss: 0.3793 - output_2_loss: 0.1617 - output_3_loss: 0.2905 - output_4_loss: 0.2161 - output_5_loss: 0.3111 - output_6_loss: 0.3105 - output_1_acc: 0.8719 - output_2_acc: 0.9233 - ETA: 1s - loss: 1.6670 - output_1_loss: 0.3791 - output_2_loss: 0.1605 - output_3_loss: 0.2912 - output_4_loss: 0.2152 - output_5_loss: 0.3112 - output_6_loss: 0.3098 - output_1_acc: 0.8720 - output_2_acc: 0.9236 - output_3_acc: 0.9034 - output_4_acc: 0.8853 - output_5_acc: 0.888\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111699/111699 [==============================] - 219s 2ms/step - loss: 1.6380 - output_1_loss: 0.3756 - output_2_loss: 0.1602 - output_3_loss: 0.2884 - output_4_loss: 0.2079 - output_5_loss: 0.3092 - output_6_loss: 0.2967 - output_1_acc: 0.8679 - output_2_acc: 0.9241 - output_3_acc: 0.9050 - output_4_acc: 0.8885 - output_5_acc: 0.8885 - output_6_acc: 0.8728 - val_loss: 1.8605 - val_output_1_loss: 0.3795 - val_output_2_loss: 0.2228 - val_output_3_loss: 0.3033 - val_output_4_loss: 0.2904 - val_output_5_loss: 0.3089 - val_output_6_loss: 0.3555 - val_output_1_acc: 0.8633 - val_output_2_acc: 0.9113 - val_output_3_acc: 0.8967 - val_output_4_acc: 0.8710 - val_output_5_acc: 0.8875 - val_output_6_acc: 0.8580 - ETA: 1:49 - loss: 1.5854 - output_1_loss: 0.3722 - output_2_loss: 0.1458 - output_3_loss: 0.2823 - output_4_loss: 0.1725 - output_5_loss: 0.3135 - output_6_loss: 0.2991 - output_1_acc: 0.8720 - output_2_acc: 0.9258 - output_3_acc: 0.9059 - output_4_acc: 0.8978 - output_5_a - ETA: 1:46 - loss: 1.5865 - output_1_loss: 0.3720 - output_2_loss: 0.1450 - output_3_loss: 0.2809 - output_4_loss: 0.1735 - output_5_loss: 0.3136 - output_6_loss: 0.3016 - output_1_acc: 0.8716 - output_2_acc: 0.9260 - output_3_acc: 0.9062 - output_4_acc: 0.8981 - output_5_acc: 0 - ETA: 1:45 - loss: 1.5780 - output_1_loss: 0.3716 - output_2_loss: 0.1438 - output_3_loss: 0.2802 - output_4_loss: 0.1715 - output_5_loss: 0.3124 - output_6_loss: 0.2985 - output_1_acc: 0.8724 - output_2_acc: 0.9264 - output_ - ETA: 1:39 - loss: 1.5834 - output_1_loss: 0.3738 - outp - ETA: 1:22 - loss: 1.6000 - output_1_loss: 0.3757 - output_2_loss: 0.1502 - output_3_loss: 0.2825 - output_4_loss: 0.1951 - output_5_loss: 0.3111 - output_6_loss: 0.2855 - output_1_acc: 0.8681 - output_2_acc: 0.9268 - output_3_acc: 0.9063 - output_4_acc: 0.8959 - output_5_acc: 0.8890 - output_6_acc - ETA: 1:22 - loss: 1.6007 - output_1_loss: 0.3761 - output_2_loss: 0.1502 - output_3_loss: 0.2826 - output_4_loss: 0.1949 - output_5_loss: 0.3111 - output_6_loss:  - ETA: 45s - loss: 1.6124 - output_1_loss: 0.3758 - output_2_loss: 0.1592 - output_3_loss: 0.2887 - output_4_loss: 0.1909 - output_5_loss: 0.3104 - output_6_loss: 0.2875 - output_1_acc: 0.8701 - output_2_acc: 0.9255 - output_3_ac - ETA: 21s - loss: 1.6263 - output_1_loss: 0.3770 - output_2_loss: 0.1608 - output_3_loss: 0.2894 - output_4_loss: 0.2012 - output_5_loss: 0.3099 - output_6_loss: 0.2880 - output_1_acc: 0.8681 - output_2_acc: 0.9243 - output_3_acc: - ETA: 19s - loss: 1.6300 - output_1_loss: 0.3766 - output_2_loss: 0.1606 - output_3_loss: 0.2888 - output_4_loss: 0.2013 - output_5_loss: 0.3105 - out - ETA: 14s - loss: 1.6292 - output_1_\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "111699/111699 [==============================] - 219s 2ms/step - loss: 1.5967 - output_1_loss: 0.3715 - output_2_loss: 0.1559 - output_3_loss: 0.2824 - output_4_loss: 0.1955 - output_5_loss: 0.3010 - output_6_loss: 0.2904 - output_1_acc: 0.8757 - output_2_acc: 0.9266 - output_3_acc: 0.9075 - output_4_acc: 0.8954 - output_5_acc: 0.8938 - output_6_acc: 0.8766 - val_loss: 2.2141 - val_output_1_loss: 0.4201 - val_output_2_loss: 0.2246 - val_output_3_loss: 0.3437 - val_output_4_loss: 0.3925 - val_output_5_loss: 0.3918 - val_output_6_loss: 0.4416 - val_output_1_acc: 0.8236 - val_output_2_acc: 0.9005 - val_output_3_acc: 0.8598 - val_output_4_acc: 0.8379 - val_output_5_acc: 0.8327 - val_output_6_acc: 0.8070ETA: 32s - loss: 1.5899 - output_1_loss: 0.3720 - output_2_loss: 0.1539 - output_3_loss: 0.2828 - output_4_loss: 0.1980 - output_5_loss: 0.3024 - outpu - ETA: 27s - loss: 1.5897 - output_1_loss: 0.3710 - output_2_loss: 0.1537 - output_3_loss: 0.2830 - output_ - ETA: 20s - loss: 1.5908 - output_1_ - ETA: 11s - loss: 1.5942 - output_1_loss: 0.3725 - output_2_loss: 0.1582 - output_3_loss: 0.2827 - output_4_loss: 0.1945 - output_5_loss: 0.3006 - output_6_loss: 0.2858 - output_1_acc: 0.8751 - output_2_acc: 0.9264 - output_3_acc: 0.9074 - output_ - ETA: 9s - loss: 1.5933 - output_1_loss: 0.3722 - output_2_loss: 0.1577 - output_3_loss: 0.2826 - output_4_loss: 0.1937 - output_5_loss: 0.3006 - output_6_loss: 0.\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n",
      "111699/111699 [==============================] - 217s 2ms/step - loss: 1.5651 - output_1_loss: 0.3703 - output_2_loss: 0.1516 - output_3_loss: 0.2806 - output_4_loss: 0.1831 - output_5_loss: 0.3011 - output_6_loss: 0.2785 - output_1_acc: 0.8721 - output_2_acc: 0.9295 - output_3_acc: 0.9062 - output_4_acc: 0.9007 - output_5_acc: 0.8894 - output_6_acc: 0.8748 - val_loss: 1.4400 - val_output_1_loss: 0.3194 - val_output_2_loss: 0.1399 - val_output_3_loss: 0.2271 - val_output_4_loss: 0.2399 - val_output_5_loss: 0.2505 - val_output_6_loss: 0.2631 - val_output_1_acc: 0.8721 - val_output_2_acc: 0.9370 - val_output_3_acc: 0.9174 - val_output_4_acc: 0.8851 - val_output_5_acc: 0.8992 - val_output_6_acc: 0.8718\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb08f2ac860>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train_texts, x_train[meta_features]], separate_targets_train, validation_data=([x_dev_texts, x_dev[meta_features]], separate_targets_dev),\n",
    "          epochs=20, batch_size=20, class_weight = multiple_class_weights, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dev = model.predict([x_dev_texts, x_dev[meta_features]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dev = np.hstack(pred_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_roc_auc(y_true, y_pred):\n",
    "    roc_auc_scores = []\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        roc_auc_scores.append(metrics.roc_auc_score(y_true[:, i], y_pred[:, i]))\n",
    "    print(roc_auc_scores)\n",
    "    return np.mean(roc_auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9086792589520657, 0.9740130048308272, 0.9455602742888023, 0.9407368590975856, 0.9373856369428853, 0.9382553888732748]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9407717371642401"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_roc_auc(y_dev, pred_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(MODEL_PATH + \"keras_contextual_lstm_cnn_classification_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(MODEL_PATH + \"keras_contextual_lstm_cnn_classification_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'keras_contextual_lstm_cnn_classification_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATA_PATH + \"preprocessed/test_ling.csv\")\n",
    "\n",
    "x_dev_texts = tokenizer.texts_to_sequences(test['comment_text'])\n",
    "x_dev_texts = pad_sequences(x_dev_texts, maxlen=max_length, padding='post')\n",
    "\n",
    "pred = model.predict([x_dev_texts, test[meta_features]])\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "sample_submission = pd.read_csv('../submissions/sample_submission.csv')\n",
    "sample_submission[list_classes] = np.hstack(pred)\n",
    "sample_submission.to_csv(\"../submissions/\" + model_name + \".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
