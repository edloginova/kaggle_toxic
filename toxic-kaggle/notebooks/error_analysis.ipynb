{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov = pickle.load(open(\"../data/preprocessed/oov_word2vec.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_swear_words = [ \"cockblocking\", \"shitwhole\", \"shitbreath\", \"jackshit\", \"fcockc\", \"asskicked\", \"pigshit\", \"crapness\", \"fucky\", \"G'damnit\", \"fuckism\", \"asshoul\", \"cockered\", \"asswhipe\", \"fuckbook\", \"cocksmoking\", \"cockrot\", \"fuckoff\", \"Dickasshole\", \"cunttwat\", \"fuckhole\", \"crackwhore\", \"fucknuckle\", \"mutherfucking\", \"mother'fuckerer\", \"damnass\", \"hoeshit\", \"whateverthefuck\", \"fuckhead\", \"dumbfuck\", \"mo'fucka\", \"cockmonkey\", \"fuckiest\", \"motherfuck\", \"dickheaded\", \"hitcock\", \"assfack\", \"sadass\", \"fuckkkkk\", \"awshit\", \"bullshit-\", \"asslickers\", \"lameasses\", \"fuckface\", \"assraped\", \"dickfucker\", \"cumshitter\", \"fuckedy\", \"shitbrown\", \"gofuckyourself\", \"cockass\", \"fuckboys\", \"bitchSon\", \"bitchboy\", \"fuckwitz\", \"cocksuer\", \"asslicking\", \"cuntlicker\", \"crapipedia\", \"jackasshole\", \"moth*rfucker\", \"shitpoke\", \"billshit\", \"fistfuckee\", \"everyfucking\", \"dumbshit\", \"dummass\", \"dickweed\", \"shittin\", \"dickbutt\", \"Motherfucked\", \"fuckass\", \"shitball\", \"Runtshit\", \"dickwads\", \"-assed\", \"crapstirrer\", \"shitass\", \"shitfucker\", \"fucktarded\", \"bitchass\", \"dipshite\", \"pissier\", \"assmuncher\", \"Deadfuck\", \"camwhore\", \"fuckability\", \"-bitch\", \"shittroll\", \"dickface\", \"dicksucking\", \"cuntface\", \"fuckwhit\", \"shitler\", \"buttfucking\", \"fucknut\", \"cocksmooching\", \"flabbergassed\", \"animalfucker\", \"asslicker\", \"cunthole\", \"dummasses\", \"fuckish\", \"wiki_harassment\", \"cuntish\", \"dickfag\", \"fuckz\", \"shitehole\", \"cuntheads\", \"fuckÄ±ng\", \"cunting\", \"motherfucka\", \"shittty\", \"sonovabitch\", \"dickwad\", \"dumass\", \"shitbags\", \"fuckng\", \"cockface\", \"cockhead\", \"sockfuck\", \"muthafucker\", \"mohterfuck\", \"shitcans\", \"douchecock\", \"lameass\", \"cockbites\", \"suckdickeer\", \"assface\", \"suckass\", \"fuckee\", \"assh*le\", \"dickbrain\", \"goshdamn\", \"Deadass\", \"mothafuckin\", \"discofucker\", \"shitgiggler\", \"crapass\", \"dickbag\", \"assholish\", \"dumbshits\", \"fuckheads\", \"motherfucken\", \"fuckbags\", \"youfuckingidiot\", \"shitposting\", \"pieceofshit\", \"shittest\", \"bitchfuck\", \"cocksusker\", \"fuckwhits\", \"pisshead\", \"fuckwads\", \"shity\", \"assehole\", \"cunthook\", \"dicklicking\", \"dickhead-\", \"dickhole\", \"bullshits\", \"dickass\", \"dicksukers\", \"fuckin't\", \"cockblaster\", \"fuckelwad\", \"Dumbfuck\", \"horsecrap\", \"cuntrag\", \"jerkass\", \"cockblock\", \"clusterfucked\", \"bitchez\", \"camwhores\", \"mutherfuckng\", \"fuckstain\", \"asswhore\", \"dipshitboy\", \"asswipes\", \"whoreism\", \"absofuckinglutely\", \"dipshitted\", \"craping\", \"muthafuckaa\", \"dumbfucks\", \"dumasss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_swear_words = set([x.lower() for x in new_swear_words])\n",
    "len(new_swear_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../models/\"\n",
    "DATA_PATH = \"../data/\"\n",
    "model_name = 'keras_lstm_fancy_classification_model'\n",
    "y_dev = pickle.load(open(DATA_PATH + \"y_dev.p\", \"rb\"))\n",
    "pred = pickle.load(open(\"../submissions/DEV_\" + model_name + \".p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = pickle.load(open(DATA_PATH + \"X_dev.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119105</th>\n",
       "      <td>7ca72b5b9c688e9e</td>\n",
       "      <td>Geez, are you forgetful!  We've already discus...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131631</th>\n",
       "      <td>c03f72fd8f8bf54f</td>\n",
       "      <td>Carioca RFA \\n\\nThanks for your support on my ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125326</th>\n",
       "      <td>9e5b8e8fc1ff2e84</td>\n",
       "      <td>\"\\n\\n Birthday \\n\\nNo worries, It's what I do ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111256</th>\n",
       "      <td>5332799e706665a6</td>\n",
       "      <td>Pseudoscience category? \\n\\nI'm assuming that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83590</th>\n",
       "      <td>dfa7d8f0b4366680</td>\n",
       "      <td>(and if such phrase exists, it would be provid...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "119105  7ca72b5b9c688e9e  Geez, are you forgetful!  We've already discus...   \n",
       "131631  c03f72fd8f8bf54f  Carioca RFA \\n\\nThanks for your support on my ...   \n",
       "125326  9e5b8e8fc1ff2e84  \"\\n\\n Birthday \\n\\nNo worries, It's what I do ...   \n",
       "111256  5332799e706665a6  Pseudoscience category? \\n\\nI'm assuming that ...   \n",
       "83590   dfa7d8f0b4366680  (and if such phrase exists, it would be provid...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "119105      0             0        0       0       0              0  \n",
       "131631      0             0        0       0       0              0  \n",
       "125326      0             0        0       0       0              0  \n",
       "111256      0             0        0       0       0              0  \n",
       "83590       0             0        0       0       0              0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47872, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47872, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_roc_auc(y_true, y_pred):\n",
    "    roc_auc_scores = []\n",
    "    for i in range(0, y_true.shape[1]):\n",
    "        roc_auc_scores.append(roc_auc_score(y_true[:, i], y_pred[:, i]))\n",
    "    print(roc_auc_scores)\n",
    "    return np.mean(roc_auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9764074251197777, 0.9869719381963974, 0.987538161857247, 0.962685471687418, 0.9822949226417165, 0.9725777883174068]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9780792846366606"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_roc_auc(y_dev, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) add more swear words back for threat and identity hate classes<br>\n",
    "2) use different lists of swear words (per category)?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rounded = np.round(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_predictions_toxic = X_dev.iloc[[i for i in range(0, len(y_dev)) if y_dev[i, 0] != pred_rounded[i, 0] ]]['comment_text'].values\n",
    "wrong_predictions_severe_toxic = X_dev.iloc[[i for i in range(0, len(y_dev)) if y_dev[i, 1] != pred_rounded[i, 1] ]]['comment_text'].values\n",
    "wrong_predictions_obscene = X_dev.iloc[[i for i in range(0, len(y_dev)) if y_dev[i, 2] != pred_rounded[i, 2] ]]['comment_text'].values\n",
    "wrong_predictions_threat = X_dev.iloc[[i for i in range(0, len(y_dev)) if y_dev[i, 3] != pred_rounded[i, 3] ]]['comment_text'].values\n",
    "wrong_predictions_insult = X_dev.iloc[[i for i in range(0, len(y_dev)) if y_dev[i, 4] != pred_rounded[i, 4] ]]['comment_text'].values\n",
    "wrong_predictions_identity_hate = X_dev.iloc[[i for i in range(0, len(y_dev)) if y_dev[i, 5] != pred_rounded[i, 5] ]]['comment_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# wrong toxic 1740\n",
      "# wrong severe_toxic 452\n",
      "# wrong obscene 962\n",
      "# wrong threat 136\n",
      "# wrong insult 1327\n",
      "# wrong identity_hate 433\n"
     ]
    }
   ],
   "source": [
    "print('# wrong toxic', len(wrong_predictions_toxic))\n",
    "print('# wrong severe_toxic', len(wrong_predictions_severe_toxic))\n",
    "print('# wrong obscene', len(wrong_predictions_obscene))\n",
    "print('# wrong threat', len(wrong_predictions_threat))\n",
    "print('# wrong insult', len(wrong_predictions_insult))\n",
    "print('# wrong identity_hate', len(wrong_predictions_identity_hate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsure_predictions_toxic = X_dev.iloc[[i for i in range(0, len(y_dev)) if abs(pred[i, 0] - 0.5) < threshold ]]['comment_text'].values\n",
    "unsure_predictions_severe_toxic = X_dev.iloc[[i for i in range(0, len(y_dev)) if abs(pred[i, 1] - 0.5) < threshold ]]['comment_text'].values\n",
    "unsure_predictions_obscene = X_dev.iloc[[i for i in range(0, len(y_dev)) if abs(pred[i, 2] - 0.5) < threshold ]]['comment_text'].values\n",
    "unsure_predictions_threat = X_dev.iloc[[i for i in range(0, len(y_dev)) if abs(pred[i, 3] - 0.5) < threshold ]]['comment_text'].values\n",
    "unsure_predictions_insult = X_dev.iloc[[i for i in range(0, len(y_dev)) if abs(pred[i, 4] - 0.5) < threshold ]]['comment_text'].values\n",
    "unsure_predictions_identity_hate = X_dev.iloc[[i for i in range(0, len(y_dev)) if abs(pred[i, 5] - 0.5) < threshold ]]['comment_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unsure toxic 1609\n",
      "# unsure severe_toxic 737\n",
      "# unsure obscene 824\n",
      "# unsure threat 0\n",
      "# unsure insult 1587\n",
      "# unsure identity_hate 228\n"
     ]
    }
   ],
   "source": [
    "print('# unsure toxic', len(unsure_predictions_toxic))\n",
    "print('# unsure severe_toxic', len(unsure_predictions_severe_toxic))\n",
    "print('# unsure obscene', len(unsure_predictions_obscene))\n",
    "print('# unsure threat', len(unsure_predictions_threat))\n",
    "print('# unsure insult', len(unsure_predictions_insult))\n",
    "print('# unsure identity_hate', len(unsure_predictions_identity_hate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) take caps into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"n't\", 67), ('stop', 65), ('wikipedia', 49), ('like', 49), ('get', 48), ('talk', 47), ('page', 44), (\"'s\", 44), ('article', 40), ('please', 39), ('people', 34), (\"'re\", 32), ('know', 32), ('go', 29), ('dont', 29), (\"'m\", 29), ('block', 27), ('u', 27), ('vandalism', 27), ('life', 26), ('think', 26), ('hey', 26), ('blocked', 25), ('man', 24), ('user', 24), ('gay', 23), ('thanks', 23), ('edits', 23), ('well', 22), ('little', 22), ('hell', 20), ('oh', 19), ('wp', 19), ('lol', 19), ('time', 19), ('message', 18), ('hate', 18), ('still', 18), ('keep', 18), ('see', 18), ('admin', 18), ('one', 18), ('wrong', 18), ('pages', 18), ('got', 17), ('wiki', 17), ('would', 17), ('im', 17), ('real', 17), ('leave', 16)]\n",
      "====\n",
      "[('fuck', 120), ('fucking', 56), ('go', 40), ('suck', 37), ('u', 28), ('bitch', 26), ('dick', 25), ('shit', 25), ('cunt', 25), ('asshole', 25), ('ass', 23), ('cock', 21), ('wikipedia', 19), (\"n't\", 18), (\"'s\", 17), ('faggot', 17), ('stop', 16), (\"'re\", 16), ('mother', 15), ('gay', 14), ('big', 14), ('fucker', 14), ('get', 13), ('page', 13), ('piece', 12), ('stupid', 12), ('life', 12), ('dont', 12), ('bastard', 11), ('hey', 11), ('talk', 10), ('little', 10), ('like', 9), (\"'m\", 9), ('hate', 9), ('die', 9), ('know', 8), ('delete', 8), ('im', 8), ('kill', 8), ('fat', 8), (\"'ll\", 8), ('shut', 7), ('fucked', 7), ('never', 7), ('fag', 7), ('thanks', 7), ('hell', 7), ('wiki', 7), ('edit', 7)]\n",
      "====\n",
      "[(\"n't\", 46), ('idiot', 39), ('u', 34), ('wikipedia', 33), ('like', 31), ('user', 31), ('stop', 30), ('go', 30), (\"'re\", 29), ('shit', 26), ('talk', 24), ('block', 24), ('page', 23), ('know', 23), ('get', 21), (\"'s\", 21), ('please', 20), ('article', 19), ('people', 18), (\"'m\", 18), ('stupid', 18), ('gay', 16), ('shut', 16), ('big', 16), ('blocked', 15), ('one', 15), ('edit', 15), ('im', 15), ('ur', 14), ('hey', 14), ('dont', 14), ('see', 13), ('think', 13), ('ban', 13), ('leave', 13), ('life', 13), ('sorry', 13), ('dick', 13), ('crap', 13), ('edits', 13), ('give', 13), ('moron', 13), ('suck', 12), ('fag', 12), ('personal', 12), ('time', 12), ('vandalism', 11), ('sex', 11), (\"'ve\", 11), ('well', 11)]\n",
      "====\n",
      "[('die', 21), ('kill', 17), ('fuck', 13), ('going', 10), ('fucking', 10), ('hope', 9), (\"'s\", 8), ('ass', 8), (\"n't\", 7), (\"'m\", 7), ('go', 7), (\"'re\", 6), ('rape', 6), (\"'ll\", 6), ('shit', 5), ('would', 5), ('hell', 5), ('hate', 5), ('im', 5), ('like', 4), ('stop', 4), ('na', 4), ('watch', 4), ('find', 4), ('death', 4), ('killing', 4), ('cunt', 4), ('bitch', 4), ('wikipedia', 4), ('gon', 4), ('head', 3), ('deserve', 3), ('house', 3), ('ur', 3), ('hi', 3), ('hey', 3), ('thanks', 3), ('shut', 3), ('muhajirs', 3), ('u', 3), ('bastard', 3), ('burn', 3), ('suggest', 3), ('live', 3), ('fucker', 3), ('right', 3), ('shoot', 3), ('come', 3), ('dont', 3), ('life', 3)]\n",
      "====\n",
      "[('fuck', 99), (\"n't\", 68), ('fucking', 54), ('wikipedia', 51), (\"'s\", 51), ('like', 50), ('u', 43), ('stop', 41), ('get', 38), (\"'re\", 38), ('go', 35), ('suck', 31), ('shit', 30), (\"'m\", 29), ('stupid', 28), ('gay', 28), ('talk', 27), ('please', 26), ('page', 26), ('people', 26), ('dick', 24), ('know', 24), ('life', 23), ('block', 23), ('want', 23), ('big', 23), ('dont', 22), ('blocked', 22), ('idiot', 22), ('ass', 22), ('user', 22), ('think', 20), ('give', 19), ('hell', 19), ('hey', 19), ('im', 19), ('would', 18), ('see', 18), ('racist', 18), ('well', 17), ('article', 17), ('really', 16), ('yeah', 16), ('read', 16), ('asshole', 15), ('little', 15), ('cock', 15), ('bullshit', 15), ('edits', 15), ('na', 14)]\n",
      "====\n",
      "[('gay', 64), ('fuck', 57), ('fucking', 37), ('nigger', 30), ('u', 27), ('go', 23), ('faggot', 20), ('like', 19), ('wikipedia', 17), ('fag', 16), (\"'re\", 16), ('bitch', 16), ('shit', 15), ('get', 14), ('jew', 14), ('nigga', 13), (\"n't\", 13), ('suck', 13), (\"'s\", 13), ('dick', 12), ('hey', 12), ('stop', 11), ('homosexual', 11), ('ass', 11), ('stupid', 11), ('life', 10), ('jews', 10), ('cunt', 10), ('mother', 9), ('would', 9), ('nazi', 9), ('user', 8), ('little', 8), ('one', 8), ('indian', 8), ('niggers', 8), ('oh', 8), ('kill', 8), ('people', 8), ('dont', 8), ('big', 8), ('black', 7), ('dumb', 7), ('know', 7), ('homo', 7), ('r', 7), ('fat', 7), ('fucker', 7), ('yes', 7), ('ur', 7)]\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "wrong_predictions = [wrong_predictions_toxic, wrong_predictions_severe_toxic, wrong_predictions_obscene, wrong_predictions_threat,\n",
    "                    wrong_predictions_insult, wrong_predictions_identity_hate]\n",
    "for wrong_pred in wrong_predictions:\n",
    "    bloblist = [tb(text) for text in wrong_pred]\n",
    "    important_words = []\n",
    "    for i, blob in enumerate(bloblist):\n",
    "    #     print(\"Top words in document {}\".format(i + 1))\n",
    "        scores = {word: tfidf(word, blob, bloblist) for word in blob.words if word.lower() not in eng_stopwords }\n",
    "        sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        important_words.extend([sorted_words[:10]])\n",
    "    #     for word, score in sorted_words[:3]:\n",
    "    #         print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
    "    important_words = [x[0].lower() for y in important_words for x in y]\n",
    "    important_words_counter = collections.Counter(important_words)\n",
    "    print(important_words_counter.most_common(50))\n",
    "    print('====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write wrong texts to files\n",
    "str_to_write = ''\n",
    "for text in wrong_predictions_toxic:\n",
    "    str_to_write = str_to_write + \"\\n\" + text\n",
    "with open(\"wrong_predictions_toxic.txt\", \"w\") as text_file:\n",
    "    text_file.write(str_to_write)\n",
    "\n",
    "str_to_write = ''\n",
    "for text in wrong_predictions_severe_toxic:\n",
    "    str_to_write = str_to_write + \"\\n\" + text\n",
    "with open(\"wrong_predictions_severe_toxic.txt\", \"w\") as text_file:\n",
    "    text_file.write(str_to_write)\n",
    "\n",
    "str_to_write = ''\n",
    "for text in wrong_predictions_obscene:\n",
    "    str_to_write = str_to_write + \"\\n\" + text\n",
    "with open(\"wrong_predictions_obscene.txt\", \"w\") as text_file:\n",
    "    text_file.write(str_to_write)\n",
    "    \n",
    "str_to_write = ''\n",
    "for text in wrong_predictions_threat:\n",
    "    str_to_write = str_to_write + \"\\n\" + text\n",
    "with open(\"wrong_predictions_threat.txt\", \"w\") as text_file:\n",
    "    text_file.write(str_to_write)\n",
    "    \n",
    "str_to_write = ''\n",
    "for text in wrong_predictions_identity_hate:\n",
    "    str_to_write = str_to_write + \"\\n\" + text\n",
    "with open(\"wrong_predictions_identity_hate.txt\", \"w\") as text_file:\n",
    "    text_file.write(str_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#check duplicates in obscence word list\n",
    "lines = []\n",
    "with open(\"../ling_src/obscene_words.txt\") as t:\n",
    "    lines = t.readlines()\n",
    "    for i in range(0,len(lines)):\n",
    "        lines[i] = lines[i].strip()\n",
    "    \n",
    "import collections\n",
    "print([item for item, count in collections.Counter(lines).items() if count > 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
